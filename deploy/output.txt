Attaching to cluster3, cluster1, cluster2
[36mcluster3    |[0m starting master, logging to /usr/local/hbase/hbase-0.98.24-hadoop2/logs/hbase--master-cluster3.out
[33mcluster1    |[0m starting master, logging to /usr/local/hbase/hbase-0.98.24-hadoop2/logs/hbase--master-cluster1.out
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:48:34,208 INFO  [main] server.ZooKeeperServer: Server environment:user.dir=/usr/local/hbase/hbase-0.98.24-hadoop2/bin
[36mcluster3    |[0m 2017-01-17 19:48:34,229 INFO  [main] server.ZooKeeperServer: Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /var/tmp/zookeeper_0/version-2 snapdir /var/tmp/zookeeper_0/version-2
[36mcluster3    |[0m 2017-01-17 19:48:34,241 INFO  [main] server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:18262
[36mcluster3    |[0m 2017-01-17 19:48:34,322 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:60514
[36mcluster3    |[0m 2017-01-17 19:48:34,328 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxn: Processing stat command from /127.0.0.1:60514
[36mcluster3    |[0m 2017-01-17 19:48:34,330 INFO  [Thread-1] server.NIOServerCnxn: Stat command output
[36mcluster3    |[0m 2017-01-17 19:48:34,331 INFO  [Thread-1] server.NIOServerCnxn: Closed socket connection for client /127.0.0.1:60514 (no session established for client)
[36mcluster3    |[0m 2017-01-17 19:48:34,331 INFO  [main] zookeeper.MiniZooKeeperCluster: Started MiniZK Cluster and connect 1 ZK server on client port: 18262
[36mcluster3    |[0m 2017-01-17 19:48:34,331 INFO  [main] master.HMasterCommandLine: Starting up instance of localHBaseCluster; master=1, regionserversCount=1
[36mcluster3    |[0m 2017-01-17 19:48:34,433 DEBUG [main] master.HMaster: master//0.0.0.0:0 HConnection server-to-server retries=350
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.out <==
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:48:34,294 INFO  [main] server.ZooKeeperServer: Server environment:user.dir=/usr/local/hbase/hbase-0.98.24-hadoop2/bin
[33mcluster1    |[0m 2017-01-17 19:48:34,313 INFO  [main] server.ZooKeeperServer: Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /var/tmp/zookeeper_0/version-2 snapdir /var/tmp/zookeeper_0/version-2
[33mcluster1    |[0m 2017-01-17 19:48:34,330 INFO  [main] server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:16262
[33mcluster1    |[0m 2017-01-17 19:48:34,407 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:57172
[33mcluster1    |[0m 2017-01-17 19:48:34,412 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxn: Processing stat command from /127.0.0.1:57172
[33mcluster1    |[0m 2017-01-17 19:48:34,416 INFO  [Thread-1] server.NIOServerCnxn: Stat command output
[33mcluster1    |[0m 2017-01-17 19:48:34,416 INFO  [Thread-1] server.NIOServerCnxn: Closed socket connection for client /127.0.0.1:57172 (no session established for client)
[33mcluster1    |[0m 2017-01-17 19:48:34,416 INFO  [main] zookeeper.MiniZooKeeperCluster: Started MiniZK Cluster and connect 1 ZK server on client port: 16262
[33mcluster1    |[0m 2017-01-17 19:48:34,416 INFO  [main] master.HMasterCommandLine: Starting up instance of localHBaseCluster; master=1, regionserversCount=1
[33mcluster1    |[0m 2017-01-17 19:48:34,538 DEBUG [main] master.HMaster: master//0.0.0.0:0 HConnection server-to-server retries=350
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.out <==
[32mcluster2    |[0m starting master, logging to /usr/local/hbase/hbase-0.98.24-hadoop2/logs/hbase--master-cluster2.out
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:48:34,640 INFO  [main] ipc.RpcServer: master//0.0.0.0:0: started 10 reader(s).
[36mcluster3    |[0m 2017-01-17 19:48:34,777 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
[36mcluster3    |[0m 2017-01-17 19:48:34,806 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
[36mcluster3    |[0m 2017-01-17 19:48:34,806 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
[36mcluster3    |[0m 2017-01-17 19:48:34,854 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[36mcluster3    |[0m 2017-01-17 19:48:35,572 INFO  [main] master.HMaster: hbase.rootdir=file:/var/tmp, hbase.cluster.distributed=false
[36mcluster3    |[0m 2017-01-17 19:48:35,578 INFO  [main] Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:48:34,742 INFO  [main] ipc.RpcServer: master//0.0.0.0:0: started 10 reader(s).
[33mcluster1    |[0m 2017-01-17 19:48:34,879 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
[33mcluster1    |[0m 2017-01-17 19:48:34,905 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
[33mcluster1    |[0m 2017-01-17 19:48:34,905 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
[33mcluster1    |[0m 2017-01-17 19:48:34,959 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[33mcluster1    |[0m 2017-01-17 19:48:35,667 INFO  [main] master.HMaster: hbase.rootdir=file:/var/tmp, hbase.cluster.distributed=false
[33mcluster1    |[0m 2017-01-17 19:48:35,675 INFO  [main] Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[32mcluster2    |[0m ==> logs/SecurityAuth.audit <==
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:java.io.tmpdir=/tmp
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:java.compiler=<NA>
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:os.name=Linux
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:os.arch=amd64
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:os.version=4.4.0-59-generic
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:user.name=root
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:user.home=/root
[32mcluster2    |[0m 2017-01-17 19:48:35,697 INFO  [main] server.ZooKeeperServer: Server environment:user.dir=/usr/local/hbase/hbase-0.98.24-hadoop2/bin
[32mcluster2    |[0m 2017-01-17 19:48:35,728 INFO  [main] server.ZooKeeperServer: Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /var/tmp/zookeeper_0/version-2 snapdir /var/tmp/zookeeper_0/version-2
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.out <==
[36mcluster3    |[0m 2017-01-17 19:48:35,658 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=master:36241 connecting to ZooKeeper ensemble=localhost:18262
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=cluster3
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_80
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-oracle/jre
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/usr/local/hbase/hbase-0.98.24-hadoop2/conf:/usr/lib/jvm/java-7-oracle//lib/tools.jar:/usr/local/hbase/hbase-0.98.24-hadoop2:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/activation-1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/aopalliance-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/asm-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/avro-1.7.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-beanutils-1.7.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-cli-1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-codec-1.7.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-collections-3.2.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-compress-1.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-configuration-1.6.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-daemon-1.0.13.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-digester-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-el-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-httpclient-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-io-2.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-lang-2.6.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-logging-1.1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-math-2.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-net-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/findbugs-annotations-1.3.9-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/gmbal-api-only-3.0.0-b023.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-framework-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-server-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-servlet-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-rcm-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guava-12.0.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guice-3.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guice-servlet-3.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-annotations-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-auth-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-client-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-hdfs-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-app-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-core-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-jobclient-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-shuffle-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-api-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-client-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-server-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-server-nodemanager-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hamcrest-core-1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-annotations-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-checkstyle-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-client-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-common-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-common-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-examples-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-hadoop-compat-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-hadoop2-compat-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-it-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-it-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-prefix-tree-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-protocol-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-resource-bundle-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-rest-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-server-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-server-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-shell-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-testing-util-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-thrift-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/high-scale-lib-1.1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/htrace-core-2.04.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/httpclient-4.1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/httpcore-4.1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-core-asl-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-jaxrs-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-mapper-asl-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-xc-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jamon-runtime-2.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jasper-compiler-5.5.23.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jasper-runtime-5.5.23.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.inject-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.servlet-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.servlet-api-3.0.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jaxb-api-2.2.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jcodings-1.0.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-client-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-core-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-grizzly2-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-guice-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-json-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-server-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-test-framework-core-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-test-framework-grizzly2-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jets3t-0.6.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jettison-1.3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-sslengine-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-util-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/joni-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jruby-complete-1.6.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsch-0.1.42.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsp-2.1-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsp-api-2.1-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/junit-4.11.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/libthrift-0.9.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/log4j-1.2.17.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/management-api-3.0.0-b012.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/metrics-core-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/netty-3.6.6.Final.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/paranamer-2.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/protobuf-java-2.5.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/protocommunication-1.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/saferegions-1.0.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/servlet-api-2.5-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/slf4j-api-1.6.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/slf4j-log4j12-1.6.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/smpc-1.0.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/snappy-java-1.0.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/xmlenc-0.52.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/xz-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/zookeeper-3.4.6.jar:
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=4.4.0-59-generic
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=root
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/root
[36mcluster3    |[0m 2017-01-17 19:48:35,666 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/usr/local/hbase/hbase-0.98.24-hadoop2/bin
[36mcluster3    |[0m 2017-01-17 19:48:35,668 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:18262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@4ff28fab
[36mcluster3    |[0m 2017-01-17 19:48:35,709 INFO  [main-SendThread(localhost:18262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:18262. Will not attempt to authenticate using SASL (unknown error)
[36mcluster3    |[0m 2017-01-17 19:48:35,709 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:60518
[36mcluster3    |[0m 2017-01-17 19:48:35,710 INFO  [main-SendThread(localhost:18262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:18262, initiating session
[36mcluster3    |[0m 2017-01-17 19:48:35,714 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:60518
[36mcluster3    |[0m 2017-01-17 19:48:35,718 INFO  [SyncThread:0] persistence.FileTxnLog: Creating new log file: log.1
[36mcluster3    |[0m 2017-01-17 19:48:35,739 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0000 with negotiated timeout 10000 for client /127.0.0.1:60518
[36mcluster3    |[0m 2017-01-17 19:48:35,740 INFO  [main-SendThread(localhost:18262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:18262, sessionid = 0x159adf94f4b0000, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:35,813 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
[36mcluster3    |[0m 2017-01-17 19:48:35,813 INFO  [RpcServer.listener,port=36241] ipc.RpcServer: RpcServer.listener,port=36241: starting
[36mcluster3    |[0m 2017-01-17 19:48:35,873 DEBUG [main] regionserver.HRegionServer: regionserver//0.0.0.0:0 HConnection server-to-server retries=350
[36mcluster3    |[0m 2017-01-17 19:48:35,991 INFO  [main] ipc.SimpleRpcScheduler: Using default user call queue, count=3
[36mcluster3    |[0m 2017-01-17 19:48:35,997 INFO  [main] ipc.RpcServer: regionserver//0.0.0.0:0: started 10 reader(s).
[36mcluster3    |[0m 2017-01-17 19:48:36,005 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7 M
[36mcluster3    |[0m 2017-01-17 19:48:36,012 INFO  [main] hfile.CacheConfig: Created cacheConfig: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 2017-01-17 19:48:36,079 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
[36mcluster3    |[0m 2017-01-17 19:48:36,125 INFO  [main] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
[36mcluster3    |[0m 2017-01-17 19:48:36,128 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context regionserver
[36mcluster3    |[0m 2017-01-17 19:48:36,128 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36mcluster3    |[0m 2017-01-17 19:48:36,138 INFO  [main] http.HttpServer: Jetty bound to port 46453
[36mcluster3    |[0m 2017-01-17 19:48:36,138 INFO  [main] mortbay.log: jetty-6.1.26
[36mcluster3    |[0m 2017-01-17 19:48:36,565 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:46453
[36mcluster3    |[0m 2017-01-17 19:48:36,594 INFO  [M:0;cluster3:36241] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
[36mcluster3    |[0m 2017-01-17 19:48:36,595 INFO  [M:0;cluster3:36241] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context master
[36mcluster3    |[0m 2017-01-17 19:48:36,595 INFO  [M:0;cluster3:36241] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[36mcluster3    |[0m 2017-01-17 19:48:36,606 INFO  [M:0;cluster3:36241] http.HttpServer: Jetty bound to port 60010
[36mcluster3    |[0m 2017-01-17 19:48:36,606 INFO  [M:0;cluster3:36241] mortbay.log: jetty-6.1.26
[33mcluster1    |[0m 2017-01-17 19:48:35,755 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=master:37102 connecting to ZooKeeper ensemble=localhost:16262
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=cluster1
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_80
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-oracle/jre
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/usr/local/hbase/hbase-0.98.24-hadoop2/conf:/usr/lib/jvm/java-7-oracle//lib/tools.jar:/usr/local/hbase/hbase-0.98.24-hadoop2:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/activation-1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/aopalliance-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/asm-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/avro-1.7.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-beanutils-1.7.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-cli-1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-codec-1.7.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-collections-3.2.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-compress-1.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-configuration-1.6.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-daemon-1.0.13.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-digester-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-el-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-httpclient-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-io-2.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-lang-2.6.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-logging-1.1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-math-2.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-net-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/findbugs-annotations-1.3.9-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/gmbal-api-only-3.0.0-b023.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-framework-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-server-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-servlet-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-rcm-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guava-12.0.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guice-3.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guice-servlet-3.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-annotations-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-auth-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-client-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-hdfs-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-app-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-core-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-jobclient-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-shuffle-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-api-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-client-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-server-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-server-nodemanager-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hamcrest-core-1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-annotations-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-checkstyle-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-client-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-common-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-common-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-examples-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-hadoop-compat-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-hadoop2-compat-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-it-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-it-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-prefix-tree-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-protocol-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-resource-bundle-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-rest-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-server-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-server-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-shell-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-testing-util-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-thrift-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/high-scale-lib-1.1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/htrace-core-2.04.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/httpclient-4.1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/httpcore-4.1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-core-asl-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-jaxrs-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-mapper-asl-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-xc-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jamon-runtime-2.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jasper-compiler-5.5.23.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jasper-runtime-5.5.23.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.inject-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.servlet-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.servlet-api-3.0.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jaxb-api-2.2.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jcodings-1.0.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-client-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-core-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-grizzly2-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-guice-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-json-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-server-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-test-framework-core-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-test-framework-grizzly2-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jets3t-0.6.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jettison-1.3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-sslengine-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-util-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/joni-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jruby-complete-1.6.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsch-0.1.42.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsp-2.1-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsp-api-2.1-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/junit-4.11.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/libthrift-0.9.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/log4j-1.2.17.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/management-api-3.0.0-b012.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/metrics-core-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/netty-3.6.6.Final.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/paranamer-2.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/protobuf-java-2.5.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/protocommunication-1.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/saferegions-1.0.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/servlet-api-2.5-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/slf4j-api-1.6.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/slf4j-log4j12-1.6.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/smpc-1.0.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/snappy-java-1.0.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/xmlenc-0.52.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/xz-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/zookeeper-3.4.6.jar:
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[33mcluster1    |[0m 2017-01-17 19:48:35,760 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=4.4.0-59-generic
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=root
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/root
[33mcluster1    |[0m 2017-01-17 19:48:35,761 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/usr/local/hbase/hbase-0.98.24-hadoop2/bin
[33mcluster1    |[0m 2017-01-17 19:48:35,762 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:16262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@1482ce17
[33mcluster1    |[0m 2017-01-17 19:48:35,780 INFO  [main-SendThread(localhost:16262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:16262. Will not attempt to authenticate using SASL (unknown error)
[33mcluster1    |[0m 2017-01-17 19:48:35,780 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:46484
[33mcluster1    |[0m 2017-01-17 19:48:35,782 INFO  [main-SendThread(localhost:16262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:16262, initiating session
[33mcluster1    |[0m 2017-01-17 19:48:35,787 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:46484
[33mcluster1    |[0m 2017-01-17 19:48:35,792 INFO  [SyncThread:0] persistence.FileTxnLog: Creating new log file: log.1
[33mcluster1    |[0m 2017-01-17 19:48:35,801 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40000 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:46484
[33mcluster1    |[0m 2017-01-17 19:48:35,802 INFO  [main-SendThread(localhost:16262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:16262, sessionid = 0x159adf94fa40000, negotiated timeout = 10000
[33mcluster1    |[0m 2017-01-17 19:48:35,862 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
[33mcluster1    |[0m 2017-01-17 19:48:35,865 INFO  [RpcServer.listener,port=37102] ipc.RpcServer: RpcServer.listener,port=37102: starting
[33mcluster1    |[0m 2017-01-17 19:48:35,942 DEBUG [main] regionserver.HRegionServer: regionserver//0.0.0.0:0 HConnection server-to-server retries=350
[33mcluster1    |[0m 2017-01-17 19:48:36,056 INFO  [main] ipc.SimpleRpcScheduler: Using default user call queue, count=3
[33mcluster1    |[0m 2017-01-17 19:48:36,062 INFO  [main] ipc.RpcServer: regionserver//0.0.0.0:0: started 10 reader(s).
[33mcluster1    |[0m 2017-01-17 19:48:36,071 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7 M
[33mcluster1    |[0m 2017-01-17 19:48:36,076 INFO  [main] hfile.CacheConfig: Created cacheConfig: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[33mcluster1    |[0m 2017-01-17 19:48:36,130 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
[33mcluster1    |[0m 2017-01-17 19:48:36,174 INFO  [main] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
[33mcluster1    |[0m 2017-01-17 19:48:36,177 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context regionserver
[33mcluster1    |[0m 2017-01-17 19:48:36,177 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33mcluster1    |[0m 2017-01-17 19:48:36,190 INFO  [main] http.HttpServer: Jetty bound to port 35051
[33mcluster1    |[0m 2017-01-17 19:48:36,190 INFO  [main] mortbay.log: jetty-6.1.26
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:48:35,747 INFO  [main] server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:17262
[32mcluster2    |[0m 2017-01-17 19:48:35,832 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:48988
[32mcluster2    |[0m 2017-01-17 19:48:35,838 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxn: Processing stat command from /127.0.0.1:48988
[32mcluster2    |[0m 2017-01-17 19:48:35,840 INFO  [Thread-1] server.NIOServerCnxn: Stat command output
[32mcluster2    |[0m 2017-01-17 19:48:35,841 INFO  [Thread-1] server.NIOServerCnxn: Closed socket connection for client /127.0.0.1:48988 (no session established for client)
[32mcluster2    |[0m 2017-01-17 19:48:35,841 INFO  [main] zookeeper.MiniZooKeeperCluster: Started MiniZK Cluster and connect 1 ZK server on client port: 17262
[32mcluster2    |[0m 2017-01-17 19:48:35,842 INFO  [main] master.HMasterCommandLine: Starting up instance of localHBaseCluster; master=1, regionserversCount=1
[32mcluster2    |[0m 2017-01-17 19:48:35,955 DEBUG [main] master.HMaster: master//0.0.0.0:0 HConnection server-to-server retries=350
[32mcluster2    |[0m 2017-01-17 19:48:36,142 INFO  [main] ipc.RpcServer: master//0.0.0.0:0: started 10 reader(s).
[32mcluster2    |[0m 2017-01-17 19:48:36,282 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
[32mcluster2    |[0m 2017-01-17 19:48:36,316 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
[32mcluster2    |[0m 2017-01-17 19:48:36,316 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
[32mcluster2    |[0m 2017-01-17 19:48:36,388 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[36mcluster3    |[0m 2017-01-17 19:48:36,813 INFO  [M:0;cluster3:36241] mortbay.log: Started SelectChannelConnector@0.0.0.0:60010
[36mcluster3    |[0m 2017-01-17 19:48:36,934 DEBUG [main-EventThread] master.ActiveMasterManager: A master is now available
[36mcluster3    |[0m 2017-01-17 19:48:36,936 INFO  [M:0;cluster3:36241] master.ActiveMasterManager: Registered Active Master=cluster3,36241,1484682515004
[36mcluster3    |[0m 2017-01-17 19:48:36,968 INFO  [M:0;cluster3:36241] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[36mcluster3    |[0m 2017-01-17 19:48:36,975 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:RS:0;0:0:0:0:0:0:0:0:42333
[36mcluster3    |[0m 2017-01-17 19:48:36,993 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:42333 connecting to ZooKeeper ensemble=localhost:18262
[36mcluster3    |[0m 2017-01-17 19:48:36,993 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:18262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@5bf2f522
[36mcluster3    |[0m 2017-01-17 19:48:37,001 INFO  [RS:0;0:0:0:0:0:0:0:0:42333-SendThread(localhost:18262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:18262. Will not attempt to authenticate using SASL (unknown error)
[36mcluster3    |[0m 2017-01-17 19:48:37,003 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:60524
[36mcluster3    |[0m 2017-01-17 19:48:37,003 INFO  [RS:0;0:0:0:0:0:0:0:0:42333-SendThread(localhost:18262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:18262, initiating session
[36mcluster3    |[0m 2017-01-17 19:48:37,004 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:60524
[36mcluster3    |[0m 2017-01-17 19:48:37,006 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0001 with negotiated timeout 10000 for client /127.0.0.1:60524
[36mcluster3    |[0m 2017-01-17 19:48:37,006 INFO  [RS:0;0:0:0:0:0:0:0:0:42333-SendThread(localhost:18262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:18262, sessionid = 0x159adf94f4b0001, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:37,029 INFO  [M:0;cluster3:36241] util.FSUtils: Created version file at file:/var/tmp with version=8
[36mcluster3    |[0m 2017-01-17 19:48:37,047 DEBUG [M:0;cluster3:36241] util.FSUtils: Created cluster ID file at file:/var/tmp/hbase.id with ID: 2d0aa130-55f8-4284-a08c-16eaeeeba9f2
[36mcluster3    |[0m 2017-01-17 19:48:37,058 INFO  [M:0;cluster3:36241] master.MasterFileSystem: BOOTSTRAP: creating hbase:meta region
[36mcluster3    |[0m 2017-01-17 19:48:37,113 INFO  [M:0;cluster3:36241] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[36mcluster3    |[0m 2017-01-17 19:48:37,322 INFO  [M:0;cluster3:36241] regionserver.HRegion: creating HRegion hbase:meta HTD == 'hbase:meta', {TABLE_ATTRIBUTES => {IS_META => 'true', coprocessor$1 => '|org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint|536870911|'}, {NAME => 'info', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => 'FOREVER', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'} RootDir = file:/var/tmp Table name == hbase:meta
[36mcluster3    |[0m 2017-01-17 19:48:37,359 INFO  [M:0;cluster3:36241] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[36mcluster3    |[0m 2017-01-17 19:48:37,390 INFO  [M:0;cluster3:36241] wal.FSHLog: New WAL /var/tmp/data/hbase/meta/1588230740/WALs/hlog.1484682517361
[36mcluster3    |[0m 2017-01-17 19:48:37,390 INFO  [M:0;cluster3:36241] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer
[36mcluster3    |[0m 2017-01-17 19:48:37,390 INFO  [M:0;cluster3:36241] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer
[36mcluster3    |[0m 2017-01-17 19:48:37,444 DEBUG [M:0;cluster3:36241] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:48:37,520 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=false] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 2017-01-17 19:48:37,529 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[36mcluster3    |[0m 2017-01-17 19:48:37,543 INFO  [StoreOpener-1588230740-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
[36mcluster3    |[0m 2017-01-17 19:48:37,545 INFO  [StoreOpener-1588230740-1] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
[36mcluster3    |[0m 2017-01-17 19:48:37,549 DEBUG [M:0;cluster3:36241] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/meta/1588230740
[36mcluster3    |[0m 2017-01-17 19:48:37,554 INFO  [M:0;cluster3:36241] regionserver.HRegion: Onlined 1588230740; next sequenceid=1
[36mcluster3    |[0m 2017-01-17 19:48:37,554 DEBUG [M:0;cluster3:36241] regionserver.HRegion: Closing hbase:meta,,1.1588230740: disabling compactions & flushes
[36mcluster3    |[0m 2017-01-17 19:48:37,555 DEBUG [M:0;cluster3:36241] regionserver.HRegion: Updates disabled for region hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:48:37,559 INFO  [StoreCloserThread-hbase:meta,,1.1588230740-1] regionserver.HStore: Closed info
[36mcluster3    |[0m 2017-01-17 19:48:37,559 INFO  [M:0;cluster3:36241] regionserver.HRegion: Closed hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:48:37,560 DEBUG [M:0;cluster3:36241-WAL.AsyncNotifier] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
[36mcluster3    |[0m 2017-01-17 19:48:37,560 INFO  [M:0;cluster3:36241-WAL.AsyncNotifier] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncNotifier exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,561 DEBUG [M:0;cluster3:36241-WAL.AsyncSyncer0] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
[36mcluster3    |[0m 2017-01-17 19:48:37,561 INFO  [M:0;cluster3:36241-WAL.AsyncSyncer0] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer0 exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,561 DEBUG [M:0;cluster3:36241-WAL.AsyncSyncer1] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
[36mcluster3    |[0m 2017-01-17 19:48:37,561 INFO  [M:0;cluster3:36241-WAL.AsyncSyncer1] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer1 exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,561 DEBUG [M:0;cluster3:36241-WAL.AsyncSyncer2] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
[36mcluster3    |[0m 2017-01-17 19:48:37,561 INFO  [M:0;cluster3:36241-WAL.AsyncSyncer2] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer2 exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,561 DEBUG [M:0;cluster3:36241-WAL.AsyncSyncer3] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
[36mcluster3    |[0m 2017-01-17 19:48:37,561 INFO  [M:0;cluster3:36241-WAL.AsyncSyncer3] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer3 exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,561 DEBUG [M:0;cluster3:36241-WAL.AsyncSyncer4] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
[36mcluster3    |[0m 2017-01-17 19:48:37,561 INFO  [M:0;cluster3:36241-WAL.AsyncSyncer4] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncSyncer4 exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,562 DEBUG [M:0;cluster3:36241-WAL.AsyncWriter] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
[36mcluster3    |[0m 2017-01-17 19:48:37,562 INFO  [M:0;cluster3:36241-WAL.AsyncWriter] wal.FSHLog: M:0;cluster3:36241-WAL.AsyncWriter exiting
[36mcluster3    |[0m 2017-01-17 19:48:37,562 DEBUG [M:0;cluster3:36241] wal.FSHLog: Closing WAL writer in file:/var/tmp/data/hbase/meta/1588230740/WALs
[36mcluster3    |[0m 2017-01-17 19:48:37,564 DEBUG [M:0;cluster3:36241] wal.FSHLog: Moved 1 WAL file(s) to /var/tmp/data/hbase/meta/1588230740/oldWALs
[36mcluster3    |[0m 2017-01-17 19:48:37,595 DEBUG [M:0;cluster3:36241] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/data/hbase/meta/.tabledesc/.tableinfo.0000000001
[36mcluster3    |[0m 2017-01-17 19:48:37,602 DEBUG [M:0;cluster3:36241] fs.HFileSystem: The file system is not a DistributedFileSystem. Skipping on block location reordering
[36mcluster3    |[0m 2017-01-17 19:48:37,612 DEBUG [M:0;cluster3:36241] master.SplitLogManager: Distributed log replay=false, hfile.format.version=2
[36mcluster3    |[0m 2017-01-17 19:48:37,616 INFO  [M:0;cluster3:36241] master.SplitLogManager: Timeout=120000, unassigned timeout=180000, distributedLogReplay=false
[36mcluster3    |[0m 2017-01-17 19:48:37,622 INFO  [M:0;cluster3:36241] master.SplitLogManager: Found 0 orphan tasks and 0 rescan nodes
[36mcluster3    |[0m 2017-01-17 19:48:37,623 DEBUG [M:0;cluster3:36241] util.FSTableDescriptors: Fetching table descriptors from the filesystem.
[33mcluster1    |[0m 2017-01-17 19:48:36,823 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:35051
[33mcluster1    |[0m 2017-01-17 19:48:36,846 INFO  [M:0;cluster1:37102] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
[33mcluster1    |[0m 2017-01-17 19:48:36,848 INFO  [M:0;cluster1:37102] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context master
[33mcluster1    |[0m 2017-01-17 19:48:36,848 INFO  [M:0;cluster1:37102] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[33mcluster1    |[0m 2017-01-17 19:48:36,852 INFO  [M:0;cluster1:37102] http.HttpServer: Jetty bound to port 60010
[33mcluster1    |[0m 2017-01-17 19:48:36,852 INFO  [M:0;cluster1:37102] mortbay.log: jetty-6.1.26
[33mcluster1    |[0m 2017-01-17 19:48:37,024 INFO  [M:0;cluster1:37102] mortbay.log: Started SelectChannelConnector@0.0.0.0:60010
[33mcluster1    |[0m 2017-01-17 19:48:37,114 DEBUG [main-EventThread] master.ActiveMasterManager: A master is now available
[33mcluster1    |[0m 2017-01-17 19:48:37,117 INFO  [M:0;cluster1:37102] master.ActiveMasterManager: Registered Active Master=cluster1,37102,1484682515111
[33mcluster1    |[0m 2017-01-17 19:48:37,148 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:RS:0;0:0:0:0:0:0:0:0:38372
[33mcluster1    |[0m 2017-01-17 19:48:37,149 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:38372 connecting to ZooKeeper ensemble=localhost:16262
[33mcluster1    |[0m 2017-01-17 19:48:37,149 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:16262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@362eec25
[33mcluster1    |[0m 2017-01-17 19:48:37,150 INFO  [RS:0;0:0:0:0:0:0:0:0:38372-SendThread(localhost:16262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:16262. Will not attempt to authenticate using SASL (unknown error)
[33mcluster1    |[0m 2017-01-17 19:48:37,150 INFO  [RS:0;0:0:0:0:0:0:0:0:38372-SendThread(localhost:16262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:16262, initiating session
[33mcluster1    |[0m 2017-01-17 19:48:37,150 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:57182
[33mcluster1    |[0m 2017-01-17 19:48:37,150 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:57182
[33mcluster1    |[0m 2017-01-17 19:48:37,150 INFO  [M:0;cluster1:37102] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[33mcluster1    |[0m 2017-01-17 19:48:37,154 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40001 with negotiated timeout 10000 for client /127.0.0.1:57182
[33mcluster1    |[0m 2017-01-17 19:48:37,154 INFO  [RS:0;0:0:0:0:0:0:0:0:38372-SendThread(localhost:16262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:16262, sessionid = 0x159adf94fa40001, negotiated timeout = 10000
[33mcluster1    |[0m 2017-01-17 19:48:37,221 INFO  [M:0;cluster1:37102] util.FSUtils: Created version file at file:/var/tmp with version=8
[33mcluster1    |[0m 2017-01-17 19:48:37,232 DEBUG [M:0;cluster1:37102] util.FSUtils: Created cluster ID file at file:/var/tmp/hbase.id with ID: 19a8f54f-f204-4d89-9d33-a3b3330d1d36
[33mcluster1    |[0m 2017-01-17 19:48:37,242 INFO  [M:0;cluster1:37102] master.MasterFileSystem: BOOTSTRAP: creating hbase:meta region
[33mcluster1    |[0m 2017-01-17 19:48:37,293 INFO  [M:0;cluster1:37102] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[33mcluster1    |[0m 2017-01-17 19:48:37,476 INFO  [M:0;cluster1:37102] regionserver.HRegion: creating HRegion hbase:meta HTD == 'hbase:meta', {TABLE_ATTRIBUTES => {IS_META => 'true', coprocessor$1 => '|org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint|536870911|'}, {NAME => 'info', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => 'FOREVER', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'} RootDir = file:/var/tmp Table name == hbase:meta
[33mcluster1    |[0m 2017-01-17 19:48:37,505 INFO  [M:0;cluster1:37102] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[33mcluster1    |[0m 2017-01-17 19:48:37,528 INFO  [M:0;cluster1:37102] wal.FSHLog: New WAL /var/tmp/data/hbase/meta/1588230740/WALs/hlog.1484682517506
[33mcluster1    |[0m 2017-01-17 19:48:37,529 INFO  [M:0;cluster1:37102] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer
[33mcluster1    |[0m 2017-01-17 19:48:37,529 INFO  [M:0;cluster1:37102] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer
[33mcluster1    |[0m 2017-01-17 19:48:37,551 DEBUG [M:0;cluster1:37102] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:48:37,638 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=false] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[33mcluster1    |[0m 2017-01-17 19:48:37,649 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[33mcluster1    |[0m 2017-01-17 19:48:37,668 INFO  [StoreOpener-1588230740-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
[33mcluster1    |[0m 2017-01-17 19:48:37,669 INFO  [StoreOpener-1588230740-1] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
[33mcluster1    |[0m 2017-01-17 19:48:37,674 DEBUG [M:0;cluster1:37102] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/meta/1588230740
[33mcluster1    |[0m 2017-01-17 19:48:37,678 INFO  [M:0;cluster1:37102] regionserver.HRegion: Onlined 1588230740; next sequenceid=1
[33mcluster1    |[0m 2017-01-17 19:48:37,678 DEBUG [M:0;cluster1:37102] regionserver.HRegion: Closing hbase:meta,,1.1588230740: disabling compactions & flushes
[33mcluster1    |[0m 2017-01-17 19:48:37,678 DEBUG [M:0;cluster1:37102] regionserver.HRegion: Updates disabled for region hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:48:37,680 INFO  [StoreCloserThread-hbase:meta,,1.1588230740-1] regionserver.HStore: Closed info
[33mcluster1    |[0m 2017-01-17 19:48:37,681 INFO  [M:0;cluster1:37102] regionserver.HRegion: Closed hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:48:37,682 DEBUG [M:0;cluster1:37102-WAL.AsyncNotifier] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
[33mcluster1    |[0m 2017-01-17 19:48:37,682 INFO  [M:0;cluster1:37102-WAL.AsyncNotifier] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncNotifier exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,682 DEBUG [M:0;cluster1:37102-WAL.AsyncSyncer0] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
[33mcluster1    |[0m 2017-01-17 19:48:37,682 INFO  [M:0;cluster1:37102-WAL.AsyncSyncer0] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer0 exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,682 DEBUG [M:0;cluster1:37102-WAL.AsyncSyncer1] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
[33mcluster1    |[0m 2017-01-17 19:48:37,682 INFO  [M:0;cluster1:37102-WAL.AsyncSyncer1] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer1 exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,683 DEBUG [M:0;cluster1:37102-WAL.AsyncSyncer2] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
[33mcluster1    |[0m 2017-01-17 19:48:37,683 INFO  [M:0;cluster1:37102-WAL.AsyncSyncer2] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer2 exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,683 DEBUG [M:0;cluster1:37102-WAL.AsyncSyncer3] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
[33mcluster1    |[0m 2017-01-17 19:48:37,683 INFO  [M:0;cluster1:37102-WAL.AsyncSyncer3] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer3 exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,683 DEBUG [M:0;cluster1:37102-WAL.AsyncSyncer4] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
[33mcluster1    |[0m 2017-01-17 19:48:37,683 INFO  [M:0;cluster1:37102-WAL.AsyncSyncer4] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncSyncer4 exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,683 DEBUG [M:0;cluster1:37102-WAL.AsyncWriter] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
[33mcluster1    |[0m 2017-01-17 19:48:37,683 INFO  [M:0;cluster1:37102-WAL.AsyncWriter] wal.FSHLog: M:0;cluster1:37102-WAL.AsyncWriter exiting
[33mcluster1    |[0m 2017-01-17 19:48:37,683 DEBUG [M:0;cluster1:37102] wal.FSHLog: Closing WAL writer in file:/var/tmp/data/hbase/meta/1588230740/WALs
[33mcluster1    |[0m 2017-01-17 19:48:37,685 DEBUG [M:0;cluster1:37102] wal.FSHLog: Moved 1 WAL file(s) to /var/tmp/data/hbase/meta/1588230740/oldWALs
[32mcluster2    |[0m 2017-01-17 19:48:37,236 INFO  [main] master.HMaster: hbase.rootdir=file:/var/tmp, hbase.cluster.distributed=false
[32mcluster2    |[0m 2017-01-17 19:48:37,245 INFO  [main] Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[32mcluster2    |[0m 2017-01-17 19:48:37,416 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=master:42021 connecting to ZooKeeper ensemble=localhost:17262
[32mcluster2    |[0m 2017-01-17 19:48:37,423 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[32mcluster2    |[0m 2017-01-17 19:48:37,423 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=cluster2
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_80
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-oracle/jre
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/usr/local/hbase/hbase-0.98.24-hadoop2/conf:/usr/lib/jvm/java-7-oracle//lib/tools.jar:/usr/local/hbase/hbase-0.98.24-hadoop2:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/activation-1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/aopalliance-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/asm-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/avro-1.7.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-beanutils-1.7.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-cli-1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-codec-1.7.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-collections-3.2.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-compress-1.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-configuration-1.6.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-daemon-1.0.13.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-digester-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-el-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-httpclient-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-io-2.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-lang-2.6.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-logging-1.1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-math-2.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/commons-net-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/findbugs-annotations-1.3.9-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/gmbal-api-only-3.0.0-b023.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-framework-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-server-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-http-servlet-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/grizzly-rcm-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guava-12.0.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guice-3.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/guice-servlet-3.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-annotations-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-auth-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-client-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-hdfs-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-app-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-core-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-jobclient-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-mapreduce-client-shuffle-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-api-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-client-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-server-common-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hadoop-yarn-server-nodemanager-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hamcrest-core-1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-annotations-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-checkstyle-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-client-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-common-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-common-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-examples-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-hadoop-compat-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-hadoop2-compat-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-it-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-it-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-prefix-tree-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-protocol-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-resource-bundle-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-rest-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-server-0.98.24-hadoop2-tests.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-server-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-shell-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-testing-util-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/hbase-thrift-0.98.24-hadoop2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/high-scale-lib-1.1.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/htrace-core-2.04.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/httpclient-4.1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/httpcore-4.1.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-core-asl-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-jaxrs-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-mapper-asl-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jackson-xc-1.8.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jamon-runtime-2.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jasper-compiler-5.5.23.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jasper-runtime-5.5.23.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.inject-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.servlet-3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/javax.servlet-api-3.0.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jaxb-api-2.2.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jcodings-1.0.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-client-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-core-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-grizzly2-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-guice-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-json-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-server-1.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-test-framework-core-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jersey-test-framework-grizzly2-1.9.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jets3t-0.6.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jettison-1.3.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-sslengine-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jetty-util-6.1.26.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/joni-2.1.2.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jruby-complete-1.6.8.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsch-0.1.42.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsp-2.1-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/jsp-api-2.1-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/junit-4.11.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/libthrift-0.9.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/log4j-1.2.17.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/management-api-3.0.0-b012.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/metrics-core-2.2.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/netty-3.6.6.Final.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/paranamer-2.3.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/protobuf-java-2.5.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/protocommunication-1.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/saferegions-1.0.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/servlet-api-2.5-6.1.14.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/slf4j-api-1.6.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/slf4j-log4j12-1.6.4.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/smpc-1.0.0-SNAPSHOT.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/snappy-java-1.0.4.1.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/xmlenc-0.52.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/xz-1.0.jar:/usr/local/hbase/hbase-0.98.24-hadoop2/lib/zookeeper-3.4.6.jar:
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=4.4.0-59-generic
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=root
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/root
[32mcluster2    |[0m 2017-01-17 19:48:37,424 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/usr/local/hbase/hbase-0.98.24-hadoop2/bin
[32mcluster2    |[0m 2017-01-17 19:48:37,426 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:17262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@385900f
[32mcluster2    |[0m 2017-01-17 19:48:37,444 INFO  [main-SendThread(localhost:17262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:17262. Will not attempt to authenticate using SASL (unknown error)
[32mcluster2    |[0m 2017-01-17 19:48:37,445 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:48994
[32mcluster2    |[0m 2017-01-17 19:48:37,445 INFO  [main-SendThread(localhost:17262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:17262, initiating session
[32mcluster2    |[0m 2017-01-17 19:48:37,449 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:48994
[32mcluster2    |[0m 2017-01-17 19:48:37,453 INFO  [SyncThread:0] persistence.FileTxnLog: Creating new log file: log.1
[32mcluster2    |[0m 2017-01-17 19:48:37,469 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0000 with negotiated timeout 10000 for client /127.0.0.1:48994
[32mcluster2    |[0m 2017-01-17 19:48:37,470 INFO  [main-SendThread(localhost:17262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:17262, sessionid = 0x159adf9552f0000, negotiated timeout = 10000
[32mcluster2    |[0m 2017-01-17 19:48:37,523 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
[32mcluster2    |[0m 2017-01-17 19:48:37,526 INFO  [RpcServer.listener,port=42021] ipc.RpcServer: RpcServer.listener,port=42021: starting
[32mcluster2    |[0m 2017-01-17 19:48:37,606 DEBUG [main] regionserver.HRegionServer: regionserver//0.0.0.0:0 HConnection server-to-server retries=350
[32mcluster2    |[0m 2017-01-17 19:48:37,712 INFO  [main] ipc.SimpleRpcScheduler: Using default user call queue, count=3
[32mcluster2    |[0m 2017-01-17 19:48:37,716 INFO  [main] ipc.RpcServer: regionserver//0.0.0.0:0: started 10 reader(s).
[32mcluster2    |[0m 2017-01-17 19:48:37,723 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7 M
[32mcluster2    |[0m 2017-01-17 19:48:37,730 INFO  [main] hfile.CacheConfig: Created cacheConfig: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 2017-01-17 19:48:38,047 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:48:38,063 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.3 port: 42884 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:48:37,667 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x47c65163 connecting to ZooKeeper ensemble=localhost:18262
[36mcluster3    |[0m 2017-01-17 19:48:37,667 INFO  [M:0;cluster3:36241] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:18262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@2311f02d
[36mcluster3    |[0m 2017-01-17 19:48:37,668 INFO  [M:0;cluster3:36241-SendThread(localhost:18262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:18262. Will not attempt to authenticate using SASL (unknown error)
[36mcluster3    |[0m 2017-01-17 19:48:37,668 INFO  [M:0;cluster3:36241-SendThread(localhost:18262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:18262, initiating session
[36mcluster3    |[0m 2017-01-17 19:48:37,668 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:60530
[36mcluster3    |[0m 2017-01-17 19:48:37,669 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:60530
[36mcluster3    |[0m 2017-01-17 19:48:37,671 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0002 with negotiated timeout 10000 for client /127.0.0.1:60530
[36mcluster3    |[0m 2017-01-17 19:48:37,671 INFO  [M:0;cluster3:36241-SendThread(localhost:18262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:18262, sessionid = 0x159adf94f4b0002, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:37,699 DEBUG [M:0;cluster3:36241] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@587b5ec5
[36mcluster3    |[0m 2017-01-17 19:48:37,760 INFO  [M:0;cluster3:36241] master.HMaster: Server active/primary master=cluster3,36241,1484682515004, sessionid=0x159adf94f4b0000, setting cluster-up flag (Was=false)
[36mcluster3    |[0m 2017-01-17 19:48:37,761 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@2dff9582
[36mcluster3    |[0m 2017-01-17 19:48:37,762 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: ClusterId : 2d0aa130-55f8-4284-a08c-16eaeeeba9f2
[36mcluster3    |[0m 2017-01-17 19:48:37,767 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
[36mcluster3    |[0m 2017-01-17 19:48:37,773 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0001 type:create cxid:0x8 zxid:0x11 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot Error:KeeperErrorCode = NoNode for /hbase/online-snapshot
[36mcluster3    |[0m 2017-01-17 19:48:37,775 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x2a zxid:0x12 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot Error:KeeperErrorCode = NoNode for /hbase/online-snapshot
[36mcluster3    |[0m 2017-01-17 19:48:37,777 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x2b zxid:0x14 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot
[36mcluster3    |[0m 2017-01-17 19:48:37,780 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot already exists and this is not a retry
[36mcluster3    |[0m 2017-01-17 19:48:37,780 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x2c zxid:0x16 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/acquired Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/acquired
[36mcluster3    |[0m 2017-01-17 19:48:37,783 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
[36mcluster3    |[0m 2017-01-17 19:48:37,784 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x2e zxid:0x18 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/reached Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/reached
[36mcluster3    |[0m 2017-01-17 19:48:37,787 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/reached already exists and this is not a retry
[36mcluster3    |[0m 2017-01-17 19:48:37,791 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x30 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/abort Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/abort
[36mcluster3    |[0m 2017-01-17 19:48:37,793 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/abort already exists and this is not a retry
[36mcluster3    |[0m 2017-01-17 19:48:37,793 INFO  [M:0;cluster3:36241] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/online-snapshot/acquired /hbase/online-snapshot/reached /hbase/online-snapshot/abort
[36mcluster3    |[0m 2017-01-17 19:48:37,794 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
[36mcluster3    |[0m 2017-01-17 19:48:37,795 DEBUG [M:0;cluster3:36241] procedure.ZKProcedureCoordinatorRpcs: Starting the controller for procedure member:cluster3,36241,1484682515004
[36mcluster3    |[0m 2017-01-17 19:48:37,798 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7 M, globalMemStoreLimitLowMark=367.3 M, maxHeap=966.7 M
[36mcluster3    |[0m 2017-01-17 19:48:37,803 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: CompactionChecker runs every 10sec
[36mcluster3    |[0m 2017-01-17 19:48:37,808 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: reportForDuty to master=cluster3,36241,1484682515004 with port=42333, startcode=1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:37,819 INFO  [M:0;cluster3:36241] master.MasterCoprocessorHost: System coprocessor loading is enabled
[36mcluster3    |[0m 2017-01-17 19:48:37,823 DEBUG [M:0;cluster3:36241] executor.ExecutorService: Starting executor service name=MASTER_OPEN_REGION-cluster3:36241, corePoolSize=5, maxPoolSize=5
[36mcluster3    |[0m 2017-01-17 19:48:37,824 DEBUG [M:0;cluster3:36241] executor.ExecutorService: Starting executor service name=MASTER_CLOSE_REGION-cluster3:36241, corePoolSize=5, maxPoolSize=5
[36mcluster3    |[0m 2017-01-17 19:48:37,824 DEBUG [M:0;cluster3:36241] executor.ExecutorService: Starting executor service name=MASTER_SERVER_OPERATIONS-cluster3:36241, corePoolSize=5, maxPoolSize=5
[36mcluster3    |[0m 2017-01-17 19:48:37,824 DEBUG [M:0;cluster3:36241] executor.ExecutorService: Starting executor service name=MASTER_META_SERVER_OPERATIONS-cluster3:36241, corePoolSize=5, maxPoolSize=5
[36mcluster3    |[0m 2017-01-17 19:48:37,824 DEBUG [M:0;cluster3:36241] executor.ExecutorService: Starting executor service name=M_LOG_REPLAY_OPS-cluster3:36241, corePoolSize=10, maxPoolSize=10
[36mcluster3    |[0m 2017-01-17 19:48:37,825 DEBUG [M:0;cluster3:36241] executor.ExecutorService: Starting executor service name=MASTER_TABLE_OPERATIONS-cluster3:36241, corePoolSize=1, maxPoolSize=1
[36mcluster3    |[0m 2017-01-17 19:48:37,827 DEBUG [M:0;cluster3:36241] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner
[36mcluster3    |[0m 2017-01-17 19:48:37,828 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Process identifier=replicationLogCleaner connecting to ZooKeeper ensemble=localhost:18262
[36mcluster3    |[0m 2017-01-17 19:48:37,828 INFO  [M:0;cluster3:36241] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:18262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@5c99cc8e
[36mcluster3    |[0m 2017-01-17 19:48:37,829 INFO  [M:0;cluster3:36241-SendThread(localhost:18262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:18262. Will not attempt to authenticate using SASL (unknown error)
[36mcluster3    |[0m 2017-01-17 19:48:37,829 INFO  [M:0;cluster3:36241-SendThread(localhost:18262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:18262, initiating session
[36mcluster3    |[0m 2017-01-17 19:48:37,829 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:51892
[36mcluster3    |[0m 2017-01-17 19:48:37,829 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:51892
[36mcluster3    |[0m 2017-01-17 19:48:37,838 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0003 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:51892
[36mcluster3    |[0m 2017-01-17 19:48:37,839 INFO  [M:0;cluster3:36241-SendThread(localhost:18262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:18262, sessionid = 0x159adf94f4b0003, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:37,852 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0003 type:create cxid:0x2 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/hbase/replication Error:KeeperErrorCode = NoNode for /hbase/replication
[36mcluster3    |[0m 2017-01-17 19:48:37,861 DEBUG [M:0;cluster3:36241] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner
[36mcluster3    |[0m 2017-01-17 19:48:37,867 DEBUG [M:0;cluster3:36241] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner
[36mcluster3    |[0m 2017-01-17 19:48:37,870 DEBUG [M:0;cluster3:36241] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner
[36mcluster3    |[0m 2017-01-17 19:48:37,871 DEBUG [M:0;cluster3:36241] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner
[36mcluster3    |[0m 2017-01-17 19:48:37,872 DEBUG [M:0;cluster3:36241] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner
[36mcluster3    |[0m 2017-01-17 19:48:37,872 INFO  [M:0;cluster3:36241] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[36mcluster3    |[0m 2017-01-17 19:48:38,093 INFO  [FifoRpcScheduler.handler1-thread-1] master.ServerManager: Registering server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:38,096 INFO  [FifoRpcScheduler.handler1-thread-1] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[36mcluster3    |[0m 2017-01-17 19:48:38,106 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: Config from master: hbase.rootdir=file:///var/tmp
[36mcluster3    |[0m 2017-01-17 19:48:38,106 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: Config from master: fs.default.name=file:/
[36mcluster3    |[0m 2017-01-17 19:48:38,106 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: Config from master: hbase.master.info.port=60010
[36mcluster3    |[0m 2017-01-17 19:48:38,106 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: Master passed us a different hostname to use; was=0:0:0:0:0:0:0:0, but now=cluster3
[36mcluster3    |[0m 2017-01-17 19:48:38,112 DEBUG [main-EventThread] zookeeper.RegionServerTracker: Added tracking of RS /hbase/rs/cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:38,120 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.RegionServerCoprocessorHost: System coprocessor loading is enabled
[36mcluster3    |[0m 2017-01-17 19:48:38,120 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.RegionServerCoprocessorHost: Table coprocessor loading is enabled
[36mcluster3    |[0m 2017-01-17 19:48:38,123 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] fs.HFileSystem: The file system is not a DistributedFileSystem. Skipping on block location reordering
[36mcluster3    |[0m 2017-01-17 19:48:38,124 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: logdir=file:/var/tmp/WALs/cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:38,125 INFO  [M:0;cluster3:36241] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 253 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[36mcluster3    |[0m 2017-01-17 19:48:38,158 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.Replication: ReplicationStatisticsThread 300
[36mcluster3    |[0m 2017-01-17 19:48:38,161 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[36mcluster3    |[0m 2017-01-17 19:48:38,182 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] wal.FSHLog: New WAL /var/tmp/WALs/cluster3,42333,1484682515998/cluster3%2C42333%2C1484682515998.1484682518162
[36mcluster3    |[0m 2017-01-17 19:48:38,182 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=java.io.BufferedOutputStream
[36mcluster3    |[0m 2017-01-17 19:48:38,182 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=java.io.BufferedOutputStream
[36mcluster3    |[0m 2017-01-17 19:48:38,191 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
[36mcluster3    |[0m 2017-01-17 19:48:38,198 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-cluster3:42333, corePoolSize=3, maxPoolSize=3
[36mcluster3    |[0m 2017-01-17 19:48:38,199 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] executor.ExecutorService: Starting executor service name=RS_OPEN_META-cluster3:42333, corePoolSize=1, maxPoolSize=1
[36mcluster3    |[0m 2017-01-17 19:48:38,199 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] executor.ExecutorService: Starting executor service name=RS_OPEN_PRIORITY_REGION-cluster3:42333, corePoolSize=3, maxPoolSize=3
[36mcluster3    |[0m 2017-01-17 19:48:38,199 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-cluster3:42333, corePoolSize=3, maxPoolSize=3
[36mcluster3    |[0m 2017-01-17 19:48:38,199 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-cluster3:42333, corePoolSize=1, maxPoolSize=1
[36mcluster3    |[0m 2017-01-17 19:48:38,199 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-cluster3:42333, corePoolSize=2, maxPoolSize=2
[36mcluster3    |[0m 2017-01-17 19:48:38,209 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.ReplicationSourceManager: Current list of replicators: [cluster3,42333,1484682515998] other RSs: [cluster3,42333,1484682515998]
[36mcluster3    |[0m 2017-01-17 19:48:38,248 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[36mcluster3    |[0m 2017-01-17 19:48:38,303 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5da397e0 connecting to ZooKeeper ensemble=localhost:18262
[36mcluster3    |[0m 2017-01-17 19:48:38,303 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:18262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@38fbe57c
[36mcluster3    |[0m 2017-01-17 19:48:38,306 INFO  [RS:0;0:0:0:0:0:0:0:0:42333-SendThread(localhost:18262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:18262. Will not attempt to authenticate using SASL (unknown error)
[36mcluster3    |[0m 2017-01-17 19:48:38,307 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:51896
[36mcluster3    |[0m 2017-01-17 19:48:38,307 INFO  [RS:0;0:0:0:0:0:0:0:0:42333-SendThread(localhost:18262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:18262, initiating session
[36mcluster3    |[0m 2017-01-17 19:48:38,308 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:51896
[36mcluster3    |[0m 2017-01-17 19:48:38,312 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0004 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:51896
[36mcluster3    |[0m 2017-01-17 19:48:38,312 INFO  [RS:0;0:0:0:0:0:0:0:0:42333-SendThread(localhost:18262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:18262, sessionid = 0x159adf94f4b0004, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:38,319 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
[36mcluster3    |[0m 2017-01-17 19:48:38,319 INFO  [RpcServer.listener,port=42333] ipc.RpcServer: RpcServer.listener,port=42333: starting
[36mcluster3    |[0m 2017-01-17 19:48:38,320 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=0 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,320 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=1 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,320 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=2 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,321 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=3 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,321 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=4 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,321 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=5 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,322 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=6 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,323 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=7 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,323 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=8 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,324 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=9 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,324 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=10 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,324 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=11 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,324 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=12 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,325 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=13 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,326 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=14 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,328 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=15 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,328 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=16 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,328 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=17 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,329 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=18 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,331 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=19 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,332 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=20 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,332 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=21 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,332 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=22 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,332 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=23 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,336 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=24 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,336 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=25 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,337 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=26 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,337 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=27 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,339 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=28 queue=1
[36mcluster3    |[0m 2017-01-17 19:48:38,340 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: B.Default Start Handler index=29 queue=2
[36mcluster3    |[0m 2017-01-17 19:48:38,340 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=0 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,340 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=1 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,341 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=2 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,341 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=3 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,341 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=4 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,343 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=5 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,343 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=6 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,343 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=7 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,343 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=8 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,344 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Priority Start Handler index=9 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,344 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Replication Start Handler index=0 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,348 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Replication Start Handler index=1 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,348 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] ipc.RpcExecutor: Replication Start Handler index=2 queue=0
[36mcluster3    |[0m 2017-01-17 19:48:38,403 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[36mcluster3    |[0m 2017-01-17 19:48:38,416 INFO  [SplitLogWorker-cluster3,42333,1484682515998] regionserver.SplitLogWorker: SplitLogWorker cluster3,42333,1484682515998 starting
[36mcluster3    |[0m 2017-01-17 19:48:38,419 INFO  [SplitLogWorker-cluster3,42333,1484682515998] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x33f99548 connecting to ZooKeeper ensemble=localhost:18262
[36mcluster3    |[0m 2017-01-17 19:48:38,419 INFO  [SplitLogWorker-cluster3,42333,1484682515998] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:18262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@1c99af01
[36mcluster3    |[0m 2017-01-17 19:48:38,420 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] regionserver.HRegionServer: Serving as cluster3,42333,1484682515998, RpcServer on 0:0:0:0:0:0:0:0/0:0:0:0:0:0:0:0:42333, sessionid=0x159adf94f4b0001
[36mcluster3    |[0m 2017-01-17 19:48:38,420 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
[36mcluster3    |[0m 2017-01-17 19:48:38,420 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] snapshot.RegionServerSnapshotManager: Start Snapshot Manager cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:38,420 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] procedure.ZKProcedureMemberRpcs: Starting procedure member 'cluster3,42333,1484682515998'
[36mcluster3    |[0m 2017-01-17 19:48:38,420 INFO  [SplitLogWorker-cluster3,42333,1484682515998-SendThread(localhost:18262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:18262. Will not attempt to authenticate using SASL (unknown error)
[36mcluster3    |[0m 2017-01-17 19:48:38,420 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
[36mcluster3    |[0m 2017-01-17 19:48:38,421 INFO  [SplitLogWorker-cluster3,42333,1484682515998-SendThread(localhost:18262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:18262, initiating session
[36mcluster3    |[0m 2017-01-17 19:48:38,421 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:60540
[36mcluster3    |[0m 2017-01-17 19:48:38,431 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:60540
[36mcluster3    |[0m 2017-01-17 19:48:38,432 DEBUG [RS:0;0:0:0:0:0:0:0:0:42333] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
[36mcluster3    |[0m 2017-01-17 19:48:38,441 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0005 with negotiated timeout 10000 for client /127.0.0.1:60540
[36mcluster3    |[0m 2017-01-17 19:48:38,441 INFO  [SplitLogWorker-cluster3,42333,1484682515998-SendThread(localhost:18262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:18262, sessionid = 0x159adf94f4b0005, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:38,442 INFO  [RS:0;0:0:0:0:0:0:0:0:42333] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 2017-01-17 19:48:38,173 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:48:38,182 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.4 port: 42372 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:48:37,707 DEBUG [M:0;cluster1:37102] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/data/hbase/meta/.tabledesc/.tableinfo.0000000001
[33mcluster1    |[0m 2017-01-17 19:48:37,713 DEBUG [M:0;cluster1:37102] fs.HFileSystem: The file system is not a DistributedFileSystem. Skipping on block location reordering
[33mcluster1    |[0m 2017-01-17 19:48:37,727 DEBUG [M:0;cluster1:37102] master.SplitLogManager: Distributed log replay=false, hfile.format.version=2
[33mcluster1    |[0m 2017-01-17 19:48:37,734 INFO  [M:0;cluster1:37102] master.SplitLogManager: Timeout=120000, unassigned timeout=180000, distributedLogReplay=false
[33mcluster1    |[0m 2017-01-17 19:48:37,736 INFO  [M:0;cluster1:37102] master.SplitLogManager: Found 0 orphan tasks and 0 rescan nodes
[33mcluster1    |[0m 2017-01-17 19:48:37,736 DEBUG [M:0;cluster1:37102] util.FSTableDescriptors: Fetching table descriptors from the filesystem.
[33mcluster1    |[0m 2017-01-17 19:48:37,771 INFO  [M:0;cluster1:37102] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x2b815118 connecting to ZooKeeper ensemble=localhost:16262
[33mcluster1    |[0m 2017-01-17 19:48:37,771 INFO  [M:0;cluster1:37102] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:16262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@7b79f9d
[33mcluster1    |[0m 2017-01-17 19:48:37,771 INFO  [M:0;cluster1:37102-SendThread(localhost:16262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:16262. Will not attempt to authenticate using SASL (unknown error)
[33mcluster1    |[0m 2017-01-17 19:48:37,772 INFO  [M:0;cluster1:37102-SendThread(localhost:16262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:16262, initiating session
[33mcluster1    |[0m 2017-01-17 19:48:37,772 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:46496
[33mcluster1    |[0m 2017-01-17 19:48:37,772 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:46496
[33mcluster1    |[0m 2017-01-17 19:48:37,774 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40002 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:46496
[33mcluster1    |[0m 2017-01-17 19:48:37,774 INFO  [M:0;cluster1:37102-SendThread(localhost:16262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:16262, sessionid = 0x159adf94fa40002, negotiated timeout = 10000
[33mcluster1    |[0m 2017-01-17 19:48:37,794 DEBUG [M:0;cluster1:37102] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@562c4ce5
[33mcluster1    |[0m 2017-01-17 19:48:37,867 INFO  [M:0;cluster1:37102] master.HMaster: Server active/primary master=cluster1,37102,1484682515111, sessionid=0x159adf94fa40000, setting cluster-up flag (Was=false)
[33mcluster1    |[0m 2017-01-17 19:48:37,867 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@7f131719
[33mcluster1    |[0m 2017-01-17 19:48:37,872 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: ClusterId : 19a8f54f-f204-4d89-9d33-a3b3330d1d36
[33mcluster1    |[0m 2017-01-17 19:48:37,876 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
[33mcluster1    |[0m 2017-01-17 19:48:37,880 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40001 type:create cxid:0x8 zxid:0x11 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot Error:KeeperErrorCode = NoNode for /hbase/online-snapshot
[33mcluster1    |[0m 2017-01-17 19:48:37,885 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40001 type:create cxid:0xa zxid:0x14 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/acquired Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/acquired
[33mcluster1    |[0m 2017-01-17 19:48:37,887 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
[33mcluster1    |[0m 2017-01-17 19:48:37,889 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40001 type:create cxid:0xc zxid:0x16 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/reached Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/reached
[33mcluster1    |[0m 2017-01-17 19:48:37,891 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/reached already exists and this is not a retry
[33mcluster1    |[0m 2017-01-17 19:48:37,892 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40000 type:create cxid:0x2e zxid:0x18 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/abort Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/abort
[33mcluster1    |[0m 2017-01-17 19:48:37,895 INFO  [M:0;cluster1:37102] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/abort already exists and this is not a retry
[33mcluster1    |[0m 2017-01-17 19:48:37,895 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
[33mcluster1    |[0m 2017-01-17 19:48:37,895 INFO  [M:0;cluster1:37102] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/online-snapshot/acquired /hbase/online-snapshot/reached /hbase/online-snapshot/abort
[33mcluster1    |[0m 2017-01-17 19:48:37,897 DEBUG [M:0;cluster1:37102] procedure.ZKProcedureCoordinatorRpcs: Starting the controller for procedure member:cluster1,37102,1484682515111
[33mcluster1    |[0m 2017-01-17 19:48:37,899 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7 M, globalMemStoreLimitLowMark=367.3 M, maxHeap=966.7 M
[33mcluster1    |[0m 2017-01-17 19:48:37,907 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: CompactionChecker runs every 10sec
[33mcluster1    |[0m 2017-01-17 19:48:37,912 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: reportForDuty to master=cluster1,37102,1484682515111 with port=38372, startcode=1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:37,926 INFO  [M:0;cluster1:37102] master.MasterCoprocessorHost: System coprocessor loading is enabled
[33mcluster1    |[0m 2017-01-17 19:48:37,932 DEBUG [M:0;cluster1:37102] executor.ExecutorService: Starting executor service name=MASTER_OPEN_REGION-cluster1:37102, corePoolSize=5, maxPoolSize=5
[33mcluster1    |[0m 2017-01-17 19:48:37,932 DEBUG [M:0;cluster1:37102] executor.ExecutorService: Starting executor service name=MASTER_CLOSE_REGION-cluster1:37102, corePoolSize=5, maxPoolSize=5
[33mcluster1    |[0m 2017-01-17 19:48:37,932 DEBUG [M:0;cluster1:37102] executor.ExecutorService: Starting executor service name=MASTER_SERVER_OPERATIONS-cluster1:37102, corePoolSize=5, maxPoolSize=5
[33mcluster1    |[0m 2017-01-17 19:48:37,932 DEBUG [M:0;cluster1:37102] executor.ExecutorService: Starting executor service name=MASTER_META_SERVER_OPERATIONS-cluster1:37102, corePoolSize=5, maxPoolSize=5
[33mcluster1    |[0m 2017-01-17 19:48:37,932 DEBUG [M:0;cluster1:37102] executor.ExecutorService: Starting executor service name=M_LOG_REPLAY_OPS-cluster1:37102, corePoolSize=10, maxPoolSize=10
[33mcluster1    |[0m 2017-01-17 19:48:37,932 DEBUG [M:0;cluster1:37102] executor.ExecutorService: Starting executor service name=MASTER_TABLE_OPERATIONS-cluster1:37102, corePoolSize=1, maxPoolSize=1
[33mcluster1    |[0m 2017-01-17 19:48:37,939 DEBUG [M:0;cluster1:37102] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner
[33mcluster1    |[0m 2017-01-17 19:48:37,941 INFO  [M:0;cluster1:37102] zookeeper.RecoverableZooKeeper: Process identifier=replicationLogCleaner connecting to ZooKeeper ensemble=localhost:16262
[33mcluster1    |[0m 2017-01-17 19:48:37,941 INFO  [M:0;cluster1:37102] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:16262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@3c2ae8f6
[33mcluster1    |[0m 2017-01-17 19:48:37,958 INFO  [M:0;cluster1:37102-SendThread(localhost:16262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:16262. Will not attempt to authenticate using SASL (unknown error)
[33mcluster1    |[0m 2017-01-17 19:48:37,959 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:57192
[33mcluster1    |[0m 2017-01-17 19:48:37,967 INFO  [M:0;cluster1:37102-SendThread(localhost:16262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:16262, initiating session
[33mcluster1    |[0m 2017-01-17 19:48:37,968 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:57192
[33mcluster1    |[0m 2017-01-17 19:48:37,970 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40003 with negotiated timeout 10000 for client /127.0.0.1:57192
[33mcluster1    |[0m 2017-01-17 19:48:37,970 INFO  [M:0;cluster1:37102-SendThread(localhost:16262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:16262, sessionid = 0x159adf94fa40003, negotiated timeout = 10000
[33mcluster1    |[0m 2017-01-17 19:48:37,972 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40003 type:create cxid:0x2 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/hbase/replication Error:KeeperErrorCode = NoNode for /hbase/replication
[33mcluster1    |[0m 2017-01-17 19:48:37,981 DEBUG [M:0;cluster1:37102] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner
[33mcluster1    |[0m 2017-01-17 19:48:37,984 DEBUG [M:0;cluster1:37102] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner
[33mcluster1    |[0m 2017-01-17 19:48:37,987 DEBUG [M:0;cluster1:37102] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner
[33mcluster1    |[0m 2017-01-17 19:48:37,988 DEBUG [M:0;cluster1:37102] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner
[33mcluster1    |[0m 2017-01-17 19:48:37,989 DEBUG [M:0;cluster1:37102] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner
[33mcluster1    |[0m 2017-01-17 19:48:37,990 INFO  [M:0;cluster1:37102] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[33mcluster1    |[0m 2017-01-17 19:48:38,227 INFO  [FifoRpcScheduler.handler1-thread-1] master.ServerManager: Registering server=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:38,230 INFO  [M:0;cluster1:37102] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 240 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[33mcluster1    |[0m 2017-01-17 19:48:38,235 INFO  [FifoRpcScheduler.handler1-thread-1] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[33mcluster1    |[0m 2017-01-17 19:48:38,253 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: Config from master: hbase.rootdir=file:///var/tmp
[33mcluster1    |[0m 2017-01-17 19:48:38,254 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: Config from master: fs.default.name=file:/
[33mcluster1    |[0m 2017-01-17 19:48:38,254 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: Config from master: hbase.master.info.port=60010
[33mcluster1    |[0m 2017-01-17 19:48:38,254 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: Master passed us a different hostname to use; was=0:0:0:0:0:0:0:0, but now=cluster1
[33mcluster1    |[0m 2017-01-17 19:48:38,267 DEBUG [main-EventThread] zookeeper.RegionServerTracker: Added tracking of RS /hbase/rs/cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:38,279 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.RegionServerCoprocessorHost: System coprocessor loading is enabled
[33mcluster1    |[0m 2017-01-17 19:48:38,279 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.RegionServerCoprocessorHost: Table coprocessor loading is enabled
[33mcluster1    |[0m 2017-01-17 19:48:38,287 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] fs.HFileSystem: The file system is not a DistributedFileSystem. Skipping on block location reordering
[33mcluster1    |[0m 2017-01-17 19:48:38,288 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: logdir=file:/var/tmp/WALs/cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:38,328 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.Replication: ReplicationStatisticsThread 300
[33mcluster1    |[0m 2017-01-17 19:48:38,330 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[33mcluster1    |[0m 2017-01-17 19:48:38,342 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] wal.FSHLog: New WAL /var/tmp/WALs/cluster1,38372,1484682516063/cluster1%2C38372%2C1484682516063.1484682518332
[33mcluster1    |[0m 2017-01-17 19:48:38,342 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=java.io.BufferedOutputStream
[33mcluster1    |[0m 2017-01-17 19:48:38,342 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=java.io.BufferedOutputStream
[33mcluster1    |[0m 2017-01-17 19:48:38,354 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
[33mcluster1    |[0m 2017-01-17 19:48:38,367 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-cluster1:38372, corePoolSize=3, maxPoolSize=3
[33mcluster1    |[0m 2017-01-17 19:48:38,367 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] executor.ExecutorService: Starting executor service name=RS_OPEN_META-cluster1:38372, corePoolSize=1, maxPoolSize=1
[33mcluster1    |[0m 2017-01-17 19:48:38,367 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] executor.ExecutorService: Starting executor service name=RS_OPEN_PRIORITY_REGION-cluster1:38372, corePoolSize=3, maxPoolSize=3
[33mcluster1    |[0m 2017-01-17 19:48:38,367 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-cluster1:38372, corePoolSize=3, maxPoolSize=3
[33mcluster1    |[0m 2017-01-17 19:48:38,368 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-cluster1:38372, corePoolSize=1, maxPoolSize=1
[33mcluster1    |[0m 2017-01-17 19:48:38,368 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-cluster1:38372, corePoolSize=2, maxPoolSize=2
[33mcluster1    |[0m 2017-01-17 19:48:38,379 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.ReplicationSourceManager: Current list of replicators: [cluster1,38372,1484682516063] other RSs: [cluster1,38372,1484682516063]
[33mcluster1    |[0m 2017-01-17 19:48:38,439 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[33mcluster1    |[0m 2017-01-17 19:48:38,454 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6cfc3629 connecting to ZooKeeper ensemble=localhost:16262
[33mcluster1    |[0m 2017-01-17 19:48:38,454 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:16262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@22503538
[33mcluster1    |[0m 2017-01-17 19:48:38,455 INFO  [RS:0;0:0:0:0:0:0:0:0:38372-SendThread(localhost:16262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:16262. Will not attempt to authenticate using SASL (unknown error)
[33mcluster1    |[0m 2017-01-17 19:48:38,456 INFO  [RS:0;0:0:0:0:0:0:0:0:38372-SendThread(localhost:16262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:16262, initiating session
[33mcluster1    |[0m 2017-01-17 19:48:38,458 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:57198
[33mcluster1    |[0m 2017-01-17 19:48:38,458 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:57198
[33mcluster1    |[0m 2017-01-17 19:48:38,465 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40004 with negotiated timeout 10000 for client /127.0.0.1:57198
[33mcluster1    |[0m 2017-01-17 19:48:38,472 INFO  [RS:0;0:0:0:0:0:0:0:0:38372-SendThread(localhost:16262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:16262, sessionid = 0x159adf94fa40004, negotiated timeout = 10000
[33mcluster1    |[0m 2017-01-17 19:48:38,476 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
[33mcluster1    |[0m 2017-01-17 19:48:38,476 INFO  [RpcServer.listener,port=38372] ipc.RpcServer: RpcServer.listener,port=38372: starting
[33mcluster1    |[0m 2017-01-17 19:48:38,477 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=0 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,477 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=1 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,477 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=2 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,478 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=3 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,478 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=4 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,478 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=5 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,478 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=6 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,479 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=7 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,479 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=8 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,479 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=9 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,479 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=10 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,479 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=11 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,480 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=12 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,480 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=13 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,482 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=14 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,482 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=15 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,483 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=16 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,484 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=17 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,484 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=18 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,484 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=19 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,485 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=20 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,485 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=21 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,485 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=22 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,486 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=23 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,486 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=24 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,486 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=25 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,486 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=26 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,486 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=27 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,487 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=28 queue=1
[33mcluster1    |[0m 2017-01-17 19:48:38,487 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: B.Default Start Handler index=29 queue=2
[33mcluster1    |[0m 2017-01-17 19:48:38,487 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=0 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,487 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=1 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,487 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=2 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,488 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=3 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,488 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=4 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,488 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=5 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,488 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=6 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,489 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=7 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,489 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=8 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,489 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Priority Start Handler index=9 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,489 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Replication Start Handler index=0 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,489 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Replication Start Handler index=1 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,490 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] ipc.RpcExecutor: Replication Start Handler index=2 queue=0
[33mcluster1    |[0m 2017-01-17 19:48:38,522 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[33mcluster1    |[0m 2017-01-17 19:48:38,534 INFO  [SplitLogWorker-cluster1,38372,1484682516063] regionserver.SplitLogWorker: SplitLogWorker cluster1,38372,1484682516063 starting
[33mcluster1    |[0m 2017-01-17 19:48:38,536 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] regionserver.HRegionServer: Serving as cluster1,38372,1484682516063, RpcServer on 0:0:0:0:0:0:0:0/0:0:0:0:0:0:0:0:38372, sessionid=0x159adf94fa40001
[33mcluster1    |[0m 2017-01-17 19:48:38,536 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
[33mcluster1    |[0m 2017-01-17 19:48:38,536 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] snapshot.RegionServerSnapshotManager: Start Snapshot Manager cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:38,536 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] procedure.ZKProcedureMemberRpcs: Starting procedure member 'cluster1,38372,1484682516063'
[33mcluster1    |[0m 2017-01-17 19:48:38,536 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
[33mcluster1    |[0m 2017-01-17 19:48:38,536 INFO  [SplitLogWorker-cluster1,38372,1484682516063] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x17249727 connecting to ZooKeeper ensemble=localhost:16262
[33mcluster1    |[0m 2017-01-17 19:48:38,537 INFO  [SplitLogWorker-cluster1,38372,1484682516063] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:16262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@71effafc
[33mcluster1    |[0m 2017-01-17 19:48:38,537 DEBUG [RS:0;0:0:0:0:0:0:0:0:38372] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
[33mcluster1    |[0m 2017-01-17 19:48:38,538 INFO  [RS:0;0:0:0:0:0:0:0:0:38372] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
[33mcluster1    |[0m 2017-01-17 19:48:38,538 INFO  [SplitLogWorker-cluster1,38372,1484682516063-SendThread(localhost:16262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:16262. Will not attempt to authenticate using SASL (unknown error)
[33mcluster1    |[0m 2017-01-17 19:48:38,539 INFO  [SplitLogWorker-cluster1,38372,1484682516063-SendThread(localhost:16262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:16262, initiating session
[33mcluster1    |[0m 2017-01-17 19:48:38,539 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:46508
[33mcluster1    |[0m 2017-01-17 19:48:38,540 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:46508
[33mcluster1    |[0m 2017-01-17 19:48:38,542 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40005 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:46508
[33mcluster1    |[0m 2017-01-17 19:48:38,543 INFO  [SplitLogWorker-cluster1,38372,1484682516063-SendThread(localhost:16262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:16262, sessionid = 0x159adf94fa40005, negotiated timeout = 10000
[32mcluster2    |[0m 2017-01-17 19:48:37,786 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
[32mcluster2    |[0m 2017-01-17 19:48:37,840 INFO  [main] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
[32mcluster2    |[0m 2017-01-17 19:48:37,844 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context regionserver
[32mcluster2    |[0m 2017-01-17 19:48:37,844 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[32mcluster2    |[0m 2017-01-17 19:48:37,857 INFO  [main] http.HttpServer: Jetty bound to port 41682
[32mcluster2    |[0m 2017-01-17 19:48:37,857 INFO  [main] mortbay.log: jetty-6.1.26
[32mcluster2    |[0m 2017-01-17 19:48:38,303 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:41682
[32mcluster2    |[0m 2017-01-17 19:48:38,325 INFO  [M:0;cluster2:42021] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
[32mcluster2    |[0m 2017-01-17 19:48:38,327 INFO  [M:0;cluster2:42021] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context master
[32mcluster2    |[0m 2017-01-17 19:48:38,327 INFO  [M:0;cluster2:42021] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
[32mcluster2    |[0m 2017-01-17 19:48:38,333 INFO  [M:0;cluster2:42021] http.HttpServer: Jetty bound to port 60010
[32mcluster2    |[0m 2017-01-17 19:48:38,334 INFO  [M:0;cluster2:42021] mortbay.log: jetty-6.1.26
[32mcluster2    |[0m 2017-01-17 19:48:38,586 INFO  [M:0;cluster2:42021] mortbay.log: Started SelectChannelConnector@0.0.0.0:60010
[32mcluster2    |[0m 2017-01-17 19:48:38,709 DEBUG [main-EventThread] master.ActiveMasterManager: A master is now available
[32mcluster2    |[0m 2017-01-17 19:48:38,709 INFO  [M:0;cluster2:42021] master.ActiveMasterManager: Registered Active Master=cluster2,42021,1484682516560
[32mcluster2    |[0m 2017-01-17 19:48:38,717 INFO  [M:0;cluster2:42021] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/SecurityAuth.audit <==
[32mcluster2    |[0m 2017-01-17 19:48:39,721 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:48:39,731 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.2 port: 35457 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:48:38,773 INFO  [M:0;cluster2:42021] util.FSUtils: Created version file at file:/var/tmp with version=8
[32mcluster2    |[0m 2017-01-17 19:48:38,783 DEBUG [M:0;cluster2:42021] util.FSUtils: Created cluster ID file at file:/var/tmp/hbase.id with ID: 5a37a10f-39ec-4937-83d4-2f075d2382c7
[32mcluster2    |[0m 2017-01-17 19:48:38,794 INFO  [M:0;cluster2:42021] master.MasterFileSystem: BOOTSTRAP: creating hbase:meta region
[32mcluster2    |[0m 2017-01-17 19:48:38,811 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:RS:0;0:0:0:0:0:0:0:0:41916
[32mcluster2    |[0m 2017-01-17 19:48:38,811 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:41916 connecting to ZooKeeper ensemble=localhost:17262
[32mcluster2    |[0m 2017-01-17 19:48:38,811 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:17262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@790b1ba
[32mcluster2    |[0m 2017-01-17 19:48:38,812 INFO  [RS:0;0:0:0:0:0:0:0:0:41916-SendThread(localhost:17262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:17262. Will not attempt to authenticate using SASL (unknown error)
[32mcluster2    |[0m 2017-01-17 19:48:38,813 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:56114
[32mcluster2    |[0m 2017-01-17 19:48:38,813 INFO  [RS:0;0:0:0:0:0:0:0:0:41916-SendThread(localhost:17262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:17262, initiating session
[32mcluster2    |[0m 2017-01-17 19:48:38,814 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:56114
[32mcluster2    |[0m 2017-01-17 19:48:38,816 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0001 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:56114
[32mcluster2    |[0m 2017-01-17 19:48:38,816 INFO  [RS:0;0:0:0:0:0:0:0:0:41916-SendThread(localhost:17262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:17262, sessionid = 0x159adf9552f0001, negotiated timeout = 10000
[32mcluster2    |[0m 2017-01-17 19:48:38,841 INFO  [M:0;cluster2:42021] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[32mcluster2    |[0m 2017-01-17 19:48:39,010 INFO  [M:0;cluster2:42021] regionserver.HRegion: creating HRegion hbase:meta HTD == 'hbase:meta', {TABLE_ATTRIBUTES => {IS_META => 'true', coprocessor$1 => '|org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint|536870911|'}, {NAME => 'info', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => 'FOREVER', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '8192', IN_MEMORY => 'false', BLOCKCACHE => 'false'} RootDir = file:/var/tmp Table name == hbase:meta
[32mcluster2    |[0m 2017-01-17 19:48:39,047 INFO  [M:0;cluster2:42021] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[32mcluster2    |[0m 2017-01-17 19:48:39,075 INFO  [M:0;cluster2:42021] wal.FSHLog: New WAL /var/tmp/data/hbase/meta/1588230740/WALs/hlog.1484682519050
[32mcluster2    |[0m 2017-01-17 19:48:39,076 INFO  [M:0;cluster2:42021] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer
[32mcluster2    |[0m 2017-01-17 19:48:39,076 INFO  [M:0;cluster2:42021] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer
[32mcluster2    |[0m 2017-01-17 19:48:39,111 DEBUG [M:0;cluster2:42021] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:48:39,179 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=false] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[32mcluster2    |[0m 2017-01-17 19:48:39,195 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[32mcluster2    |[0m 2017-01-17 19:48:39,218 INFO  [StoreOpener-1588230740-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
[32mcluster2    |[0m 2017-01-17 19:48:39,220 INFO  [StoreOpener-1588230740-1] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
[32mcluster2    |[0m 2017-01-17 19:48:39,225 DEBUG [M:0;cluster2:42021] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/meta/1588230740
[32mcluster2    |[0m 2017-01-17 19:48:39,229 INFO  [M:0;cluster2:42021] regionserver.HRegion: Onlined 1588230740; next sequenceid=1
[32mcluster2    |[0m 2017-01-17 19:48:39,229 DEBUG [M:0;cluster2:42021] regionserver.HRegion: Closing hbase:meta,,1.1588230740: disabling compactions & flushes
[32mcluster2    |[0m 2017-01-17 19:48:39,229 DEBUG [M:0;cluster2:42021] regionserver.HRegion: Updates disabled for region hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:48:39,231 INFO  [StoreCloserThread-hbase:meta,,1.1588230740-1] regionserver.HStore: Closed info
[32mcluster2    |[0m 2017-01-17 19:48:39,232 INFO  [M:0;cluster2:42021] regionserver.HRegion: Closed hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:48:39,232 DEBUG [M:0;cluster2:42021-WAL.AsyncNotifier] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
[32mcluster2    |[0m 2017-01-17 19:48:39,232 INFO  [M:0;cluster2:42021-WAL.AsyncNotifier] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncNotifier exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,232 DEBUG [M:0;cluster2:42021-WAL.AsyncSyncer0] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
[32mcluster2    |[0m 2017-01-17 19:48:39,233 INFO  [M:0;cluster2:42021-WAL.AsyncSyncer0] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer0 exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,233 DEBUG [M:0;cluster2:42021-WAL.AsyncSyncer1] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
[32mcluster2    |[0m 2017-01-17 19:48:39,233 INFO  [M:0;cluster2:42021-WAL.AsyncSyncer1] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer1 exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,233 DEBUG [M:0;cluster2:42021-WAL.AsyncSyncer2] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
[32mcluster2    |[0m 2017-01-17 19:48:39,233 INFO  [M:0;cluster2:42021-WAL.AsyncSyncer2] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer2 exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,233 DEBUG [M:0;cluster2:42021-WAL.AsyncSyncer3] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
[32mcluster2    |[0m 2017-01-17 19:48:39,233 INFO  [M:0;cluster2:42021-WAL.AsyncSyncer3] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer3 exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,233 DEBUG [M:0;cluster2:42021-WAL.AsyncSyncer4] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
[32mcluster2    |[0m 2017-01-17 19:48:39,233 INFO  [M:0;cluster2:42021-WAL.AsyncSyncer4] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncSyncer4 exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,233 DEBUG [M:0;cluster2:42021-WAL.AsyncWriter] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
[32mcluster2    |[0m 2017-01-17 19:48:39,234 INFO  [M:0;cluster2:42021-WAL.AsyncWriter] wal.FSHLog: M:0;cluster2:42021-WAL.AsyncWriter exiting
[32mcluster2    |[0m 2017-01-17 19:48:39,234 DEBUG [M:0;cluster2:42021] wal.FSHLog: Closing WAL writer in file:/var/tmp/data/hbase/meta/1588230740/WALs
[32mcluster2    |[0m 2017-01-17 19:48:39,236 DEBUG [M:0;cluster2:42021] wal.FSHLog: Moved 1 WAL file(s) to /var/tmp/data/hbase/meta/1588230740/oldWALs
[32mcluster2    |[0m 2017-01-17 19:48:39,260 DEBUG [M:0;cluster2:42021] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/data/hbase/meta/.tabledesc/.tableinfo.0000000001
[32mcluster2    |[0m 2017-01-17 19:48:39,267 DEBUG [M:0;cluster2:42021] fs.HFileSystem: The file system is not a DistributedFileSystem. Skipping on block location reordering
[32mcluster2    |[0m 2017-01-17 19:48:39,278 DEBUG [M:0;cluster2:42021] master.SplitLogManager: Distributed log replay=false, hfile.format.version=2
[32mcluster2    |[0m 2017-01-17 19:48:39,281 INFO  [M:0;cluster2:42021] master.SplitLogManager: Timeout=120000, unassigned timeout=180000, distributedLogReplay=false
[32mcluster2    |[0m 2017-01-17 19:48:39,283 INFO  [M:0;cluster2:42021] master.SplitLogManager: Found 0 orphan tasks and 0 rescan nodes
[32mcluster2    |[0m 2017-01-17 19:48:39,284 DEBUG [M:0;cluster2:42021] util.FSTableDescriptors: Fetching table descriptors from the filesystem.
[32mcluster2    |[0m 2017-01-17 19:48:39,321 INFO  [M:0;cluster2:42021] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1055e4f3 connecting to ZooKeeper ensemble=localhost:17262
[32mcluster2    |[0m 2017-01-17 19:48:39,321 INFO  [M:0;cluster2:42021] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:17262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@6df8952f
[32mcluster2    |[0m 2017-01-17 19:48:39,322 INFO  [M:0;cluster2:42021-SendThread(localhost:17262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:17262. Will not attempt to authenticate using SASL (unknown error)
[32mcluster2    |[0m 2017-01-17 19:48:39,322 INFO  [M:0;cluster2:42021-SendThread(localhost:17262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:17262, initiating session
[32mcluster2    |[0m 2017-01-17 19:48:39,322 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:56116
[32mcluster2    |[0m 2017-01-17 19:48:39,323 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:56116
[32mcluster2    |[0m 2017-01-17 19:48:39,325 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0002 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:56116
[32mcluster2    |[0m 2017-01-17 19:48:39,325 INFO  [M:0;cluster2:42021-SendThread(localhost:17262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:17262, sessionid = 0x159adf9552f0002, negotiated timeout = 10000
[32mcluster2    |[0m 2017-01-17 19:48:39,350 DEBUG [M:0;cluster2:42021] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@71999260
[32mcluster2    |[0m 2017-01-17 19:48:39,424 INFO  [M:0;cluster2:42021] master.HMaster: Server active/primary master=cluster2,42021,1484682516560, sessionid=0x159adf9552f0000, setting cluster-up flag (Was=false)
[32mcluster2    |[0m 2017-01-17 19:48:39,425 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@359fe30c
[32mcluster2    |[0m 2017-01-17 19:48:39,428 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: ClusterId : 5a37a10f-39ec-4937-83d4-2f075d2382c7
[32mcluster2    |[0m 2017-01-17 19:48:39,434 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
[32mcluster2    |[0m 2017-01-17 19:48:39,441 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0001 type:create cxid:0x8 zxid:0x11 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot Error:KeeperErrorCode = NoNode for /hbase/online-snapshot
[32mcluster2    |[0m 2017-01-17 19:48:39,446 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0001 type:create cxid:0xa zxid:0x14 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/acquired Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/acquired
[32mcluster2    |[0m 2017-01-17 19:48:39,449 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
[32mcluster2    |[0m 2017-01-17 19:48:39,450 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0001 type:create cxid:0xc zxid:0x16 txntype:-1 reqpath:n/a Error Path:/hbase/online-snapshot/reached Error:KeeperErrorCode = NodeExists for /hbase/online-snapshot/reached
[32mcluster2    |[0m 2017-01-17 19:48:39,453 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/reached already exists and this is not a retry
[32mcluster2    |[0m 2017-01-17 19:48:39,456 INFO  [M:0;cluster2:42021] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/online-snapshot/acquired /hbase/online-snapshot/reached /hbase/online-snapshot/abort
[32mcluster2    |[0m 2017-01-17 19:48:39,458 DEBUG [M:0;cluster2:42021] procedure.ZKProcedureCoordinatorRpcs: Starting the controller for procedure member:cluster2,42021,1484682516560
[32mcluster2    |[0m 2017-01-17 19:48:39,459 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
[32mcluster2    |[0m 2017-01-17 19:48:39,466 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7 M, globalMemStoreLimitLowMark=367.3 M, maxHeap=966.7 M
[32mcluster2    |[0m 2017-01-17 19:48:39,475 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: CompactionChecker runs every 10sec
[32mcluster2    |[0m 2017-01-17 19:48:39,481 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: reportForDuty to master=cluster2,42021,1484682516560 with port=41916, startcode=1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:39,488 INFO  [M:0;cluster2:42021] master.MasterCoprocessorHost: System coprocessor loading is enabled
[32mcluster2    |[0m 2017-01-17 19:48:39,493 DEBUG [M:0;cluster2:42021] executor.ExecutorService: Starting executor service name=MASTER_OPEN_REGION-cluster2:42021, corePoolSize=5, maxPoolSize=5
[32mcluster2    |[0m 2017-01-17 19:48:39,494 DEBUG [M:0;cluster2:42021] executor.ExecutorService: Starting executor service name=MASTER_CLOSE_REGION-cluster2:42021, corePoolSize=5, maxPoolSize=5
[32mcluster2    |[0m 2017-01-17 19:48:39,494 DEBUG [M:0;cluster2:42021] executor.ExecutorService: Starting executor service name=MASTER_SERVER_OPERATIONS-cluster2:42021, corePoolSize=5, maxPoolSize=5
[32mcluster2    |[0m 2017-01-17 19:48:39,494 DEBUG [M:0;cluster2:42021] executor.ExecutorService: Starting executor service name=MASTER_META_SERVER_OPERATIONS-cluster2:42021, corePoolSize=5, maxPoolSize=5
[32mcluster2    |[0m 2017-01-17 19:48:39,494 DEBUG [M:0;cluster2:42021] executor.ExecutorService: Starting executor service name=M_LOG_REPLAY_OPS-cluster2:42021, corePoolSize=10, maxPoolSize=10
[32mcluster2    |[0m 2017-01-17 19:48:39,494 DEBUG [M:0;cluster2:42021] executor.ExecutorService: Starting executor service name=MASTER_TABLE_OPERATIONS-cluster2:42021, corePoolSize=1, maxPoolSize=1
[32mcluster2    |[0m 2017-01-17 19:48:39,496 DEBUG [M:0;cluster2:42021] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner
[32mcluster2    |[0m 2017-01-17 19:48:39,498 INFO  [M:0;cluster2:42021] zookeeper.RecoverableZooKeeper: Process identifier=replicationLogCleaner connecting to ZooKeeper ensemble=localhost:17262
[32mcluster2    |[0m 2017-01-17 19:48:39,498 INFO  [M:0;cluster2:42021] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:17262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@3a4961d6
[32mcluster2    |[0m 2017-01-17 19:48:39,499 INFO  [M:0;cluster2:42021-SendThread(localhost:17262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:17262. Will not attempt to authenticate using SASL (unknown error)
[32mcluster2    |[0m 2017-01-17 19:48:39,499 INFO  [M:0;cluster2:42021-SendThread(localhost:17262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:17262, initiating session
[32mcluster2    |[0m 2017-01-17 19:48:39,500 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:49016
[32mcluster2    |[0m 2017-01-17 19:48:39,500 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:49016
[32mcluster2    |[0m 2017-01-17 19:48:39,507 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0003 with negotiated timeout 10000 for client /127.0.0.1:49016
[32mcluster2    |[0m 2017-01-17 19:48:39,507 INFO  [M:0;cluster2:42021-SendThread(localhost:17262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:17262, sessionid = 0x159adf9552f0003, negotiated timeout = 10000
[32mcluster2    |[0m 2017-01-17 19:48:39,512 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0003 type:create cxid:0x2 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/hbase/replication Error:KeeperErrorCode = NoNode for /hbase/replication
[32mcluster2    |[0m 2017-01-17 19:48:39,521 DEBUG [M:0;cluster2:42021] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner
[32mcluster2    |[0m 2017-01-17 19:48:39,525 DEBUG [M:0;cluster2:42021] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner
[32mcluster2    |[0m 2017-01-17 19:48:39,527 DEBUG [M:0;cluster2:42021] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner
[32mcluster2    |[0m 2017-01-17 19:48:39,528 DEBUG [M:0;cluster2:42021] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner
[32mcluster2    |[0m 2017-01-17 19:48:39,529 DEBUG [M:0;cluster2:42021] cleaner.CleanerChore: initialize cleaner=org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner
[32mcluster2    |[0m 2017-01-17 19:48:39,530 INFO  [M:0;cluster2:42021] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[36mcluster3    |[0m 2017-01-17 19:48:39,640 INFO  [M:0;cluster3:36241] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 1768 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[33mcluster1    |[0m 2017-01-17 19:48:39,737 INFO  [M:0;cluster1:37102] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 1747 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[32mcluster2    |[0m 2017-01-17 19:48:39,758 INFO  [FifoRpcScheduler.handler1-thread-1] master.ServerManager: Registering server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:39,762 INFO  [FifoRpcScheduler.handler1-thread-1] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[32mcluster2    |[0m 2017-01-17 19:48:39,773 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: Config from master: hbase.rootdir=file:///var/tmp
[32mcluster2    |[0m 2017-01-17 19:48:39,773 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: Config from master: fs.default.name=file:/
[32mcluster2    |[0m 2017-01-17 19:48:39,773 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: Config from master: hbase.master.info.port=60010
[32mcluster2    |[0m 2017-01-17 19:48:39,773 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: Master passed us a different hostname to use; was=0:0:0:0:0:0:0:0, but now=cluster2
[32mcluster2    |[0m 2017-01-17 19:48:39,779 DEBUG [main-EventThread] zookeeper.RegionServerTracker: Added tracking of RS /hbase/rs/cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:39,780 INFO  [M:0;cluster2:42021] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 250 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[32mcluster2    |[0m 2017-01-17 19:48:39,783 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.RegionServerCoprocessorHost: System coprocessor loading is enabled
[32mcluster2    |[0m 2017-01-17 19:48:39,784 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.RegionServerCoprocessorHost: Table coprocessor loading is enabled
[32mcluster2    |[0m 2017-01-17 19:48:39,787 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] fs.HFileSystem: The file system is not a DistributedFileSystem. Skipping on block location reordering
[32mcluster2    |[0m 2017-01-17 19:48:39,788 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: logdir=file:/var/tmp/WALs/cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:39,809 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.Replication: ReplicationStatisticsThread 300
[32mcluster2    |[0m 2017-01-17 19:48:39,811 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[32mcluster2    |[0m 2017-01-17 19:48:39,820 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] wal.FSHLog: New WAL /var/tmp/WALs/cluster2,41916,1484682517717/cluster2%2C41916%2C1484682517717.1484682519812
[32mcluster2    |[0m 2017-01-17 19:48:39,820 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=java.io.BufferedOutputStream
[32mcluster2    |[0m 2017-01-17 19:48:39,820 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=java.io.BufferedOutputStream
[32mcluster2    |[0m 2017-01-17 19:48:39,827 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
[32mcluster2    |[0m 2017-01-17 19:48:39,833 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-cluster2:41916, corePoolSize=3, maxPoolSize=3
[32mcluster2    |[0m 2017-01-17 19:48:39,834 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] executor.ExecutorService: Starting executor service name=RS_OPEN_META-cluster2:41916, corePoolSize=1, maxPoolSize=1
[32mcluster2    |[0m 2017-01-17 19:48:39,834 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] executor.ExecutorService: Starting executor service name=RS_OPEN_PRIORITY_REGION-cluster2:41916, corePoolSize=3, maxPoolSize=3
[32mcluster2    |[0m 2017-01-17 19:48:39,834 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-cluster2:41916, corePoolSize=3, maxPoolSize=3
[32mcluster2    |[0m 2017-01-17 19:48:39,834 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-cluster2:41916, corePoolSize=1, maxPoolSize=1
[32mcluster2    |[0m 2017-01-17 19:48:39,834 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-cluster2:41916, corePoolSize=2, maxPoolSize=2
[32mcluster2    |[0m 2017-01-17 19:48:39,838 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.ReplicationSourceManager: Current list of replicators: [cluster2,41916,1484682517717] other RSs: [cluster2,41916,1484682517717]
[32mcluster2    |[0m 2017-01-17 19:48:39,865 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[32mcluster2    |[0m 2017-01-17 19:48:39,876 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1ad3c643 connecting to ZooKeeper ensemble=localhost:17262
[32mcluster2    |[0m 2017-01-17 19:48:39,876 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:17262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@2bd23245
[32mcluster2    |[0m 2017-01-17 19:48:39,877 INFO  [RS:0;0:0:0:0:0:0:0:0:41916-SendThread(localhost:17262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:17262. Will not attempt to authenticate using SASL (unknown error)
[32mcluster2    |[0m 2017-01-17 19:48:39,877 INFO  [RS:0;0:0:0:0:0:0:0:0:41916-SendThread(localhost:17262)] zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:17262, initiating session
[32mcluster2    |[0m 2017-01-17 19:48:39,877 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /127.0.0.1:49018
[32mcluster2    |[0m 2017-01-17 19:48:39,878 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /127.0.0.1:49018
[32mcluster2    |[0m 2017-01-17 19:48:39,880 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0004 with negotiated timeout 10000 for client /127.0.0.1:49018
[32mcluster2    |[0m 2017-01-17 19:48:39,880 INFO  [RS:0;0:0:0:0:0:0:0:0:41916-SendThread(localhost:17262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:17262, sessionid = 0x159adf9552f0004, negotiated timeout = 10000
[32mcluster2    |[0m 2017-01-17 19:48:39,884 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
[32mcluster2    |[0m 2017-01-17 19:48:39,884 INFO  [RpcServer.listener,port=41916] ipc.RpcServer: RpcServer.listener,port=41916: starting
[32mcluster2    |[0m 2017-01-17 19:48:39,885 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=0 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,885 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=1 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,885 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=2 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,885 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=3 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,885 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=4 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,885 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=5 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=6 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=7 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=8 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=9 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=10 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=11 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=12 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,886 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=13 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,887 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=14 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,887 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=15 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,887 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=16 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,887 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=17 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,888 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=18 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,888 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=19 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,888 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=20 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,889 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=21 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,891 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=22 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,891 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=23 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,891 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=24 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,891 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=25 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,891 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=26 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,892 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=27 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,892 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=28 queue=1
[32mcluster2    |[0m 2017-01-17 19:48:39,893 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: B.Default Start Handler index=29 queue=2
[32mcluster2    |[0m 2017-01-17 19:48:39,893 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=0 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,894 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=1 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,894 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=2 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,894 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=3 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,894 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=4 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,894 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=5 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,895 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=6 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,895 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=7 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,895 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=8 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,895 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Priority Start Handler index=9 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,895 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Replication Start Handler index=0 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,896 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Replication Start Handler index=1 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,896 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] ipc.RpcExecutor: Replication Start Handler index=2 queue=0
[32mcluster2    |[0m 2017-01-17 19:48:39,919 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
[32mcluster2    |[0m 2017-01-17 19:48:39,927 INFO  [SplitLogWorker-cluster2,41916,1484682517717] regionserver.SplitLogWorker: SplitLogWorker cluster2,41916,1484682517717 starting
[32mcluster2    |[0m 2017-01-17 19:48:39,929 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] regionserver.HRegionServer: Serving as cluster2,41916,1484682517717, RpcServer on 0:0:0:0:0:0:0:0/0:0:0:0:0:0:0:0:41916, sessionid=0x159adf9552f0001
[32mcluster2    |[0m 2017-01-17 19:48:39,929 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
[32mcluster2    |[0m 2017-01-17 19:48:39,929 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] snapshot.RegionServerSnapshotManager: Start Snapshot Manager cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:39,929 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] procedure.ZKProcedureMemberRpcs: Starting procedure member 'cluster2,41916,1484682517717'
[32mcluster2    |[0m 2017-01-17 19:48:39,929 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
[32mcluster2    |[0m 2017-01-17 19:48:39,929 INFO  [SplitLogWorker-cluster2,41916,1484682517717] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6cede9cb connecting to ZooKeeper ensemble=localhost:17262
[32mcluster2    |[0m 2017-01-17 19:48:39,929 INFO  [SplitLogWorker-cluster2,41916,1484682517717] zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:17262 sessionTimeout=10000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@20f72340
[32mcluster2    |[0m 2017-01-17 19:48:39,930 DEBUG [RS:0;0:0:0:0:0:0:0:0:41916] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
[32mcluster2    |[0m 2017-01-17 19:48:39,930 INFO  [SplitLogWorker-cluster2,41916,1484682517717-SendThread(localhost:17262)] zookeeper.ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:17262. Will not attempt to authenticate using SASL (unknown error)
[32mcluster2    |[0m 2017-01-17 19:48:39,931 INFO  [RS:0;0:0:0:0:0:0:0:0:41916] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
[32mcluster2    |[0m 2017-01-17 19:48:39,931 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /0:0:0:0:0:0:0:1:56122
[32mcluster2    |[0m 2017-01-17 19:48:39,931 INFO  [SplitLogWorker-cluster2,41916,1484682517717-SendThread(localhost:17262)] zookeeper.ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:17262, initiating session
[32mcluster2    |[0m 2017-01-17 19:48:39,932 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /0:0:0:0:0:0:0:1:56122
[32mcluster2    |[0m 2017-01-17 19:48:39,935 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0005 with negotiated timeout 10000 for client /0:0:0:0:0:0:0:1:56122
[32mcluster2    |[0m 2017-01-17 19:48:39,935 INFO  [SplitLogWorker-cluster2,41916,1484682517717-SendThread(localhost:17262)] zookeeper.ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:17262, sessionid = 0x159adf9552f0005, negotiated timeout = 10000
[36mcluster3    |[0m 2017-01-17 19:48:41,144 INFO  [M:0;cluster3:36241] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 3272 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[33mcluster1    |[0m 2017-01-17 19:48:41,240 INFO  [M:0;cluster1:37102] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 3250 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[32mcluster2    |[0m 2017-01-17 19:48:41,284 INFO  [M:0;cluster2:42021] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 1754 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[36mcluster3    |[0m 2017-01-17 19:48:42,396 INFO  [M:0;cluster3:36241] master.ServerManager: Finished waiting for region servers count to settle; checked in 1, slept for 4524 ms, expecting minimum of 1, maximum of 2147483647, master is running.
[36mcluster3    |[0m 2017-01-17 19:48:42,397 INFO  [M:0;cluster3:36241] master.MasterFileSystem: Log folder file:/var/tmp/WALs/cluster3,42333,1484682515998 belongs to an existing region server
[33mcluster1    |[0m 2017-01-17 19:48:42,492 INFO  [M:0;cluster1:37102] master.ServerManager: Finished waiting for region servers count to settle; checked in 1, slept for 4502 ms, expecting minimum of 1, maximum of 2147483647, master is running.
[33mcluster1    |[0m 2017-01-17 19:48:42,494 INFO  [M:0;cluster1:37102] master.MasterFileSystem: Log folder file:/var/tmp/WALs/cluster1,38372,1484682516063 belongs to an existing region server
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 2017-01-17 19:48:43,258 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:48:43,258 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.3 port: 57116 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:48:43,211 INFO  [M:0;cluster3:36241] zookeeper.ZooKeeperNodeTracker: Unsetting hbase:meta region location in ZooKeeper
[36mcluster3    |[0m 2017-01-17 19:48:43,213 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:delete cxid:0x44 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/hbase/meta-region-server Error:KeeperErrorCode = NoNode for /hbase/meta-region-server
[36mcluster3    |[0m 2017-01-17 19:48:43,214 DEBUG [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/meta-region-server already deleted, retry=false
[36mcluster3    |[0m 2017-01-17 19:48:43,217 DEBUG [M:0;cluster3:36241] master.AssignmentManager: No previous transition plan found (or ignoring an existing plan) for hbase:meta,,1.1588230740; generated random plan=hri=hbase:meta,,1.1588230740, src=, dest=cluster3,42333,1484682515998; 1 (online=1, available=1) available servers, forceNewPlan=false
[36mcluster3    |[0m 2017-01-17 19:48:43,217 INFO  [M:0;cluster3:36241] master.AssignmentManager: Setting node as OFFLINED in ZooKeeper for region {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[36mcluster3    |[0m 2017-01-17 19:48:43,217 DEBUG [M:0;cluster3:36241] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Creating (or updating) unassigned node 1588230740 with OFFLINE state
[36mcluster3    |[0m 2017-01-17 19:48:43,225 DEBUG [M:0;cluster3:36241] master.AssignmentManager: Setting table hbase:meta to ENABLED state.
[36mcluster3    |[0m 2017-01-17 19:48:43,233 INFO  [M:0;cluster3:36241] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:43,234 INFO  [M:0;cluster3:36241] master.RegionStates: Transition {1588230740 state=OFFLINE, ts=1484682523217, server=null} to {1588230740 state=PENDING_OPEN, ts=1484682523233, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:48:43,234 DEBUG [M:0;cluster3:36241] master.ServerManager: New admin connection to cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:43,260 INFO  [PriorityRpcServer.handler=1,queue=0,port=42333] regionserver.HRegionServer: Open hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:48:43,262 DEBUG [RS_OPEN_META-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioning 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 2017-01-17 19:48:43,266 INFO  [M:0;cluster3:36241] master.ServerManager: AssignmentManager hasn't finished failover cleanup; waiting
[36mcluster3    |[0m 2017-01-17 19:48:43,266 DEBUG [RS_OPEN_META-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioned node 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 2017-01-17 19:48:43,266 DEBUG [RS_OPEN_META-cluster3:42333-0] regionserver.HRegionServer: logdir=file:/var/tmp/WALs/cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:48:43,267 INFO  [RS_OPEN_META-cluster3:42333-0] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[36mcluster3    |[0m 2017-01-17 19:48:43,267 DEBUG [AM.ZK.Worker-pool2-t1] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster3,42333,1484682515998, region=1588230740, current_state={1588230740 state=PENDING_OPEN, ts=1484682523233, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:48:43,268 INFO  [AM.ZK.Worker-pool2-t1] master.RegionStates: Transition {1588230740 state=PENDING_OPEN, ts=1484682523233, server=cluster3,42333,1484682515998} to {1588230740 state=OPENING, ts=1484682523268, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:48:43,272 INFO  [RS_OPEN_META-cluster3:42333-0] wal.FSHLog: New WAL /var/tmp/WALs/cluster3,42333,1484682515998/cluster3%2C42333%2C1484682515998.1484682523267.meta
[36mcluster3    |[0m 2017-01-17 19:48:43,272 INFO  [RS_OPEN_META-cluster3:42333-0] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=java.io.BufferedOutputStream
[36mcluster3    |[0m 2017-01-17 19:48:43,272 INFO  [RS_OPEN_META-cluster3:42333-0] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=java.io.BufferedOutputStream
[36mcluster3    |[0m 2017-01-17 19:48:43,274 DEBUG [RS_OPEN_META-cluster3:42333-0] regionserver.HRegion: Opening region: {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[36mcluster3    |[0m 2017-01-17 19:48:43,322 INFO  [RS_OPEN_META-cluster3:42333-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor hbase:meta
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 2017-01-17 19:48:43,366 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:48:43,366 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.4 port: 59746 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:48:43,307 INFO  [M:0;cluster1:37102] zookeeper.ZooKeeperNodeTracker: Unsetting hbase:meta region location in ZooKeeper
[33mcluster1    |[0m 2017-01-17 19:48:43,308 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40000 type:delete cxid:0x42 zxid:0x22 txntype:-1 reqpath:n/a Error Path:/hbase/meta-region-server Error:KeeperErrorCode = NoNode for /hbase/meta-region-server
[33mcluster1    |[0m 2017-01-17 19:48:43,310 DEBUG [M:0;cluster1:37102] zookeeper.RecoverableZooKeeper: Node /hbase/meta-region-server already deleted, retry=false
[33mcluster1    |[0m 2017-01-17 19:48:43,313 DEBUG [M:0;cluster1:37102] master.AssignmentManager: No previous transition plan found (or ignoring an existing plan) for hbase:meta,,1.1588230740; generated random plan=hri=hbase:meta,,1.1588230740, src=, dest=cluster1,38372,1484682516063; 1 (online=1, available=1) available servers, forceNewPlan=false
[33mcluster1    |[0m 2017-01-17 19:48:43,313 INFO  [M:0;cluster1:37102] master.AssignmentManager: Setting node as OFFLINED in ZooKeeper for region {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[33mcluster1    |[0m 2017-01-17 19:48:43,313 DEBUG [M:0;cluster1:37102] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Creating (or updating) unassigned node 1588230740 with OFFLINE state
[33mcluster1    |[0m 2017-01-17 19:48:43,320 DEBUG [M:0;cluster1:37102] master.AssignmentManager: Setting table hbase:meta to ENABLED state.
[33mcluster1    |[0m 2017-01-17 19:48:43,329 INFO  [M:0;cluster1:37102] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:43,329 INFO  [M:0;cluster1:37102] master.RegionStates: Transition {1588230740 state=OFFLINE, ts=1484682523313, server=null} to {1588230740 state=PENDING_OPEN, ts=1484682523329, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:48:43,329 DEBUG [M:0;cluster1:37102] master.ServerManager: New admin connection to cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:43,369 INFO  [PriorityRpcServer.handler=1,queue=0,port=38372] regionserver.HRegionServer: Open hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:48:43,371 DEBUG [RS_OPEN_META-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioning 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[33mcluster1    |[0m 2017-01-17 19:48:43,373 INFO  [M:0;cluster1:37102] master.ServerManager: AssignmentManager hasn't finished failover cleanup; waiting
[33mcluster1    |[0m 2017-01-17 19:48:43,374 DEBUG [RS_OPEN_META-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioned node 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[33mcluster1    |[0m 2017-01-17 19:48:43,374 DEBUG [RS_OPEN_META-cluster1:38372-0] regionserver.HRegionServer: logdir=file:/var/tmp/WALs/cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:48:43,375 INFO  [RS_OPEN_META-cluster1:38372-0] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[33mcluster1    |[0m 2017-01-17 19:48:43,376 DEBUG [AM.ZK.Worker-pool2-t1] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster1,38372,1484682516063, region=1588230740, current_state={1588230740 state=PENDING_OPEN, ts=1484682523329, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:48:43,376 INFO  [AM.ZK.Worker-pool2-t1] master.RegionStates: Transition {1588230740 state=PENDING_OPEN, ts=1484682523329, server=cluster1,38372,1484682516063} to {1588230740 state=OPENING, ts=1484682523376, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:48:43,380 INFO  [RS_OPEN_META-cluster1:38372-0] wal.FSHLog: New WAL /var/tmp/WALs/cluster1,38372,1484682516063/cluster1%2C38372%2C1484682516063.1484682523375.meta
[33mcluster1    |[0m 2017-01-17 19:48:43,380 INFO  [RS_OPEN_META-cluster1:38372-0] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=java.io.BufferedOutputStream
[33mcluster1    |[0m 2017-01-17 19:48:43,380 INFO  [RS_OPEN_META-cluster1:38372-0] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=java.io.BufferedOutputStream
[33mcluster1    |[0m 2017-01-17 19:48:43,381 DEBUG [RS_OPEN_META-cluster1:38372-0] regionserver.HRegion: Opening region: {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[33mcluster1    |[0m 2017-01-17 19:48:43,421 INFO  [RS_OPEN_META-cluster1:38372-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor hbase:meta
[32mcluster2    |[0m 2017-01-17 19:48:42,787 INFO  [M:0;cluster2:42021] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 3257 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
[32mcluster2    |[0m 2017-01-17 19:48:44,039 INFO  [M:0;cluster2:42021] master.ServerManager: Finished waiting for region servers count to settle; checked in 1, slept for 4509 ms, expecting minimum of 1, maximum of 2147483647, master is running.
[32mcluster2    |[0m 2017-01-17 19:48:44,040 INFO  [M:0;cluster2:42021] master.MasterFileSystem: Log folder file:/var/tmp/WALs/cluster2,41916,1484682517717 belongs to an existing region server
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/SecurityAuth.audit <==
[32mcluster2    |[0m 2017-01-17 19:48:44,906 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:48:44,907 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.2 port: 54064 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:48:44,853 INFO  [M:0;cluster2:42021] zookeeper.ZooKeeperNodeTracker: Unsetting hbase:meta region location in ZooKeeper
[32mcluster2    |[0m 2017-01-17 19:48:44,854 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0000 type:delete cxid:0x42 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/hbase/meta-region-server Error:KeeperErrorCode = NoNode for /hbase/meta-region-server
[32mcluster2    |[0m 2017-01-17 19:48:44,856 DEBUG [M:0;cluster2:42021] zookeeper.RecoverableZooKeeper: Node /hbase/meta-region-server already deleted, retry=false
[32mcluster2    |[0m 2017-01-17 19:48:44,859 DEBUG [M:0;cluster2:42021] master.AssignmentManager: No previous transition plan found (or ignoring an existing plan) for hbase:meta,,1.1588230740; generated random plan=hri=hbase:meta,,1.1588230740, src=, dest=cluster2,41916,1484682517717; 1 (online=1, available=1) available servers, forceNewPlan=false
[32mcluster2    |[0m 2017-01-17 19:48:44,859 INFO  [M:0;cluster2:42021] master.AssignmentManager: Setting node as OFFLINED in ZooKeeper for region {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[32mcluster2    |[0m 2017-01-17 19:48:44,859 DEBUG [M:0;cluster2:42021] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Creating (or updating) unassigned node 1588230740 with OFFLINE state
[32mcluster2    |[0m 2017-01-17 19:48:44,867 DEBUG [M:0;cluster2:42021] master.AssignmentManager: Setting table hbase:meta to ENABLED state.
[32mcluster2    |[0m 2017-01-17 19:48:44,876 INFO  [M:0;cluster2:42021] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:44,876 INFO  [M:0;cluster2:42021] master.RegionStates: Transition {1588230740 state=OFFLINE, ts=1484682524859, server=null} to {1588230740 state=PENDING_OPEN, ts=1484682524876, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:48:44,876 DEBUG [M:0;cluster2:42021] master.ServerManager: New admin connection to cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:44,909 INFO  [PriorityRpcServer.handler=1,queue=0,port=41916] regionserver.HRegionServer: Open hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:48:44,912 DEBUG [RS_OPEN_META-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioning 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[32mcluster2    |[0m 2017-01-17 19:48:44,915 INFO  [M:0;cluster2:42021] master.ServerManager: AssignmentManager hasn't finished failover cleanup; waiting
[32mcluster2    |[0m 2017-01-17 19:48:44,916 DEBUG [RS_OPEN_META-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioned node 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[32mcluster2    |[0m 2017-01-17 19:48:44,916 DEBUG [RS_OPEN_META-cluster2:41916-0] regionserver.HRegionServer: logdir=file:/var/tmp/WALs/cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:48:44,917 INFO  [RS_OPEN_META-cluster2:41916-0] wal.FSHLog: WAL/HLog configuration: blocksize=32 MB, rollsize=30.40 MB, enabled=true
[32mcluster2    |[0m 2017-01-17 19:48:44,918 DEBUG [AM.ZK.Worker-pool2-t1] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster2,41916,1484682517717, region=1588230740, current_state={1588230740 state=PENDING_OPEN, ts=1484682524876, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:48:44,918 INFO  [AM.ZK.Worker-pool2-t1] master.RegionStates: Transition {1588230740 state=PENDING_OPEN, ts=1484682524876, server=cluster2,41916,1484682517717} to {1588230740 state=OPENING, ts=1484682524918, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:48:44,922 INFO  [RS_OPEN_META-cluster2:41916-0] wal.FSHLog: New WAL /var/tmp/WALs/cluster2,41916,1484682517717/cluster2%2C41916%2C1484682517717.1484682524917.meta
[32mcluster2    |[0m 2017-01-17 19:48:44,923 INFO  [RS_OPEN_META-cluster2:41916-0] wal.FSHLog: FileSystem's output stream doesn't support getNumCurrentReplicas; --HDFS-826 not available; fsOut=java.io.BufferedOutputStream
[32mcluster2    |[0m 2017-01-17 19:48:44,923 INFO  [RS_OPEN_META-cluster2:41916-0] wal.FSHLog: FileSystem's output stream doesn't support getPipeline; not available; fsOut=java.io.BufferedOutputStream
[32mcluster2    |[0m 2017-01-17 19:48:44,924 DEBUG [RS_OPEN_META-cluster2:41916-0] regionserver.HRegion: Opening region: {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[32mcluster2    |[0m 2017-01-17 19:48:44,981 INFO  [RS_OPEN_META-cluster2:41916-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor hbase:meta
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 2017-01-17 19:49:03,400 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:49:03,401 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.3 port: 57126 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[36mcluster3    |[0m 2017-01-17 19:49:03,598 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:49:03,598 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.3 port: 57134 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:49:03,328 INFO  [Thread-80] CMiddleware.RelayClient: 6262 is going to connect to cluster1:6262
[36mcluster3    |[0m 2017-01-17 19:49:03,329 INFO  [RS_OPEN_META-cluster3:42333-0] smcoprocessors.SmpcCoprocessor: Resources initated 2
[36mcluster3    |[0m 2017-01-17 19:49:03,329 INFO  [RS_OPEN_META-cluster3:42333-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[36mcluster3    |[0m 2017-01-17 19:49:03,329 INFO  [Thread-80] CMiddleware.RelayClient: 6262 is going to connect to cluster2:6262
[36mcluster3    |[0m 2017-01-17 19:49:03,330 DEBUG [RS_OPEN_META-cluster3:42333-0] coprocessor.CoprocessorHost: Loading coprocessor class org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint with path null and priority 536870911
[36mcluster3    |[0m 2017-01-17 19:49:03,330 INFO  [Thread-80] CMiddleware.IORelay: 6262 waiting for clients to connect to server
[36mcluster3    |[0m 2017-01-17 19:49:03,334 DEBUG [RS_OPEN_META-cluster3:42333-0] regionserver.HRegion: Registered coprocessor service: region=hbase:meta,,1 service=MultiRowMutationService
[36mcluster3    |[0m 2017-01-17 19:49:03,334 INFO  [RS_OPEN_META-cluster3:42333-0] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint from HTD of hbase:meta successfully.
[36mcluster3    |[0m 2017-01-17 19:49:03,340 DEBUG [RS_OPEN_META-cluster3:42333-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table meta 1588230740
[36mcluster3    |[0m 2017-01-17 19:49:03,340 DEBUG [RS_OPEN_META-cluster3:42333-0] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:49:03,343 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 2017-01-17 19:49:03,344 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[36mcluster3    |[0m 2017-01-17 19:49:03,346 DEBUG [RS_OPEN_META-cluster3:42333-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/meta/1588230740
[36mcluster3    |[0m 2017-01-17 19:49:03,346 INFO  [RS_OPEN_META-cluster3:42333-0] regionserver.HRegion: Onlined 1588230740; next sequenceid=1
[36mcluster3    |[0m 2017-01-17 19:49:03,346 DEBUG [RS_OPEN_META-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Attempting to retransition opening state of node 1588230740
[36mcluster3    |[0m 2017-01-17 19:49:03,348 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Post open deploy tasks for region=hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:49:03,349 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Updating zk with meta location
[36mcluster3    |[0m 2017-01-17 19:49:03,349 INFO  [PostOpenDeployTasks:1588230740] zookeeper.ZooKeeperNodeTracker: Setting hbase:meta region location in ZooKeeper as cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,357 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Finished post open deploy task for hbase:meta,,1.1588230740
[36mcluster3    |[0m 2017-01-17 19:49:03,357 DEBUG [RS_OPEN_META-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioning 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:03,360 DEBUG [RS_OPEN_META-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioned node 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:03,360 DEBUG [RS_OPEN_META-cluster3:42333-0] handler.OpenRegionHandler: Transitioned 1588230740 to OPENED in zk on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,361 DEBUG [RS_OPEN_META-cluster3:42333-0] handler.OpenRegionHandler: Opened hbase:meta,,1.1588230740 on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,361 DEBUG [AM.ZK.Worker-pool2-t2] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster3,42333,1484682515998, region=1588230740, current_state={1588230740 state=OPENING, ts=1484682523268, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,361 INFO  [AM.ZK.Worker-pool2-t2] master.RegionStates: Transition {1588230740 state=OPENING, ts=1484682523268, server=cluster3,42333,1484682515998} to {1588230740 state=OPEN, ts=1484682543361, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,363 INFO  [AM.ZK.Worker-pool2-t2] handler.OpenedRegionHandler: Handling OPENED of 1588230740 from cluster3,42333,1484682515998; deleting unassigned node
[36mcluster3    |[0m 2017-01-17 19:49:03,365 DEBUG [AM.ZK.Worker-pool2-t2] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Deleted unassigned node 1588230740 in expected state RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:03,366 DEBUG [AM.ZK.Worker-pool2-t3] master.AssignmentManager: Znode hbase:meta,,1.1588230740 deleted, state: {1588230740 state=OPEN, ts=1484682543361, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,366 INFO  [AM.ZK.Worker-pool2-t3] master.RegionStates: Onlined 1588230740 on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,367 INFO  [M:0;cluster3:36241] master.HMaster: hbase:meta assigned=1, rit=false, location=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,421 INFO  [M:0;cluster3:36241] catalog.MetaMigrationConvertingToPB: hbase:meta doesn't have any entries to update.
[36mcluster3    |[0m 2017-01-17 19:49:03,421 INFO  [M:0;cluster3:36241] catalog.MetaMigrationConvertingToPB: META already up-to date with PB serialization
[36mcluster3    |[0m 2017-01-17 19:49:03,439 DEBUG [M:0;cluster3:36241] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Deleting any existing unassigned nodes
[36mcluster3    |[0m 2017-01-17 19:49:03,440 INFO  [M:0;cluster3:36241] master.AssignmentManager: Clean cluster startup. Assigning user regions
[36mcluster3    |[0m 2017-01-17 19:49:03,442 INFO  [M:0;cluster3:36241] master.SnapshotOfRegionAssignmentFromMeta: Start to scan the hbase:meta for the current region assignment snappshot
[36mcluster3    |[0m 2017-01-17 19:49:03,448 INFO  [M:0;cluster3:36241] master.SnapshotOfRegionAssignmentFromMeta: Finished to scan the hbase:meta for the current region assignmentsnapshot
[36mcluster3    |[0m 2017-01-17 19:49:03,449 INFO  [M:0;cluster3:36241] master.AssignmentManager: Joined the cluster in 28ms, failover=false
[36mcluster3    |[0m 2017-01-17 19:49:03,460 INFO  [M:0;cluster3:36241] master.TableNamespaceManager: Namespace table not found. Creating...
[36mcluster3    |[0m 2017-01-17 19:49:03,474 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x27680 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/hbase/table-lock/hbase:namespace Error:KeeperErrorCode = NoNode for /hbase/table-lock/hbase:namespace
[36mcluster3    |[0m 2017-01-17 19:49:03,481 DEBUG [M:0;cluster3:36241] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/hbase:namespace/write-master:362410000000000
[36mcluster3    |[0m 2017-01-17 19:49:03,484 WARN  [M:0;cluster3:36241] zookeeper.ZKTable: Moving table hbase:namespace state to enabling but was not first in disabled state: null
[36mcluster3    |[0m 2017-01-17 19:49:03,489 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] handler.CreateTableHandler: Create table hbase:namespace
[36mcluster3    |[0m 2017-01-17 19:49:03,498 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/.tmp/data/hbase/namespace/.tabledesc/.tableinfo.0000000001
[36mcluster3    |[0m 2017-01-17 19:49:03,501 INFO  [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: creating HRegion hbase:namespace HTD == 'hbase:namespace', {NAME => 'info', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => 'FOREVER', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '8192', IN_MEMORY => 'true', BLOCKCACHE => 'true'} RootDir = file:/var/tmp/.tmp Table name == hbase:namespace
[36mcluster3    |[0m 2017-01-17 19:49:03,508 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Instantiated hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[36mcluster3    |[0m 2017-01-17 19:49:03,508 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Closing hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.: disabling compactions & flushes
[36mcluster3    |[0m 2017-01-17 19:49:03,508 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Updates disabled for region hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[36mcluster3    |[0m 2017-01-17 19:49:03,508 INFO  [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Closed hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[36mcluster3    |[0m 2017-01-17 19:49:03,581 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] catalog.MetaEditor: Added 1
[36mcluster3    |[0m 2017-01-17 19:49:03,581 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.AssignmentManager: Assigning 1 region(s) to cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,583 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Async create of unassigned node 8c8ee6a8dfa41a38537126f7863d1ae6 with OFFLINE state
[36mcluster3    |[0m 2017-01-17 19:49:03,592 DEBUG [main-EventThread] master.OfflineCallback: rs={8c8ee6a8dfa41a38537126f7863d1ae6 state=OFFLINE, ts=1484682543581, server=null}, server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,592 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback: rs={8c8ee6a8dfa41a38537126f7863d1ae6 state=OFFLINE, ts=1484682543581, server=null}, server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,596 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.AssignmentManager: cluster3,42333,1484682515998 unassigned znodes=1 of total=1
[36mcluster3    |[0m 2017-01-17 19:49:03,597 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.RegionStates: Transition {8c8ee6a8dfa41a38537126f7863d1ae6 state=OFFLINE, ts=1484682543583, server=null} to {8c8ee6a8dfa41a38537126f7863d1ae6 state=PENDING_OPEN, ts=1484682543596, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,599 INFO  [PriorityRpcServer.handler=1,queue=0,port=42333] regionserver.HRegionServer: Open hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[36mcluster3    |[0m 2017-01-17 19:49:03,602 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioning 8c8ee6a8dfa41a38537126f7863d1ae6 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 2017-01-17 19:49:03,602 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.AssignmentManager: Bulk assigning done for cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,611 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioned node 8c8ee6a8dfa41a38537126f7863d1ae6 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 2017-01-17 19:49:03,611 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] regionserver.HRegion: Opening region: {ENCODED => 8c8ee6a8dfa41a38537126f7863d1ae6, NAME => 'hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.', STARTKEY => '', ENDKEY => ''}
[36mcluster3    |[0m 2017-01-17 19:49:03,612 INFO  [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor hbase:namespace
[36mcluster3    |[0m 2017-01-17 19:49:03,613 INFO  [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[36mcluster3    |[0m 2017-01-17 19:49:03,613 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace 8c8ee6a8dfa41a38537126f7863d1ae6
[36mcluster3    |[0m 2017-01-17 19:49:03,614 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] regionserver.HRegion: Instantiated hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[36mcluster3    |[0m 2017-01-17 19:49:03,614 DEBUG [AM.ZK.Worker-pool2-t5] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster3,42333,1484682515998, region=8c8ee6a8dfa41a38537126f7863d1ae6, current_state={8c8ee6a8dfa41a38537126f7863d1ae6 state=PENDING_OPEN, ts=1484682543596, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,614 INFO  [AM.ZK.Worker-pool2-t5] master.RegionStates: Transition {8c8ee6a8dfa41a38537126f7863d1ae6 state=PENDING_OPEN, ts=1484682543596, server=cluster3,42333,1484682515998} to {8c8ee6a8dfa41a38537126f7863d1ae6 state=OPENING, ts=1484682543614, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,615 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] lock.ZKInterProcessLockBase: Released /hbase/table-lock/hbase:namespace/write-master:362410000000000
[36mcluster3    |[0m 2017-01-17 19:49:03,616 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] handler.CreateTableHandler: failed. null
[36mcluster3    |[0m 2017-01-17 19:49:03,620 INFO  [StoreOpener-8c8ee6a8dfa41a38537126f7863d1ae6-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 2017-01-17 19:49:03,620 INFO  [StoreOpener-8c8ee6a8dfa41a38537126f7863d1ae6-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[36mcluster3    |[0m 2017-01-17 19:49:03,626 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/namespace/8c8ee6a8dfa41a38537126f7863d1ae6
[36mcluster3    |[0m 2017-01-17 19:49:03,627 INFO  [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] regionserver.HRegion: Onlined 8c8ee6a8dfa41a38537126f7863d1ae6; next sequenceid=1
[36mcluster3    |[0m 2017-01-17 19:49:03,627 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Attempting to retransition opening state of node 8c8ee6a8dfa41a38537126f7863d1ae6
[36mcluster3    |[0m 2017-01-17 19:49:03,628 INFO  [PostOpenDeployTasks:8c8ee6a8dfa41a38537126f7863d1ae6] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 2017-01-17 19:49:03,503 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:49:03,503 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.4 port: 59760 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:49:03,425 INFO  [Thread-80] CMiddleware.RelayClient: 6262 is going to connect to cluster2:6262
[33mcluster1    |[0m 2017-01-17 19:49:03,426 INFO  [RS_OPEN_META-cluster1:38372-0] smcoprocessors.SmpcCoprocessor: Resources initated 0
[33mcluster1    |[0m 2017-01-17 19:49:03,426 INFO  [RS_OPEN_META-cluster1:38372-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[33mcluster1    |[0m 2017-01-17 19:49:03,426 INFO  [Thread-80] CMiddleware.RelayClient: 6262 is going to connect to cluster3:6262
[33mcluster1    |[0m 2017-01-17 19:49:03,427 DEBUG [RS_OPEN_META-cluster1:38372-0] coprocessor.CoprocessorHost: Loading coprocessor class org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint with path null and priority 536870911
[33mcluster1    |[0m 2017-01-17 19:49:03,430 INFO  [Thread-80] CMiddleware.IORelay: 6262 waiting for clients to connect to server
[33mcluster1    |[0m 2017-01-17 19:49:03,430 DEBUG [RS_OPEN_META-cluster1:38372-0] regionserver.HRegion: Registered coprocessor service: region=hbase:meta,,1 service=MultiRowMutationService
[33mcluster1    |[0m 2017-01-17 19:49:03,430 INFO  [RS_OPEN_META-cluster1:38372-0] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint from HTD of hbase:meta successfully.
[33mcluster1    |[0m 2017-01-17 19:49:03,436 DEBUG [RS_OPEN_META-cluster1:38372-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table meta 1588230740
[33mcluster1    |[0m 2017-01-17 19:49:03,436 DEBUG [RS_OPEN_META-cluster1:38372-0] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:49:03,440 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[33mcluster1    |[0m 2017-01-17 19:49:03,440 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[33mcluster1    |[0m 2017-01-17 19:49:03,444 DEBUG [RS_OPEN_META-cluster1:38372-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/meta/1588230740
[33mcluster1    |[0m 2017-01-17 19:49:03,444 INFO  [RS_OPEN_META-cluster1:38372-0] regionserver.HRegion: Onlined 1588230740; next sequenceid=1
[33mcluster1    |[0m 2017-01-17 19:49:03,444 DEBUG [RS_OPEN_META-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Attempting to retransition opening state of node 1588230740
[33mcluster1    |[0m 2017-01-17 19:49:03,446 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Post open deploy tasks for region=hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:49:03,447 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Updating zk with meta location
[33mcluster1    |[0m 2017-01-17 19:49:03,447 INFO  [PostOpenDeployTasks:1588230740] zookeeper.ZooKeeperNodeTracker: Setting hbase:meta region location in ZooKeeper as cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,458 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Finished post open deploy task for hbase:meta,,1.1588230740
[33mcluster1    |[0m 2017-01-17 19:49:03,459 DEBUG [RS_OPEN_META-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioning 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:03,463 DEBUG [RS_OPEN_META-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioned node 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:03,463 DEBUG [RS_OPEN_META-cluster1:38372-0] handler.OpenRegionHandler: Transitioned 1588230740 to OPENED in zk on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,463 DEBUG [RS_OPEN_META-cluster1:38372-0] handler.OpenRegionHandler: Opened hbase:meta,,1.1588230740 on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,463 DEBUG [AM.ZK.Worker-pool2-t2] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster1,38372,1484682516063, region=1588230740, current_state={1588230740 state=OPENING, ts=1484682523376, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,463 INFO  [AM.ZK.Worker-pool2-t2] master.RegionStates: Transition {1588230740 state=OPENING, ts=1484682523376, server=cluster1,38372,1484682516063} to {1588230740 state=OPEN, ts=1484682543463, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,465 INFO  [AM.ZK.Worker-pool2-t2] handler.OpenedRegionHandler: Handling OPENED of 1588230740 from cluster1,38372,1484682516063; deleting unassigned node
[33mcluster1    |[0m 2017-01-17 19:49:03,467 DEBUG [AM.ZK.Worker-pool2-t2] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Deleted unassigned node 1588230740 in expected state RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:03,469 DEBUG [AM.ZK.Worker-pool2-t3] master.AssignmentManager: Znode hbase:meta,,1.1588230740 deleted, state: {1588230740 state=OPEN, ts=1484682543463, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,469 INFO  [AM.ZK.Worker-pool2-t3] master.RegionStates: Onlined 1588230740 on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,470 INFO  [M:0;cluster1:37102] master.HMaster: hbase:meta assigned=1, rit=false, location=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,526 INFO  [M:0;cluster1:37102] catalog.MetaMigrationConvertingToPB: hbase:meta doesn't have any entries to update.
[33mcluster1    |[0m 2017-01-17 19:49:03,526 INFO  [M:0;cluster1:37102] catalog.MetaMigrationConvertingToPB: META already up-to date with PB serialization
[33mcluster1    |[0m 2017-01-17 19:49:03,537 DEBUG [M:0;cluster1:37102] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Deleting any existing unassigned nodes
[33mcluster1    |[0m 2017-01-17 19:49:03,537 INFO  [M:0;cluster1:37102] master.AssignmentManager: Clean cluster startup. Assigning user regions
[33mcluster1    |[0m 2017-01-17 19:49:03,539 INFO  [M:0;cluster1:37102] master.SnapshotOfRegionAssignmentFromMeta: Start to scan the hbase:meta for the current region assignment snappshot
[33mcluster1    |[0m 2017-01-17 19:49:03,544 INFO  [M:0;cluster1:37102] master.SnapshotOfRegionAssignmentFromMeta: Finished to scan the hbase:meta for the current region assignmentsnapshot
[33mcluster1    |[0m 2017-01-17 19:49:03,545 INFO  [M:0;cluster1:37102] master.AssignmentManager: Joined the cluster in 19ms, failover=false
[33mcluster1    |[0m 2017-01-17 19:49:03,557 INFO  [M:0;cluster1:37102] master.TableNamespaceManager: Namespace table not found. Creating...
[33mcluster1    |[0m 2017-01-17 19:49:03,575 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40000 type:create cxid:0x28516 zxid:0x2a txntype:-1 reqpath:n/a Error Path:/hbase/table-lock/hbase:namespace Error:KeeperErrorCode = NoNode for /hbase/table-lock/hbase:namespace
[33mcluster1    |[0m 2017-01-17 19:49:03,582 DEBUG [M:0;cluster1:37102] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/hbase:namespace/write-master:371020000000000
[33mcluster1    |[0m 2017-01-17 19:49:03,585 WARN  [M:0;cluster1:37102] zookeeper.ZKTable: Moving table hbase:namespace state to enabling but was not first in disabled state: null
[33mcluster1    |[0m 2017-01-17 19:49:03,594 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] handler.CreateTableHandler: Create table hbase:namespace
[33mcluster1    |[0m 2017-01-17 19:49:03,603 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/.tmp/data/hbase/namespace/.tabledesc/.tableinfo.0000000001
[33mcluster1    |[0m 2017-01-17 19:49:03,607 INFO  [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: creating HRegion hbase:namespace HTD == 'hbase:namespace', {NAME => 'info', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => 'FOREVER', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '8192', IN_MEMORY => 'true', BLOCKCACHE => 'true'} RootDir = file:/var/tmp/.tmp Table name == hbase:namespace
[33mcluster1    |[0m 2017-01-17 19:49:03,615 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Instantiated hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,615 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Closing hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.: disabling compactions & flushes
[33mcluster1    |[0m 2017-01-17 19:49:03,615 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Updates disabled for region hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,615 INFO  [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Closed hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,689 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] catalog.MetaEditor: Added 1
[33mcluster1    |[0m 2017-01-17 19:49:03,690 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.AssignmentManager: Assigning 1 region(s) to cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,691 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Async create of unassigned node 6c24cf52d16371c33779affda6f273d9 with OFFLINE state
[33mcluster1    |[0m 2017-01-17 19:49:03,695 DEBUG [main-EventThread] master.OfflineCallback: rs={6c24cf52d16371c33779affda6f273d9 state=OFFLINE, ts=1484682543689, server=null}, server=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,695 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback: rs={6c24cf52d16371c33779affda6f273d9 state=OFFLINE, ts=1484682543689, server=null}, server=cluster1,38372,1484682516063
[36mcluster3    |[0m 2017-01-17 19:49:03,632 INFO  [PostOpenDeployTasks:8c8ee6a8dfa41a38537126f7863d1ae6] catalog.MetaEditor: Updated row hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6. with server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,640 INFO  [PostOpenDeployTasks:8c8ee6a8dfa41a38537126f7863d1ae6] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6.
[36mcluster3    |[0m 2017-01-17 19:49:03,640 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioning 8c8ee6a8dfa41a38537126f7863d1ae6 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:03,642 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioned node 8c8ee6a8dfa41a38537126f7863d1ae6 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:03,642 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] handler.OpenRegionHandler: Transitioned 8c8ee6a8dfa41a38537126f7863d1ae6 to OPENED in zk on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,643 DEBUG [RS_OPEN_PRIORITY_REGION-cluster3:42333-0] handler.OpenRegionHandler: Opened hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6. on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,643 DEBUG [AM.ZK.Worker-pool2-t6] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster3,42333,1484682515998, region=8c8ee6a8dfa41a38537126f7863d1ae6, current_state={8c8ee6a8dfa41a38537126f7863d1ae6 state=OPENING, ts=1484682543614, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,643 INFO  [AM.ZK.Worker-pool2-t6] master.RegionStates: Transition {8c8ee6a8dfa41a38537126f7863d1ae6 state=OPENING, ts=1484682543614, server=cluster3,42333,1484682515998} to {8c8ee6a8dfa41a38537126f7863d1ae6 state=OPEN, ts=1484682543643, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,643 DEBUG [AM.ZK.Worker-pool2-t6] handler.OpenedRegionHandler: Handling OPENED of 8c8ee6a8dfa41a38537126f7863d1ae6 from cluster3,42333,1484682515998; deleting unassigned node
[36mcluster3    |[0m 2017-01-17 19:49:03,645 DEBUG [AM.ZK.Worker-pool2-t6] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Deleted unassigned node 8c8ee6a8dfa41a38537126f7863d1ae6 in expected state RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:03,647 DEBUG [AM.ZK.Worker-pool2-t8] master.AssignmentManager: Znode hbase:namespace,,1484682543460.8c8ee6a8dfa41a38537126f7863d1ae6. deleted, state: {8c8ee6a8dfa41a38537126f7863d1ae6 state=OPEN, ts=1484682543643, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:03,648 INFO  [AM.ZK.Worker-pool2-t8] master.RegionStates: Onlined 8c8ee6a8dfa41a38537126f7863d1ae6 on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:03,722 DEBUG [M:0;cluster3:36241] client.ClientSmallScanner: Finished with small scan at {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[36mcluster3    |[0m 2017-01-17 19:49:03,736 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node default with data: \x0A\x07default
[36mcluster3    |[0m 2017-01-17 19:49:03,745 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node default with data: \x0A\x07default
[36mcluster3    |[0m 2017-01-17 19:49:03,745 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node hbase with data: \x0A\x05hbase
[36mcluster3    |[0m 2017-01-17 19:49:03,748 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x276a2 zxid:0x3c txntype:-1 reqpath:n/a Error Path:/hbase/namespace/default Error:KeeperErrorCode = NodeExists for /hbase/namespace/default
[36mcluster3    |[0m 2017-01-17 19:49:03,750 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/namespace/default already exists and this is not a retry
[36mcluster3    |[0m 2017-01-17 19:49:03,752 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x276a5 zxid:0x3e txntype:-1 reqpath:n/a Error Path:/hbase/namespace/hbase Error:KeeperErrorCode = NodeExists for /hbase/namespace/hbase
[36mcluster3    |[0m 2017-01-17 19:49:03,753 INFO  [M:0;cluster3:36241] zookeeper.RecoverableZooKeeper: Node /hbase/namespace/hbase already exists and this is not a retry
[36mcluster3    |[0m 2017-01-17 19:49:03,755 INFO  [M:0;cluster3:36241] master.HMaster: Master has completed initialization
[36mcluster3    |[0m 2017-01-17 19:49:03,755 INFO  [M:0;cluster3:36241] zookeeper.ZooKeeperWatcher: not a secure deployment, proceeding
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 2017-01-17 19:49:03,698 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:49:03,698 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.4 port: 59764 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:49:03,696 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.AssignmentManager: cluster1,38372,1484682516063 unassigned znodes=1 of total=1
[33mcluster1    |[0m 2017-01-17 19:49:03,697 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.RegionStates: Transition {6c24cf52d16371c33779affda6f273d9 state=OFFLINE, ts=1484682543691, server=null} to {6c24cf52d16371c33779affda6f273d9 state=PENDING_OPEN, ts=1484682543697, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,699 INFO  [PriorityRpcServer.handler=1,queue=0,port=38372] regionserver.HRegionServer: Open hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,701 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioning 6c24cf52d16371c33779affda6f273d9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[33mcluster1    |[0m 2017-01-17 19:49:03,701 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.AssignmentManager: Bulk assigning done for cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,703 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioned node 6c24cf52d16371c33779affda6f273d9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[33mcluster1    |[0m 2017-01-17 19:49:03,704 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] regionserver.HRegion: Opening region: {ENCODED => 6c24cf52d16371c33779affda6f273d9, NAME => 'hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.', STARTKEY => '', ENDKEY => ''}
[33mcluster1    |[0m 2017-01-17 19:49:03,704 DEBUG [AM.ZK.Worker-pool2-t5] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster1,38372,1484682516063, region=6c24cf52d16371c33779affda6f273d9, current_state={6c24cf52d16371c33779affda6f273d9 state=PENDING_OPEN, ts=1484682543697, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,704 INFO  [AM.ZK.Worker-pool2-t5] master.RegionStates: Transition {6c24cf52d16371c33779affda6f273d9 state=PENDING_OPEN, ts=1484682543697, server=cluster1,38372,1484682516063} to {6c24cf52d16371c33779affda6f273d9 state=OPENING, ts=1484682543704, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,705 INFO  [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor hbase:namespace
[33mcluster1    |[0m 2017-01-17 19:49:03,705 INFO  [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[33mcluster1    |[0m 2017-01-17 19:49:03,705 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace 6c24cf52d16371c33779affda6f273d9
[33mcluster1    |[0m 2017-01-17 19:49:03,706 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] regionserver.HRegion: Instantiated hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,706 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] lock.ZKInterProcessLockBase: Released /hbase/table-lock/hbase:namespace/write-master:371020000000000
[33mcluster1    |[0m 2017-01-17 19:49:03,707 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] handler.CreateTableHandler: failed. null
[33mcluster1    |[0m 2017-01-17 19:49:03,710 INFO  [StoreOpener-6c24cf52d16371c33779affda6f273d9-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[33mcluster1    |[0m 2017-01-17 19:49:03,711 INFO  [StoreOpener-6c24cf52d16371c33779affda6f273d9-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[33mcluster1    |[0m 2017-01-17 19:49:03,712 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/namespace/6c24cf52d16371c33779affda6f273d9
[33mcluster1    |[0m 2017-01-17 19:49:03,713 INFO  [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] regionserver.HRegion: Onlined 6c24cf52d16371c33779affda6f273d9; next sequenceid=1
[33mcluster1    |[0m 2017-01-17 19:49:03,713 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Attempting to retransition opening state of node 6c24cf52d16371c33779affda6f273d9
[33mcluster1    |[0m 2017-01-17 19:49:03,714 INFO  [PostOpenDeployTasks:6c24cf52d16371c33779affda6f273d9] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,718 INFO  [PostOpenDeployTasks:6c24cf52d16371c33779affda6f273d9] catalog.MetaEditor: Updated row hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9. with server=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,718 INFO  [PostOpenDeployTasks:6c24cf52d16371c33779affda6f273d9] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9.
[33mcluster1    |[0m 2017-01-17 19:49:03,719 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioning 6c24cf52d16371c33779affda6f273d9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:03,721 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioned node 6c24cf52d16371c33779affda6f273d9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:03,721 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] handler.OpenRegionHandler: Transitioned 6c24cf52d16371c33779affda6f273d9 to OPENED in zk on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,722 DEBUG [RS_OPEN_PRIORITY_REGION-cluster1:38372-0] handler.OpenRegionHandler: Opened hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9. on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,722 DEBUG [AM.ZK.Worker-pool2-t6] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster1,38372,1484682516063, region=6c24cf52d16371c33779affda6f273d9, current_state={6c24cf52d16371c33779affda6f273d9 state=OPENING, ts=1484682543704, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,722 INFO  [AM.ZK.Worker-pool2-t6] master.RegionStates: Transition {6c24cf52d16371c33779affda6f273d9 state=OPENING, ts=1484682543704, server=cluster1,38372,1484682516063} to {6c24cf52d16371c33779affda6f273d9 state=OPEN, ts=1484682543722, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,722 DEBUG [AM.ZK.Worker-pool2-t6] handler.OpenedRegionHandler: Handling OPENED of 6c24cf52d16371c33779affda6f273d9 from cluster1,38372,1484682516063; deleting unassigned node
[33mcluster1    |[0m 2017-01-17 19:49:03,725 DEBUG [AM.ZK.Worker-pool2-t6] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Deleted unassigned node 6c24cf52d16371c33779affda6f273d9 in expected state RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:03,727 DEBUG [AM.ZK.Worker-pool2-t8] master.AssignmentManager: Znode hbase:namespace,,1484682543557.6c24cf52d16371c33779affda6f273d9. deleted, state: {6c24cf52d16371c33779affda6f273d9 state=OPEN, ts=1484682543722, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:03,727 INFO  [AM.ZK.Worker-pool2-t8] master.RegionStates: Onlined 6c24cf52d16371c33779affda6f273d9 on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:03,820 DEBUG [M:0;cluster1:37102] client.ClientSmallScanner: Finished with small scan at {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[33mcluster1    |[0m 2017-01-17 19:49:03,834 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node default with data: \x0A\x07default
[33mcluster1    |[0m 2017-01-17 19:49:03,842 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node default with data: \x0A\x07default
[33mcluster1    |[0m 2017-01-17 19:49:03,842 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node hbase with data: \x0A\x05hbase
[33mcluster1    |[0m 2017-01-17 19:49:03,846 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40000 type:create cxid:0x28538 zxid:0x3a txntype:-1 reqpath:n/a Error Path:/hbase/namespace/default Error:KeeperErrorCode = NodeExists for /hbase/namespace/default
[33mcluster1    |[0m 2017-01-17 19:49:03,847 INFO  [M:0;cluster1:37102] zookeeper.RecoverableZooKeeper: Node /hbase/namespace/default already exists and this is not a retry
[33mcluster1    |[0m 2017-01-17 19:49:03,849 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40000 type:create cxid:0x2853b zxid:0x3c txntype:-1 reqpath:n/a Error Path:/hbase/namespace/hbase Error:KeeperErrorCode = NodeExists for /hbase/namespace/hbase
[33mcluster1    |[0m 2017-01-17 19:49:03,851 INFO  [M:0;cluster1:37102] zookeeper.RecoverableZooKeeper: Node /hbase/namespace/hbase already exists and this is not a retry
[33mcluster1    |[0m 2017-01-17 19:49:03,852 INFO  [M:0;cluster1:37102] master.HMaster: Master has completed initialization
[33mcluster1    |[0m 2017-01-17 19:49:03,853 INFO  [M:0;cluster1:37102] zookeeper.ZooKeeperWatcher: not a secure deployment, proceeding
[36mcluster3    |[0m 2017-01-17 19:49:04,987 INFO  [Thread-80] CMiddleware.IORelay: 6262 completed booting phase
[33mcluster1    |[0m 2017-01-17 19:49:04,994 INFO  [Thread-80] CMiddleware.IORelay: 6262 completed booting phase
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/SecurityAuth.audit <==
[32mcluster2    |[0m 2017-01-17 19:49:05,055 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:49:05,055 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.2 port: 54086 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[32mcluster2    |[0m 2017-01-17 19:49:05,213 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:49:05,214 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.2 port: 54088 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:49:04,985 INFO  [Thread-80] CMiddleware.RelayClient: 6262 is going to connect to cluster3:6262
[32mcluster2    |[0m 2017-01-17 19:49:04,986 INFO  [RS_OPEN_META-cluster2:41916-0] smcoprocessors.SmpcCoprocessor: Resources initated 1
[32mcluster2    |[0m 2017-01-17 19:49:04,986 INFO  [RS_OPEN_META-cluster2:41916-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[32mcluster2    |[0m 2017-01-17 19:49:04,987 DEBUG [RS_OPEN_META-cluster2:41916-0] coprocessor.CoprocessorHost: Loading coprocessor class org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint with path null and priority 536870911
[32mcluster2    |[0m 2017-01-17 19:49:04,991 DEBUG [RS_OPEN_META-cluster2:41916-0] regionserver.HRegion: Registered coprocessor service: region=hbase:meta,,1 service=MultiRowMutationService
[32mcluster2    |[0m 2017-01-17 19:49:04,991 INFO  [RS_OPEN_META-cluster2:41916-0] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint from HTD of hbase:meta successfully.
[32mcluster2    |[0m 2017-01-17 19:49:04,993 INFO  [Thread-80] CMiddleware.RelayClient: 6262 is going to connect to cluster1:6262
[32mcluster2    |[0m 2017-01-17 19:49:04,994 INFO  [Thread-80] CMiddleware.IORelay: 6262 waiting for clients to connect to server
[32mcluster2    |[0m 2017-01-17 19:49:04,994 INFO  [Thread-80] CMiddleware.IORelay: 6262 completed booting phase
[32mcluster2    |[0m 2017-01-17 19:49:04,998 DEBUG [RS_OPEN_META-cluster2:41916-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table meta 1588230740
[32mcluster2    |[0m 2017-01-17 19:49:04,998 DEBUG [RS_OPEN_META-cluster2:41916-0] regionserver.HRegion: Instantiated hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:49:05,002 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[32mcluster2    |[0m 2017-01-17 19:49:05,003 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[32mcluster2    |[0m 2017-01-17 19:49:05,004 DEBUG [RS_OPEN_META-cluster2:41916-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/meta/1588230740
[32mcluster2    |[0m 2017-01-17 19:49:05,005 INFO  [RS_OPEN_META-cluster2:41916-0] regionserver.HRegion: Onlined 1588230740; next sequenceid=1
[32mcluster2    |[0m 2017-01-17 19:49:05,005 DEBUG [RS_OPEN_META-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Attempting to retransition opening state of node 1588230740
[32mcluster2    |[0m 2017-01-17 19:49:05,006 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Post open deploy tasks for region=hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:49:05,007 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Updating zk with meta location
[32mcluster2    |[0m 2017-01-17 19:49:05,007 INFO  [PostOpenDeployTasks:1588230740] zookeeper.ZooKeeperNodeTracker: Setting hbase:meta region location in ZooKeeper as cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,014 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Finished post open deploy task for hbase:meta,,1.1588230740
[32mcluster2    |[0m 2017-01-17 19:49:05,014 DEBUG [RS_OPEN_META-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioning 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:05,016 DEBUG [RS_OPEN_META-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioned node 1588230740 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:05,016 DEBUG [RS_OPEN_META-cluster2:41916-0] handler.OpenRegionHandler: Transitioned 1588230740 to OPENED in zk on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,016 DEBUG [RS_OPEN_META-cluster2:41916-0] handler.OpenRegionHandler: Opened hbase:meta,,1.1588230740 on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,016 DEBUG [AM.ZK.Worker-pool2-t2] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster2,41916,1484682517717, region=1588230740, current_state={1588230740 state=OPENING, ts=1484682524918, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,016 INFO  [AM.ZK.Worker-pool2-t2] master.RegionStates: Transition {1588230740 state=OPENING, ts=1484682524918, server=cluster2,41916,1484682517717} to {1588230740 state=OPEN, ts=1484682545016, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,018 INFO  [AM.ZK.Worker-pool2-t2] handler.OpenedRegionHandler: Handling OPENED of 1588230740 from cluster2,41916,1484682517717; deleting unassigned node
[32mcluster2    |[0m 2017-01-17 19:49:05,021 DEBUG [AM.ZK.Worker-pool2-t2] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Deleted unassigned node 1588230740 in expected state RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:05,023 DEBUG [AM.ZK.Worker-pool2-t3] master.AssignmentManager: Znode hbase:meta,,1.1588230740 deleted, state: {1588230740 state=OPEN, ts=1484682545016, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,023 INFO  [AM.ZK.Worker-pool2-t3] master.RegionStates: Onlined 1588230740 on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,024 INFO  [M:0;cluster2:42021] master.HMaster: hbase:meta assigned=1, rit=false, location=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,072 INFO  [M:0;cluster2:42021] catalog.MetaMigrationConvertingToPB: hbase:meta doesn't have any entries to update.
[32mcluster2    |[0m 2017-01-17 19:49:05,072 INFO  [M:0;cluster2:42021] catalog.MetaMigrationConvertingToPB: META already up-to date with PB serialization
[32mcluster2    |[0m 2017-01-17 19:49:05,080 DEBUG [M:0;cluster2:42021] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Deleting any existing unassigned nodes
[32mcluster2    |[0m 2017-01-17 19:49:05,081 INFO  [M:0;cluster2:42021] master.AssignmentManager: Clean cluster startup. Assigning user regions
[32mcluster2    |[0m 2017-01-17 19:49:05,082 INFO  [M:0;cluster2:42021] master.SnapshotOfRegionAssignmentFromMeta: Start to scan the hbase:meta for the current region assignment snappshot
[32mcluster2    |[0m 2017-01-17 19:49:05,086 INFO  [M:0;cluster2:42021] master.SnapshotOfRegionAssignmentFromMeta: Finished to scan the hbase:meta for the current region assignmentsnapshot
[32mcluster2    |[0m 2017-01-17 19:49:05,088 INFO  [M:0;cluster2:42021] master.AssignmentManager: Joined the cluster in 16ms, failover=false
[32mcluster2    |[0m 2017-01-17 19:49:05,097 INFO  [M:0;cluster2:42021] master.TableNamespaceManager: Namespace table not found. Creating...
[32mcluster2    |[0m 2017-01-17 19:49:05,109 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0000 type:create cxid:0x2914e zxid:0x29 txntype:-1 reqpath:n/a Error Path:/hbase/table-lock/hbase:namespace Error:KeeperErrorCode = NoNode for /hbase/table-lock/hbase:namespace
[32mcluster2    |[0m 2017-01-17 19:49:05,117 DEBUG [M:0;cluster2:42021] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/hbase:namespace/write-master:420210000000000
[32mcluster2    |[0m 2017-01-17 19:49:05,120 WARN  [M:0;cluster2:42021] zookeeper.ZKTable: Moving table hbase:namespace state to enabling but was not first in disabled state: null
[32mcluster2    |[0m 2017-01-17 19:49:05,124 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] handler.CreateTableHandler: Create table hbase:namespace
[32mcluster2    |[0m 2017-01-17 19:49:05,131 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/.tmp/data/hbase/namespace/.tabledesc/.tableinfo.0000000001
[32mcluster2    |[0m 2017-01-17 19:49:05,134 INFO  [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: creating HRegion hbase:namespace HTD == 'hbase:namespace', {NAME => 'info', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => 'FOREVER', MIN_VERSIONS => '0', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '8192', IN_MEMORY => 'true', BLOCKCACHE => 'true'} RootDir = file:/var/tmp/.tmp Table name == hbase:namespace
[32mcluster2    |[0m 2017-01-17 19:49:05,141 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Instantiated hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,141 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Closing hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.: disabling compactions & flushes
[32mcluster2    |[0m 2017-01-17 19:49:05,141 DEBUG [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Updates disabled for region hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,141 INFO  [RegionOpenAndInitThread-hbase:namespace-1] regionserver.HRegion: Closed hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,205 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] catalog.MetaEditor: Added 1
[32mcluster2    |[0m 2017-01-17 19:49:05,205 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.AssignmentManager: Assigning 1 region(s) to cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,207 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Async create of unassigned node c227570b295f03a6982d7768e6b48221 with OFFLINE state
[32mcluster2    |[0m 2017-01-17 19:49:05,211 DEBUG [main-EventThread] master.OfflineCallback: rs={c227570b295f03a6982d7768e6b48221 state=OFFLINE, ts=1484682545205, server=null}, server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,211 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback: rs={c227570b295f03a6982d7768e6b48221 state=OFFLINE, ts=1484682545205, server=null}, server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,212 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.AssignmentManager: cluster2,41916,1484682517717 unassigned znodes=1 of total=1
[32mcluster2    |[0m 2017-01-17 19:49:05,212 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.RegionStates: Transition {c227570b295f03a6982d7768e6b48221 state=OFFLINE, ts=1484682545207, server=null} to {c227570b295f03a6982d7768e6b48221 state=PENDING_OPEN, ts=1484682545212, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,214 INFO  [PriorityRpcServer.handler=1,queue=0,port=41916] regionserver.HRegionServer: Open hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,216 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioning c227570b295f03a6982d7768e6b48221 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[32mcluster2    |[0m 2017-01-17 19:49:05,217 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.AssignmentManager: Bulk assigning done for cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,219 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioned node c227570b295f03a6982d7768e6b48221 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[32mcluster2    |[0m 2017-01-17 19:49:05,219 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] regionserver.HRegion: Opening region: {ENCODED => c227570b295f03a6982d7768e6b48221, NAME => 'hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.', STARTKEY => '', ENDKEY => ''}
[32mcluster2    |[0m 2017-01-17 19:49:05,219 DEBUG [AM.ZK.Worker-pool2-t5] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster2,41916,1484682517717, region=c227570b295f03a6982d7768e6b48221, current_state={c227570b295f03a6982d7768e6b48221 state=PENDING_OPEN, ts=1484682545212, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,219 INFO  [AM.ZK.Worker-pool2-t5] master.RegionStates: Transition {c227570b295f03a6982d7768e6b48221 state=PENDING_OPEN, ts=1484682545212, server=cluster2,41916,1484682517717} to {c227570b295f03a6982d7768e6b48221 state=OPENING, ts=1484682545219, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,220 INFO  [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor hbase:namespace
[32mcluster2    |[0m 2017-01-17 19:49:05,220 INFO  [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[32mcluster2    |[0m 2017-01-17 19:49:05,221 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace c227570b295f03a6982d7768e6b48221
[32mcluster2    |[0m 2017-01-17 19:49:05,221 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] regionserver.HRegion: Instantiated hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,223 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] lock.ZKInterProcessLockBase: Released /hbase/table-lock/hbase:namespace/write-master:420210000000000
[32mcluster2    |[0m 2017-01-17 19:49:05,223 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] handler.CreateTableHandler: failed. null
[32mcluster2    |[0m 2017-01-17 19:49:05,225 INFO  [StoreOpener-c227570b295f03a6982d7768e6b48221-1] hfile.CacheConfig: Created cacheConfig for info: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[32mcluster2    |[0m 2017-01-17 19:49:05,225 INFO  [StoreOpener-c227570b295f03a6982d7768e6b48221-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[32mcluster2    |[0m 2017-01-17 19:49:05,227 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/hbase/namespace/c227570b295f03a6982d7768e6b48221
[32mcluster2    |[0m 2017-01-17 19:49:05,227 INFO  [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] regionserver.HRegion: Onlined c227570b295f03a6982d7768e6b48221; next sequenceid=1
[32mcluster2    |[0m 2017-01-17 19:49:05,227 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Attempting to retransition opening state of node c227570b295f03a6982d7768e6b48221
[32mcluster2    |[0m 2017-01-17 19:49:05,228 INFO  [PostOpenDeployTasks:c227570b295f03a6982d7768e6b48221] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,231 INFO  [PostOpenDeployTasks:c227570b295f03a6982d7768e6b48221] catalog.MetaEditor: Updated row hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221. with server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,231 INFO  [PostOpenDeployTasks:c227570b295f03a6982d7768e6b48221] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221.
[32mcluster2    |[0m 2017-01-17 19:49:05,232 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioning c227570b295f03a6982d7768e6b48221 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:05,234 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioned node c227570b295f03a6982d7768e6b48221 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:05,234 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] handler.OpenRegionHandler: Transitioned c227570b295f03a6982d7768e6b48221 to OPENED in zk on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,234 DEBUG [RS_OPEN_PRIORITY_REGION-cluster2:41916-0] handler.OpenRegionHandler: Opened hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221. on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,234 DEBUG [AM.ZK.Worker-pool2-t6] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster2,41916,1484682517717, region=c227570b295f03a6982d7768e6b48221, current_state={c227570b295f03a6982d7768e6b48221 state=OPENING, ts=1484682545219, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,234 INFO  [AM.ZK.Worker-pool2-t6] master.RegionStates: Transition {c227570b295f03a6982d7768e6b48221 state=OPENING, ts=1484682545219, server=cluster2,41916,1484682517717} to {c227570b295f03a6982d7768e6b48221 state=OPEN, ts=1484682545234, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,235 DEBUG [AM.ZK.Worker-pool2-t6] handler.OpenedRegionHandler: Handling OPENED of c227570b295f03a6982d7768e6b48221 from cluster2,41916,1484682517717; deleting unassigned node
[32mcluster2    |[0m 2017-01-17 19:49:05,237 DEBUG [AM.ZK.Worker-pool2-t6] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Deleted unassigned node c227570b295f03a6982d7768e6b48221 in expected state RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:05,239 DEBUG [AM.ZK.Worker-pool2-t8] master.AssignmentManager: Znode hbase:namespace,,1484682545097.c227570b295f03a6982d7768e6b48221. deleted, state: {c227570b295f03a6982d7768e6b48221 state=OPEN, ts=1484682545234, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:05,239 INFO  [AM.ZK.Worker-pool2-t8] master.RegionStates: Onlined c227570b295f03a6982d7768e6b48221 on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:05,350 DEBUG [M:0;cluster2:42021] client.ClientSmallScanner: Finished with small scan at {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
[32mcluster2    |[0m 2017-01-17 19:49:05,365 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node default with data: \x0A\x07default
[32mcluster2    |[0m 2017-01-17 19:49:05,373 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node default with data: \x0A\x07default
[32mcluster2    |[0m 2017-01-17 19:49:05,373 DEBUG [main-EventThread] hbase.ZKNamespaceManager: Updating namespace cache from node hbase with data: \x0A\x05hbase
[32mcluster2    |[0m 2017-01-17 19:49:05,377 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0000 type:create cxid:0x29170 zxid:0x39 txntype:-1 reqpath:n/a Error Path:/hbase/namespace/default Error:KeeperErrorCode = NodeExists for /hbase/namespace/default
[32mcluster2    |[0m 2017-01-17 19:49:05,378 INFO  [M:0;cluster2:42021] zookeeper.RecoverableZooKeeper: Node /hbase/namespace/default already exists and this is not a retry
[32mcluster2    |[0m 2017-01-17 19:49:05,380 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0000 type:create cxid:0x29173 zxid:0x3b txntype:-1 reqpath:n/a Error Path:/hbase/namespace/hbase Error:KeeperErrorCode = NodeExists for /hbase/namespace/hbase
[32mcluster2    |[0m 2017-01-17 19:49:05,382 INFO  [M:0;cluster2:42021] zookeeper.RecoverableZooKeeper: Node /hbase/namespace/hbase already exists and this is not a retry
[32mcluster2    |[0m 2017-01-17 19:49:05,384 INFO  [M:0;cluster2:42021] master.HMaster: Master has completed initialization
[32mcluster2    |[0m 2017-01-17 19:49:05,384 INFO  [M:0;cluster2:42021] zookeeper.ZooKeeperWatcher: not a secure deployment, proceeding
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 2017-01-17 19:49:07,519 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:49:07,519 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.5 port: 36144 with version info: version: "0.98.14-hadoop2" url: "git://buildbox/home/apurtell/src/hbase" revision: "4e4aabb93b52f1b0fef6b66edd06ec8923014dec" user: "apurtell" date: "Tue Aug 25 22:37:01 PDT 2015" src_checksum: "00fc92d840a39816e8f6155f5e81580a"
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:49:07,249 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:57936
[36mcluster3    |[0m 2017-01-17 19:49:07,250 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:57936
[36mcluster3    |[0m 2017-01-17 19:49:07,252 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0006 with negotiated timeout 40000 for client /172.19.0.5:57936
[36mcluster3    |[0m 2017-01-17 19:49:07,513 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:57946
[36mcluster3    |[0m 2017-01-17 19:49:07,513 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:57946
[36mcluster3    |[0m 2017-01-17 19:49:07,515 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0007 with negotiated timeout 40000 for client /172.19.0.5:57946
[36mcluster3    |[0m 2017-01-17 19:49:07,522 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94f4b0007
[36mcluster3    |[0m 2017-01-17 19:49:07,525 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:57946 which had sessionid 0x159adf94f4b0007
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 2017-01-17 19:49:07,477 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:49:07,477 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.5 port: 50102 with version info: version: "0.98.14-hadoop2" url: "git://buildbox/home/apurtell/src/hbase" revision: "4e4aabb93b52f1b0fef6b66edd06ec8923014dec" user: "apurtell" date: "Tue Aug 25 22:37:01 PDT 2015" src_checksum: "00fc92d840a39816e8f6155f5e81580a"
[33mcluster1    |[0m 2017-01-17 19:49:07,638 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:49:07,638 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.5 port: 42206 with version info: version: "0.98.14-hadoop2" url: "git://buildbox/home/apurtell/src/hbase" revision: "4e4aabb93b52f1b0fef6b66edd06ec8923014dec" user: "apurtell" date: "Tue Aug 25 22:37:01 PDT 2015" src_checksum: "00fc92d840a39816e8f6155f5e81580a"
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:49:06,967 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:34704
[33mcluster1    |[0m 2017-01-17 19:49:06,971 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:34704
[33mcluster1    |[0m 2017-01-17 19:49:06,973 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40006 with negotiated timeout 40000 for client /172.19.0.5:34704
[33mcluster1    |[0m 2017-01-17 19:49:07,262 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:34710
[33mcluster1    |[0m 2017-01-17 19:49:07,262 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:34710
[33mcluster1    |[0m 2017-01-17 19:49:07,264 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40007 with negotiated timeout 40000 for client /172.19.0.5:34710
[33mcluster1    |[0m 2017-01-17 19:49:07,494 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94fa40007
[33mcluster1    |[0m 2017-01-17 19:49:07,496 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:34710 which had sessionid 0x159adf94fa40007
[33mcluster1    |[0m 2017-01-17 19:49:07,672 INFO  [FifoRpcScheduler.handler1-thread-13] master.HMaster: Client=root//172.19.0.5 create 'roger', {NAME => 'cf1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'columns', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}
[33mcluster1    |[0m 2017-01-17 19:49:07,673 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94fa40000 type:create cxid:0x28540 zxid:0x41 txntype:-1 reqpath:n/a Error Path:/hbase/table-lock/roger Error:KeeperErrorCode = NoNode for /hbase/table-lock/roger
[33mcluster1    |[0m 2017-01-17 19:49:07,681 DEBUG [FifoRpcScheduler.handler1-thread-13] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/roger/write-master:371020000000000
[33mcluster1    |[0m 2017-01-17 19:49:07,691 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] handler.CreateTableHandler: Create table roger
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/SecurityAuth.audit <==
[32mcluster2    |[0m 2017-01-17 19:49:07,505 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:49:07,505 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.5 port: 40852 with version info: version: "0.98.14-hadoop2" url: "git://buildbox/home/apurtell/src/hbase" revision: "4e4aabb93b52f1b0fef6b66edd06ec8923014dec" user: "apurtell" date: "Tue Aug 25 22:37:01 PDT 2015" src_checksum: "00fc92d840a39816e8f6155f5e81580a"
[32mcluster2    |[0m 2017-01-17 19:49:07,638 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:49:07,638 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.5 port: 58060 with version info: version: "0.98.14-hadoop2" url: "git://buildbox/home/apurtell/src/hbase" revision: "4e4aabb93b52f1b0fef6b66edd06ec8923014dec" user: "apurtell" date: "Tue Aug 25 22:37:01 PDT 2015" src_checksum: "00fc92d840a39816e8f6155f5e81580a"
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:49:07,217 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:42398
[32mcluster2    |[0m 2017-01-17 19:49:07,218 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:42398
[32mcluster2    |[0m 2017-01-17 19:49:07,221 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0006 with negotiated timeout 40000 for client /172.19.0.5:42398
[32mcluster2    |[0m 2017-01-17 19:49:07,500 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:42406
[32mcluster2    |[0m 2017-01-17 19:49:07,500 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:42406
[32mcluster2    |[0m 2017-01-17 19:49:07,502 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0007 with negotiated timeout 40000 for client /172.19.0.5:42406
[32mcluster2    |[0m 2017-01-17 19:49:07,508 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf9552f0007
[32mcluster2    |[0m 2017-01-17 19:49:07,510 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:42406 which had sessionid 0x159adf9552f0007
[32mcluster2    |[0m 2017-01-17 19:49:07,670 INFO  [FifoRpcScheduler.handler1-thread-13] master.HMaster: Client=root//172.19.0.5 create 'roger', {NAME => 'cf1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'columns', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}
[32mcluster2    |[0m 2017-01-17 19:49:07,674 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf9552f0000 type:create cxid:0x29178 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/hbase/table-lock/roger Error:KeeperErrorCode = NoNode for /hbase/table-lock/roger
[32mcluster2    |[0m 2017-01-17 19:49:07,682 DEBUG [FifoRpcScheduler.handler1-thread-13] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/roger/write-master:420210000000000
[32mcluster2    |[0m 2017-01-17 19:49:07,695 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] handler.CreateTableHandler: Create table roger
[32mcluster2    |[0m 2017-01-17 19:49:07,708 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/.tmp/data/default/roger/.tabledesc/.tableinfo.0000000001
[32mcluster2    |[0m 2017-01-17 19:49:07,709 INFO  [RegionOpenAndInitThread-roger-1] regionserver.HRegion: creating HRegion roger HTD == 'roger', {NAME => 'cf1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'columns', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = file:/var/tmp/.tmp Table name == roger
[32mcluster2    |[0m 2017-01-17 19:49:07,717 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Instantiated roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,717 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Closing roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.: disabling compactions & flushes
[32mcluster2    |[0m 2017-01-17 19:49:07,717 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Updates disabled for region roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,717 INFO  [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Closed roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,721 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] catalog.MetaEditor: Added 1
[32mcluster2    |[0m 2017-01-17 19:49:07,722 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.AssignmentManager: Assigning 1 region(s) to cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,722 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Async create of unassigned node 430c1f5923f76d1ca948f3c40d61a498 with OFFLINE state
[32mcluster2    |[0m 2017-01-17 19:49:07,725 DEBUG [main-EventThread] master.OfflineCallback: rs={430c1f5923f76d1ca948f3c40d61a498 state=OFFLINE, ts=1484682547722, server=null}, server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,725 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback: rs={430c1f5923f76d1ca948f3c40d61a498 state=OFFLINE, ts=1484682547722, server=null}, server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,727 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.AssignmentManager: cluster2,41916,1484682517717 unassigned znodes=1 of total=1
[32mcluster2    |[0m 2017-01-17 19:49:07,728 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.RegionStates: Transition {430c1f5923f76d1ca948f3c40d61a498 state=OFFLINE, ts=1484682547722, server=null} to {430c1f5923f76d1ca948f3c40d61a498 state=PENDING_OPEN, ts=1484682547727, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:07,728 INFO  [PriorityRpcServer.handler=3,queue=0,port=41916] regionserver.HRegionServer: Open roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,732 DEBUG [RS_OPEN_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioning 430c1f5923f76d1ca948f3c40d61a498 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[32mcluster2    |[0m 2017-01-17 19:49:07,733 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] master.AssignmentManager: Bulk assigning done for cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,739 DEBUG [RS_OPEN_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioned node 430c1f5923f76d1ca948f3c40d61a498 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 2017-01-17 19:49:07,638 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:49:07,638 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.5 port: 47226 with version info: version: "0.98.14-hadoop2" url: "git://buildbox/home/apurtell/src/hbase" revision: "4e4aabb93b52f1b0fef6b66edd06ec8923014dec" user: "apurtell" date: "Tue Aug 25 22:37:01 PDT 2015" src_checksum: "00fc92d840a39816e8f6155f5e81580a"
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:49:07,670 INFO  [FifoRpcScheduler.handler1-thread-13] master.HMaster: Client=root//172.19.0.5 create 'roger', {NAME => 'cf1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'columns', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}
[36mcluster3    |[0m 2017-01-17 19:49:07,671 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x159adf94f4b0000 type:create cxid:0x276aa zxid:0x43 txntype:-1 reqpath:n/a Error Path:/hbase/table-lock/roger Error:KeeperErrorCode = NoNode for /hbase/table-lock/roger
[36mcluster3    |[0m 2017-01-17 19:49:07,678 DEBUG [FifoRpcScheduler.handler1-thread-13] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/roger/write-master:362410000000000
[36mcluster3    |[0m 2017-01-17 19:49:07,687 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] handler.CreateTableHandler: Create table roger
[36mcluster3    |[0m 2017-01-17 19:49:07,696 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/.tmp/data/default/roger/.tabledesc/.tableinfo.0000000001
[36mcluster3    |[0m 2017-01-17 19:49:07,699 INFO  [RegionOpenAndInitThread-roger-1] regionserver.HRegion: creating HRegion roger HTD == 'roger', {NAME => 'cf1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'columns', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = file:/var/tmp/.tmp Table name == roger
[36mcluster3    |[0m 2017-01-17 19:49:07,708 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Instantiated roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,708 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Closing roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.: disabling compactions & flushes
[36mcluster3    |[0m 2017-01-17 19:49:07,708 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Updates disabled for region roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,708 INFO  [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Closed roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,712 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] catalog.MetaEditor: Added 1
[36mcluster3    |[0m 2017-01-17 19:49:07,712 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.AssignmentManager: Assigning 1 region(s) to cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,713 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Async create of unassigned node 7c42436e884b40a75ea848f0fbb3975a with OFFLINE state
[36mcluster3    |[0m 2017-01-17 19:49:07,716 DEBUG [main-EventThread] master.OfflineCallback: rs={7c42436e884b40a75ea848f0fbb3975a state=OFFLINE, ts=1484682547712, server=null}, server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,717 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback: rs={7c42436e884b40a75ea848f0fbb3975a state=OFFLINE, ts=1484682547712, server=null}, server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,718 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.AssignmentManager: cluster3,42333,1484682515998 unassigned znodes=1 of total=1
[36mcluster3    |[0m 2017-01-17 19:49:07,718 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.RegionStates: Transition {7c42436e884b40a75ea848f0fbb3975a state=OFFLINE, ts=1484682547713, server=null} to {7c42436e884b40a75ea848f0fbb3975a state=PENDING_OPEN, ts=1484682547718, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:07,719 INFO  [PriorityRpcServer.handler=0,queue=0,port=42333] regionserver.HRegionServer: Open roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,721 DEBUG [RS_OPEN_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioning 7c42436e884b40a75ea848f0fbb3975a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 2017-01-17 19:49:07,721 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] master.AssignmentManager: Bulk assigning done for cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,728 DEBUG [RS_OPEN_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioned node 7c42436e884b40a75ea848f0fbb3975a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[36mcluster3    |[0m 2017-01-17 19:49:07,729 DEBUG [RS_OPEN_REGION-cluster3:42333-0] regionserver.HRegion: Opening region: {ENCODED => 7c42436e884b40a75ea848f0fbb3975a, NAME => 'roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.', STARTKEY => '', ENDKEY => ''}
[36mcluster3    |[0m 2017-01-17 19:49:07,729 INFO  [RS_OPEN_REGION-cluster3:42333-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor roger
[36mcluster3    |[0m 2017-01-17 19:49:07,729 INFO  [RS_OPEN_REGION-cluster3:42333-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[36mcluster3    |[0m 2017-01-17 19:49:07,729 DEBUG [RS_OPEN_REGION-cluster3:42333-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table roger 7c42436e884b40a75ea848f0fbb3975a
[36mcluster3    |[0m 2017-01-17 19:49:07,729 DEBUG [RS_OPEN_REGION-cluster3:42333-0] regionserver.HRegion: Instantiated roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,730 DEBUG [MASTER_TABLE_OPERATIONS-cluster3:36241-0] lock.ZKInterProcessLockBase: Released /hbase/table-lock/roger/write-master:362410000000000
[36mcluster3    |[0m 2017-01-17 19:49:07,730 INFO  [MASTER_TABLE_OPERATIONS-cluster3:36241-0] handler.CreateTableHandler: failed. null
[36mcluster3    |[0m 2017-01-17 19:49:07,731 DEBUG [AM.ZK.Worker-pool2-t10] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster3,42333,1484682515998, region=7c42436e884b40a75ea848f0fbb3975a, current_state={7c42436e884b40a75ea848f0fbb3975a state=PENDING_OPEN, ts=1484682547718, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:07,731 INFO  [AM.ZK.Worker-pool2-t10] master.RegionStates: Transition {7c42436e884b40a75ea848f0fbb3975a state=PENDING_OPEN, ts=1484682547718, server=cluster3,42333,1484682515998} to {7c42436e884b40a75ea848f0fbb3975a state=OPENING, ts=1484682547731, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:07,735 INFO  [StoreOpener-7c42436e884b40a75ea848f0fbb3975a-1] hfile.CacheConfig: Created cacheConfig for cf1: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 2017-01-17 19:49:07,736 INFO  [StoreOpener-7c42436e884b40a75ea848f0fbb3975a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[36mcluster3    |[0m 2017-01-17 19:49:07,740 INFO  [StoreOpener-7c42436e884b40a75ea848f0fbb3975a-1] hfile.CacheConfig: Created cacheConfig for columns: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[36mcluster3    |[0m 2017-01-17 19:49:07,741 INFO  [StoreOpener-7c42436e884b40a75ea848f0fbb3975a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[36mcluster3    |[0m 2017-01-17 19:49:07,743 DEBUG [RS_OPEN_REGION-cluster3:42333-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/default/roger/7c42436e884b40a75ea848f0fbb3975a
[36mcluster3    |[0m 2017-01-17 19:49:07,743 INFO  [RS_OPEN_REGION-cluster3:42333-0] regionserver.HRegion: Onlined 7c42436e884b40a75ea848f0fbb3975a; next sequenceid=1
[36mcluster3    |[0m 2017-01-17 19:49:07,743 DEBUG [RS_OPEN_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Attempting to retransition opening state of node 7c42436e884b40a75ea848f0fbb3975a
[36mcluster3    |[0m 2017-01-17 19:49:07,745 INFO  [PostOpenDeployTasks:7c42436e884b40a75ea848f0fbb3975a] regionserver.HRegionServer: Post open deploy tasks for region=roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,750 INFO  [PostOpenDeployTasks:7c42436e884b40a75ea848f0fbb3975a] catalog.MetaEditor: Updated row roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a. with server=cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,751 INFO  [PostOpenDeployTasks:7c42436e884b40a75ea848f0fbb3975a] regionserver.HRegionServer: Finished post open deploy task for roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a.
[36mcluster3    |[0m 2017-01-17 19:49:07,751 DEBUG [RS_OPEN_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioning 7c42436e884b40a75ea848f0fbb3975a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:07,754 DEBUG [RS_OPEN_REGION-cluster3:42333-0] zookeeper.ZKAssign: regionserver:42333-0x159adf94f4b0001, quorum=localhost:18262, baseZNode=/hbase Transitioned node 7c42436e884b40a75ea848f0fbb3975a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:07,754 DEBUG [RS_OPEN_REGION-cluster3:42333-0] handler.OpenRegionHandler: Transitioned 7c42436e884b40a75ea848f0fbb3975a to OPENED in zk on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,754 DEBUG [RS_OPEN_REGION-cluster3:42333-0] handler.OpenRegionHandler: Opened roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a. on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,755 DEBUG [AM.ZK.Worker-pool2-t11] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster3,42333,1484682515998, region=7c42436e884b40a75ea848f0fbb3975a, current_state={7c42436e884b40a75ea848f0fbb3975a state=OPENING, ts=1484682547731, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:07,755 INFO  [AM.ZK.Worker-pool2-t11] master.RegionStates: Transition {7c42436e884b40a75ea848f0fbb3975a state=OPENING, ts=1484682547731, server=cluster3,42333,1484682515998} to {7c42436e884b40a75ea848f0fbb3975a state=OPEN, ts=1484682547755, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:07,755 DEBUG [AM.ZK.Worker-pool2-t11] handler.OpenedRegionHandler: Handling OPENED of 7c42436e884b40a75ea848f0fbb3975a from cluster3,42333,1484682515998; deleting unassigned node
[36mcluster3    |[0m 2017-01-17 19:49:07,757 DEBUG [AM.ZK.Worker-pool2-t11] zookeeper.ZKAssign: master:36241-0x159adf94f4b0000, quorum=localhost:18262, baseZNode=/hbase Deleted unassigned node 7c42436e884b40a75ea848f0fbb3975a in expected state RS_ZK_REGION_OPENED
[36mcluster3    |[0m 2017-01-17 19:49:07,757 DEBUG [AM.ZK.Worker-pool2-t13] master.AssignmentManager: Znode roger,,1484682547669.7c42436e884b40a75ea848f0fbb3975a. deleted, state: {7c42436e884b40a75ea848f0fbb3975a state=OPEN, ts=1484682547755, server=cluster3,42333,1484682515998}
[36mcluster3    |[0m 2017-01-17 19:49:07,758 INFO  [AM.ZK.Worker-pool2-t13] master.RegionStates: Onlined 7c42436e884b40a75ea848f0fbb3975a on cluster3,42333,1484682515998
[36mcluster3    |[0m 2017-01-17 19:49:07,843 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:57960
[36mcluster3    |[0m 2017-01-17 19:49:07,843 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:57960
[36mcluster3    |[0m 2017-01-17 19:49:07,847 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94f4b0008 with negotiated timeout 40000 for client /172.19.0.5:57960
[36mcluster3    |[0m 2017-01-17 19:49:07,854 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94f4b0008
[36mcluster3    |[0m 2017-01-17 19:49:07,856 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:57960 which had sessionid 0x159adf94f4b0008
[36mcluster3    |[0m 2017-01-17 19:49:08,440 WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxn: caught end of stream exception
[36mcluster3    |[0m EndOfStreamException: Unable to read additional data from client sessionid 0x159adf94f4b0006, likely client has closed socket
[36mcluster3    |[0m 	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
[36mcluster3    |[0m 	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
[36mcluster3    |[0m 	at java.lang.Thread.run(Thread.java:745)
[36mcluster3    |[0m 2017-01-17 19:49:08,442 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:57936 which had sessionid 0x159adf94f4b0006
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.out <==
[36mcluster3    |[0m table name is roger
[36mcluster3    |[0m RequestID 1
[36mcluster3    |[0m Next in SecureRegionScanner was issued 
[36mcluster3    |[0m Number of results is 11
[36mcluster3    |[0m Found protected value
[36mcluster3    |[0m qualifier is shareKey
[36mcluster3    |[0m Going to do search for rowID 0
[36mcluster3    |[0m Search with the protectedValue 1685781476737299124432762383843
[36mcluster3    |[0m Going to evaluate SearchValue 
[36mcluster3    |[0m Row id 0
[36mcluster3    |[0m First Value 978631347916654606052818457436
[36mcluster3    |[0m Second Value 1685781476737299124432762383843
[36mcluster3    |[0m Going to run protocol Equal
[36mcluster3    |[0m protocol executed
[36mcluster3    |[0m Player is targetPlayer false
[36mcluster3    |[0m Going to clean results 
[36mcluster3    |[0m Protocol result is false
[36mcluster3    |[0m After Search for match. HasMore: false; MatchFound: false
[36mcluster3    |[0m Next result is false
[36mcluster3    |[0m The filtered result found are: 
[33mcluster1    |[0m 2017-01-17 19:49:07,705 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] util.FSTableDescriptors: Wrote descriptor into: file:/var/tmp/.tmp/data/default/roger/.tabledesc/.tableinfo.0000000001
[33mcluster1    |[0m 2017-01-17 19:49:07,707 INFO  [RegionOpenAndInitThread-roger-1] regionserver.HRegion: creating HRegion roger HTD == 'roger', {NAME => 'cf1', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'columns', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = file:/var/tmp/.tmp Table name == roger
[33mcluster1    |[0m 2017-01-17 19:49:07,717 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Instantiated roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,717 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Closing roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.: disabling compactions & flushes
[33mcluster1    |[0m 2017-01-17 19:49:07,717 DEBUG [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Updates disabled for region roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,717 INFO  [RegionOpenAndInitThread-roger-1] regionserver.HRegion: Closed roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,721 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] catalog.MetaEditor: Added 1
[33mcluster1    |[0m 2017-01-17 19:49:07,722 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.AssignmentManager: Assigning 1 region(s) to cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,722 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Async create of unassigned node 74cf3bc1c04862d41c348505202c7ebc with OFFLINE state
[33mcluster1    |[0m 2017-01-17 19:49:07,725 DEBUG [main-EventThread] master.OfflineCallback: rs={74cf3bc1c04862d41c348505202c7ebc state=OFFLINE, ts=1484682547722, server=null}, server=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,725 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback: rs={74cf3bc1c04862d41c348505202c7ebc state=OFFLINE, ts=1484682547722, server=null}, server=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,728 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.AssignmentManager: cluster1,38372,1484682516063 unassigned znodes=1 of total=1
[33mcluster1    |[0m 2017-01-17 19:49:07,728 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.RegionStates: Transition {74cf3bc1c04862d41c348505202c7ebc state=OFFLINE, ts=1484682547722, server=null} to {74cf3bc1c04862d41c348505202c7ebc state=PENDING_OPEN, ts=1484682547728, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:07,729 INFO  [PriorityRpcServer.handler=3,queue=0,port=38372] regionserver.HRegionServer: Open roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,735 DEBUG [RS_OPEN_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioning 74cf3bc1c04862d41c348505202c7ebc from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[33mcluster1    |[0m 2017-01-17 19:49:07,736 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] master.AssignmentManager: Bulk assigning done for cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,738 DEBUG [RS_OPEN_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioned node 74cf3bc1c04862d41c348505202c7ebc from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[33mcluster1    |[0m 2017-01-17 19:49:07,738 DEBUG [RS_OPEN_REGION-cluster1:38372-0] regionserver.HRegion: Opening region: {ENCODED => 74cf3bc1c04862d41c348505202c7ebc, NAME => 'roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.', STARTKEY => '', ENDKEY => ''}
[33mcluster1    |[0m 2017-01-17 19:49:07,738 DEBUG [AM.ZK.Worker-pool2-t10] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster1,38372,1484682516063, region=74cf3bc1c04862d41c348505202c7ebc, current_state={74cf3bc1c04862d41c348505202c7ebc state=PENDING_OPEN, ts=1484682547728, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:07,738 INFO  [AM.ZK.Worker-pool2-t10] master.RegionStates: Transition {74cf3bc1c04862d41c348505202c7ebc state=PENDING_OPEN, ts=1484682547728, server=cluster1,38372,1484682516063} to {74cf3bc1c04862d41c348505202c7ebc state=OPENING, ts=1484682547738, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:07,739 INFO  [RS_OPEN_REGION-cluster1:38372-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor roger
[33mcluster1    |[0m 2017-01-17 19:49:07,739 INFO  [RS_OPEN_REGION-cluster1:38372-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[33mcluster1    |[0m 2017-01-17 19:49:07,739 DEBUG [RS_OPEN_REGION-cluster1:38372-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table roger 74cf3bc1c04862d41c348505202c7ebc
[33mcluster1    |[0m 2017-01-17 19:49:07,739 DEBUG [RS_OPEN_REGION-cluster1:38372-0] regionserver.HRegion: Instantiated roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,744 INFO  [StoreOpener-74cf3bc1c04862d41c348505202c7ebc-1] hfile.CacheConfig: Created cacheConfig for cf1: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[33mcluster1    |[0m 2017-01-17 19:49:07,744 INFO  [StoreOpener-74cf3bc1c04862d41c348505202c7ebc-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[33mcluster1    |[0m 2017-01-17 19:49:07,745 DEBUG [MASTER_TABLE_OPERATIONS-cluster1:37102-0] lock.ZKInterProcessLockBase: Released /hbase/table-lock/roger/write-master:371020000000000
[33mcluster1    |[0m 2017-01-17 19:49:07,745 INFO  [MASTER_TABLE_OPERATIONS-cluster1:37102-0] handler.CreateTableHandler: failed. null
[33mcluster1    |[0m 2017-01-17 19:49:07,750 INFO  [StoreOpener-74cf3bc1c04862d41c348505202c7ebc-1] hfile.CacheConfig: Created cacheConfig for columns: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[33mcluster1    |[0m 2017-01-17 19:49:07,751 INFO  [StoreOpener-74cf3bc1c04862d41c348505202c7ebc-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[33mcluster1    |[0m 2017-01-17 19:49:07,753 DEBUG [RS_OPEN_REGION-cluster1:38372-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/default/roger/74cf3bc1c04862d41c348505202c7ebc
[33mcluster1    |[0m 2017-01-17 19:49:07,753 INFO  [RS_OPEN_REGION-cluster1:38372-0] regionserver.HRegion: Onlined 74cf3bc1c04862d41c348505202c7ebc; next sequenceid=1
[33mcluster1    |[0m 2017-01-17 19:49:07,753 DEBUG [RS_OPEN_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Attempting to retransition opening state of node 74cf3bc1c04862d41c348505202c7ebc
[33mcluster1    |[0m 2017-01-17 19:49:07,754 INFO  [PostOpenDeployTasks:74cf3bc1c04862d41c348505202c7ebc] regionserver.HRegionServer: Post open deploy tasks for region=roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,758 INFO  [PostOpenDeployTasks:74cf3bc1c04862d41c348505202c7ebc] catalog.MetaEditor: Updated row roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc. with server=cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,758 INFO  [PostOpenDeployTasks:74cf3bc1c04862d41c348505202c7ebc] regionserver.HRegionServer: Finished post open deploy task for roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc.
[33mcluster1    |[0m 2017-01-17 19:49:07,758 DEBUG [RS_OPEN_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioning 74cf3bc1c04862d41c348505202c7ebc from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:07,761 DEBUG [RS_OPEN_REGION-cluster1:38372-0] zookeeper.ZKAssign: regionserver:38372-0x159adf94fa40001, quorum=localhost:16262, baseZNode=/hbase Transitioned node 74cf3bc1c04862d41c348505202c7ebc from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:07,761 DEBUG [RS_OPEN_REGION-cluster1:38372-0] handler.OpenRegionHandler: Transitioned 74cf3bc1c04862d41c348505202c7ebc to OPENED in zk on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,761 DEBUG [RS_OPEN_REGION-cluster1:38372-0] handler.OpenRegionHandler: Opened roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc. on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,761 DEBUG [AM.ZK.Worker-pool2-t11] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster1,38372,1484682516063, region=74cf3bc1c04862d41c348505202c7ebc, current_state={74cf3bc1c04862d41c348505202c7ebc state=OPENING, ts=1484682547738, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:07,761 INFO  [AM.ZK.Worker-pool2-t11] master.RegionStates: Transition {74cf3bc1c04862d41c348505202c7ebc state=OPENING, ts=1484682547738, server=cluster1,38372,1484682516063} to {74cf3bc1c04862d41c348505202c7ebc state=OPEN, ts=1484682547761, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:07,761 DEBUG [AM.ZK.Worker-pool2-t11] handler.OpenedRegionHandler: Handling OPENED of 74cf3bc1c04862d41c348505202c7ebc from cluster1,38372,1484682516063; deleting unassigned node
[33mcluster1    |[0m 2017-01-17 19:49:07,763 DEBUG [AM.ZK.Worker-pool2-t11] zookeeper.ZKAssign: master:37102-0x159adf94fa40000, quorum=localhost:16262, baseZNode=/hbase Deleted unassigned node 74cf3bc1c04862d41c348505202c7ebc in expected state RS_ZK_REGION_OPENED
[33mcluster1    |[0m 2017-01-17 19:49:07,764 DEBUG [AM.ZK.Worker-pool2-t13] master.AssignmentManager: Znode roger,,1484682547671.74cf3bc1c04862d41c348505202c7ebc. deleted, state: {74cf3bc1c04862d41c348505202c7ebc state=OPEN, ts=1484682547761, server=cluster1,38372,1484682516063}
[33mcluster1    |[0m 2017-01-17 19:49:07,764 INFO  [AM.ZK.Worker-pool2-t13] master.RegionStates: Onlined 74cf3bc1c04862d41c348505202c7ebc on cluster1,38372,1484682516063
[33mcluster1    |[0m 2017-01-17 19:49:07,843 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:34728
[33mcluster1    |[0m 2017-01-17 19:49:07,843 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:34728
[33mcluster1    |[0m 2017-01-17 19:49:07,847 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf94fa40008 with negotiated timeout 40000 for client /172.19.0.5:34728
[33mcluster1    |[0m 2017-01-17 19:49:07,856 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94fa40008
[33mcluster1    |[0m 2017-01-17 19:49:07,858 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:34728 which had sessionid 0x159adf94fa40008
[33mcluster1    |[0m 2017-01-17 19:49:08,440 WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxn: caught end of stream exception
[33mcluster1    |[0m EndOfStreamException: Unable to read additional data from client sessionid 0x159adf94fa40006, likely client has closed socket
[33mcluster1    |[0m 	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
[33mcluster1    |[0m 	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
[33mcluster1    |[0m 	at java.lang.Thread.run(Thread.java:745)
[33mcluster1    |[0m 2017-01-17 19:49:08,442 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:16262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:34704 which had sessionid 0x159adf94fa40006
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.out <==
[33mcluster1    |[0m table name is roger
[33mcluster1    |[0m RequestID 1
[33mcluster1    |[0m Next in SecureRegionScanner was issued 
[33mcluster1    |[0m Number of results is 11
[33mcluster1    |[0m Found protected value
[33mcluster1    |[0m qualifier is shareKey
[33mcluster1    |[0m Going to do search for rowID 0
[33mcluster1    |[0m Search with the protectedValue 1527474721554302683441081750062
[33mcluster1    |[0m Going to evaluate SearchValue 
[33mcluster1    |[0m Row id 0
[33mcluster1    |[0m First Value 2411378514340665724092523040824
[33mcluster1    |[0m Second Value 1527474721554302683441081750062
[33mcluster1    |[0m Going to run protocol Equal
[33mcluster1    |[0m protocol executed
[33mcluster1    |[0m Player is targetPlayer false
[33mcluster1    |[0m Going to clean results 
[33mcluster1    |[0m Protocol result is false
[33mcluster1    |[0m After Search for match. HasMore: false; MatchFound: false
[33mcluster1    |[0m Next result is false
[33mcluster1    |[0m The filtered result found are: 
[32mcluster2    |[0m 2017-01-17 19:49:07,739 DEBUG [RS_OPEN_REGION-cluster2:41916-0] regionserver.HRegion: Opening region: {ENCODED => 430c1f5923f76d1ca948f3c40d61a498, NAME => 'roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.', STARTKEY => '', ENDKEY => ''}
[32mcluster2    |[0m 2017-01-17 19:49:07,740 INFO  [RS_OPEN_REGION-cluster2:41916-0] smcoprocessors.SmpcCoprocessor: Starting coprocessor roger
[32mcluster2    |[0m 2017-01-17 19:49:07,740 INFO  [RS_OPEN_REGION-cluster2:41916-0] coprocessor.CoprocessorHost: System coprocessor pt.uminho.haslab.smcoprocessors.SmpcCoprocessor was loaded successfully with priority (536870911).
[32mcluster2    |[0m 2017-01-17 19:49:07,740 DEBUG [RS_OPEN_REGION-cluster2:41916-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table roger 430c1f5923f76d1ca948f3c40d61a498
[32mcluster2    |[0m 2017-01-17 19:49:07,740 DEBUG [RS_OPEN_REGION-cluster2:41916-0] regionserver.HRegion: Instantiated roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,741 DEBUG [MASTER_TABLE_OPERATIONS-cluster2:42021-0] lock.ZKInterProcessLockBase: Released /hbase/table-lock/roger/write-master:420210000000000
[32mcluster2    |[0m 2017-01-17 19:49:07,741 INFO  [MASTER_TABLE_OPERATIONS-cluster2:42021-0] handler.CreateTableHandler: failed. null
[32mcluster2    |[0m 2017-01-17 19:49:07,741 DEBUG [AM.ZK.Worker-pool2-t10] master.AssignmentManager: Handling RS_ZK_REGION_OPENING, server=cluster2,41916,1484682517717, region=430c1f5923f76d1ca948f3c40d61a498, current_state={430c1f5923f76d1ca948f3c40d61a498 state=PENDING_OPEN, ts=1484682547727, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:07,742 INFO  [AM.ZK.Worker-pool2-t10] master.RegionStates: Transition {430c1f5923f76d1ca948f3c40d61a498 state=PENDING_OPEN, ts=1484682547727, server=cluster2,41916,1484682517717} to {430c1f5923f76d1ca948f3c40d61a498 state=OPENING, ts=1484682547742, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:07,744 INFO  [StoreOpener-430c1f5923f76d1ca948f3c40d61a498-1] hfile.CacheConfig: Created cacheConfig for cf1: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[32mcluster2    |[0m 2017-01-17 19:49:07,745 INFO  [StoreOpener-430c1f5923f76d1ca948f3c40d61a498-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[32mcluster2    |[0m 2017-01-17 19:49:07,750 INFO  [StoreOpener-430c1f5923f76d1ca948f3c40d61a498-1] hfile.CacheConfig: Created cacheConfig for columns: CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheDataCompressed=false] [prefetchOnOpen=false]
[32mcluster2    |[0m 2017-01-17 19:49:07,751 INFO  [StoreOpener-430c1f5923f76d1ca948f3c40d61a498-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; major period 604800000, major jitter 0.500000, min locality to compact 0.000000; tiered compaction: max_age 9223372036854775807, incoming window min 6, compaction policy for tiered window org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy, single output for minor true, compaction window factory org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory
[32mcluster2    |[0m 2017-01-17 19:49:07,753 DEBUG [RS_OPEN_REGION-cluster2:41916-0] regionserver.HRegion: Found 0 recovered edits file(s) under file:/var/tmp/data/default/roger/430c1f5923f76d1ca948f3c40d61a498
[32mcluster2    |[0m 2017-01-17 19:49:07,753 INFO  [RS_OPEN_REGION-cluster2:41916-0] regionserver.HRegion: Onlined 430c1f5923f76d1ca948f3c40d61a498; next sequenceid=1
[32mcluster2    |[0m 2017-01-17 19:49:07,753 DEBUG [RS_OPEN_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Attempting to retransition opening state of node 430c1f5923f76d1ca948f3c40d61a498
[32mcluster2    |[0m 2017-01-17 19:49:07,754 INFO  [PostOpenDeployTasks:430c1f5923f76d1ca948f3c40d61a498] regionserver.HRegionServer: Post open deploy tasks for region=roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,758 INFO  [PostOpenDeployTasks:430c1f5923f76d1ca948f3c40d61a498] catalog.MetaEditor: Updated row roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498. with server=cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,758 INFO  [PostOpenDeployTasks:430c1f5923f76d1ca948f3c40d61a498] regionserver.HRegionServer: Finished post open deploy task for roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498.
[32mcluster2    |[0m 2017-01-17 19:49:07,758 DEBUG [RS_OPEN_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioning 430c1f5923f76d1ca948f3c40d61a498 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:07,763 DEBUG [RS_OPEN_REGION-cluster2:41916-0] zookeeper.ZKAssign: regionserver:41916-0x159adf9552f0001, quorum=localhost:17262, baseZNode=/hbase Transitioned node 430c1f5923f76d1ca948f3c40d61a498 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:07,763 DEBUG [RS_OPEN_REGION-cluster2:41916-0] handler.OpenRegionHandler: Transitioned 430c1f5923f76d1ca948f3c40d61a498 to OPENED in zk on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,763 DEBUG [RS_OPEN_REGION-cluster2:41916-0] handler.OpenRegionHandler: Opened roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498. on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,765 DEBUG [AM.ZK.Worker-pool2-t11] master.AssignmentManager: Handling RS_ZK_REGION_OPENED, server=cluster2,41916,1484682517717, region=430c1f5923f76d1ca948f3c40d61a498, current_state={430c1f5923f76d1ca948f3c40d61a498 state=OPENING, ts=1484682547742, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:07,765 INFO  [AM.ZK.Worker-pool2-t11] master.RegionStates: Transition {430c1f5923f76d1ca948f3c40d61a498 state=OPENING, ts=1484682547742, server=cluster2,41916,1484682517717} to {430c1f5923f76d1ca948f3c40d61a498 state=OPEN, ts=1484682547765, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:07,765 DEBUG [AM.ZK.Worker-pool2-t11] handler.OpenedRegionHandler: Handling OPENED of 430c1f5923f76d1ca948f3c40d61a498 from cluster2,41916,1484682517717; deleting unassigned node
[32mcluster2    |[0m 2017-01-17 19:49:07,767 DEBUG [AM.ZK.Worker-pool2-t11] zookeeper.ZKAssign: master:42021-0x159adf9552f0000, quorum=localhost:17262, baseZNode=/hbase Deleted unassigned node 430c1f5923f76d1ca948f3c40d61a498 in expected state RS_ZK_REGION_OPENED
[32mcluster2    |[0m 2017-01-17 19:49:07,768 DEBUG [AM.ZK.Worker-pool2-t13] master.AssignmentManager: Znode roger,,1484682547668.430c1f5923f76d1ca948f3c40d61a498. deleted, state: {430c1f5923f76d1ca948f3c40d61a498 state=OPEN, ts=1484682547765, server=cluster2,41916,1484682517717}
[32mcluster2    |[0m 2017-01-17 19:49:07,768 INFO  [AM.ZK.Worker-pool2-t13] master.RegionStates: Onlined 430c1f5923f76d1ca948f3c40d61a498 on cluster2,41916,1484682517717
[32mcluster2    |[0m 2017-01-17 19:49:07,843 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxnFactory: Accepted socket connection from /172.19.0.5:42422
[32mcluster2    |[0m 2017-01-17 19:49:07,843 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.ZooKeeperServer: Client attempting to establish new session at /172.19.0.5:42422
[32mcluster2    |[0m 2017-01-17 19:49:07,845 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x159adf9552f0008 with negotiated timeout 40000 for client /172.19.0.5:42422
[32mcluster2    |[0m 2017-01-17 19:49:07,857 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf9552f0008
[32mcluster2    |[0m 2017-01-17 19:49:07,859 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:42422 which had sessionid 0x159adf9552f0008
[32mcluster2    |[0m 2017-01-17 19:49:08,448 WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxn: caught end of stream exception
[32mcluster2    |[0m EndOfStreamException: Unable to read additional data from client sessionid 0x159adf9552f0006, likely client has closed socket
[32mcluster2    |[0m 	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
[32mcluster2    |[0m 	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
[32mcluster2    |[0m 	at java.lang.Thread.run(Thread.java:745)
[32mcluster2    |[0m 2017-01-17 19:49:08,450 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:17262] server.NIOServerCnxn: Closed socket connection for client /172.19.0.5:42398 which had sessionid 0x159adf9552f0006
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.out <==
[32mcluster2    |[0m table name is roger
[32mcluster2    |[0m RequestID 1
[32mcluster2    |[0m Next in SecureRegionScanner was issued 
[32mcluster2    |[0m Number of results is 11
[32mcluster2    |[0m Found protected value
[32mcluster2    |[0m qualifier is shareKey
[32mcluster2    |[0m Going to do search for rowID 0
[32mcluster2    |[0m Search with the protectedValue 1857346202623724169977669722336
[32mcluster2    |[0m Going to evaluate SearchValue 
[32mcluster2    |[0m Row id 0
[32mcluster2    |[0m First Value 1680592538663294607240578318831
[32mcluster2    |[0m Second Value 1857346202623724169977669722336
[32mcluster2    |[0m Going to run protocol Equal
[32mcluster2    |[0m protocol executed
[32mcluster2    |[0m Player is targetPlayer true
[32mcluster2    |[0m Going to search for a matching index
[32mcluster2    |[0m Going to clean results 
[32mcluster2    |[0m Protocol result is false
[32mcluster2    |[0m After Search for match. HasMore: false; MatchFound: false
[32mcluster2    |[0m Next result is false
[32mcluster2    |[0m The filtered result found are: 
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:49:48,000 INFO  [SessionTracker] server.ZooKeeperServer: Expiring session 0x159adf94f4b0006, timeout of 40000ms exceeded
[36mcluster3    |[0m 2017-01-17 19:49:48,001 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94f4b0006
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:49:48,000 INFO  [SessionTracker] server.ZooKeeperServer: Expiring session 0x159adf94fa40006, timeout of 40000ms exceeded
[33mcluster1    |[0m 2017-01-17 19:49:48,000 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94fa40006
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:49:48,001 INFO  [SessionTracker] server.ZooKeeperServer: Expiring session 0x159adf9552f0006, timeout of 40000ms exceeded
[32mcluster2    |[0m 2017-01-17 19:49:48,001 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf9552f0006
[36mcluster3    |[0m 2017-01-17 19:53:36,040 DEBUG [LruStats #0] hfile.LruBlockCache: Total=407.30 KB, free=386.28 MB, max=386.68 MB, blockCount=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=30, evicted=0, evictedPerRun=0.0
[33mcluster1    |[0m 2017-01-17 19:53:36,076 DEBUG [LruStats #0] hfile.LruBlockCache: Total=407.30 KB, free=386.28 MB, max=386.68 MB, blockCount=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=29, evicted=0, evictedPerRun=0.0
[32mcluster2    |[0m 2017-01-17 19:53:37,729 DEBUG [LruStats #0] hfile.LruBlockCache: Total=407.30 KB, free=386.28 MB, max=386.68 MB, blockCount=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=29, evicted=0, evictedPerRun=0.0
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/SecurityAuth.audit <==
[36mcluster3    |[0m 2017-01-17 19:54:03,456 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[36mcluster3    |[0m 2017-01-17 19:54:03,456 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.3 port: 57288 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[36mcluster3    |[0m 
[36mcluster3    |[0m ==> logs/hbase--master-cluster3.log <==
[36mcluster3    |[0m 2017-01-17 19:54:03,455 DEBUG [cluster3,36241,1484682515004-BalancerChore] balancer.BaseLoadBalancer: Not running balancer because only 1 active regionserver(s)
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/SecurityAuth.audit <==
[33mcluster1    |[0m 2017-01-17 19:54:03,551 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[33mcluster1    |[0m 2017-01-17 19:54:03,552 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.4 port: 59918 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[33mcluster1    |[0m 
[33mcluster1    |[0m ==> logs/hbase--master-cluster1.log <==
[33mcluster1    |[0m 2017-01-17 19:54:03,551 DEBUG [cluster1,37102,1484682515111-BalancerChore] balancer.BaseLoadBalancer: Not running balancer because only 1 active regionserver(s)
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/SecurityAuth.audit <==
[32mcluster2    |[0m 2017-01-17 19:54:05,095 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Auth successful for null
[32mcluster2    |[0m 2017-01-17 19:54:05,095 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from 172.19.0.2 port: 54236 with version info: version: "0.98.24-hadoop2" url: "git://buildbox/data/src/hbase" revision: "9c13a1c3d8cf999014f30104d1aa9d79e74ca3d6" user: "apurtell" date: "Thu Dec 22 02:36:05 UTC 2016" src_checksum: "286dfd46f04c92066a514339558c8bf2"
[32mcluster2    |[0m 
[32mcluster2    |[0m ==> logs/hbase--master-cluster2.log <==
[32mcluster2    |[0m 2017-01-17 19:54:05,094 DEBUG [cluster2,42021,1484682516560-BalancerChore] balancer.BaseLoadBalancer: Not running balancer because only 1 active regionserver(s)
[36mcluster3    |[0m 2017-01-17 19:54:37,658 INFO  [ZooKeeperWatcher and Master delayed closing for connection hconnection-0x47c65163] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x159adf94f4b0002
[36mcluster3    |[0m 2017-01-17 19:54:37,658 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x159adf94f4b0002
[36mcluster3    |[0m 2017-01-17 19:54:37,660 INFO  [ZooKeeperWatcher and Master delayed closing for connection hconnection-0x47c65163] zookeeper.ZooKeeper: Session: 0x159adf94f4b0002 closed
[36mcluster3    |[0m 2017-01-17 19:54:37,660 INFO  [M:0;cluster3:36241-EventThread] zookeeper.ClientCnxn: EventThread shut down
[36mcluster3    |[0m 2017-01-17 19:54:37,660 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:18262] server.NIOServerCnxn: Closed socket connection for client /127.0.0.1:60530 which had sessionid 0x159adf94f4b0002
Gracefully stopping... (press Ctrl+C again to force)
